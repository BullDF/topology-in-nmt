\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[none]{hyphenat}
\usepackage[a4paper, total={7in, 8.5in}]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage{mathdots}
\usepackage{fancyhdr}
\usepackage[parfill]{parskip}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[round]{natbib}

\renewcommand{\qedsymbol}{\(\blacksquare\)}
\newcommand{\del}{\partial}
\newcommand{\x}{\mathbf{x}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ds}{\displaystyle}
\renewcommand{\del}{\partial}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\size}{\text{size}}

\newcommand{\assignmentname}{Topology in Neural Machine Translation}
\newcommand{\duedate}{3 Nov 2025}

\title{Topology in Neural Machine Translation: A Topological Study of Transformers through Attention}
\author{Yuwei (Johnny) Meng}
\date{\duedate}

\setlength{\parskip}{2mm}

\onehalfspacing

\setlength{\headheight}{14.5pt}

\begin{document}

\pagestyle{fancy}
\fancyhead[L]{\duedate}
\fancyhead[C]{\assignmentname}
\fancyhead[R]{Johnny Meng}

\maketitle

\section{Introduction}

In 2017, a group of researchers at Google proudly announced the architecture of the transformer neural model, which revolutionized the field of Natural Language Processing (NLP) \citep{vaswani}. Before then, neural NLP models mainly relied on recurrent structures, such as RNNs and LSTMs, optionally with the attention mechanism to boost performance. The transformer architecture, however, abandoned the recurrent structure in RNNs and LSTMs and solely utilized attention for language modeling, which was a novel but successful approach. Since the invention of transformer, NLP researchers have been applying this architecture to various NLP tasks, one of which is Machine Translation (MT). MT is an NLP task that takes a sentence in a source language as input and outputs the translated sentence in a target language, and Neural Machine Translation (NMT) is a subfield of MT that specifically uses neural networks as the model for the translation. According to \citet{vaswani}, the transformer model achieved a BLEU score of 41.0 on the WMT14 English-to-French benchmark, establishing a new state-of-the-art performance.

Despite the prominent performance of transformer, similar to other neural network architectures, the specific reasons behind its success remain largely unknown, particularly in the context of NMT. One method to probe the interpretability of neural networks is through Topological Data Analysis (TDA), where topological features that are intrinsic to the data and model are extracted and explained. This method, nevertheless, is also underexplored in NMT. Therefore, in this research project, I propose to apply TDA to explain the power of transformer on the task of NMT. Since the attention mechanism is the core of transformer, topology-related techniques will be applied to analyze the attention maps generated by transformer models during translation. Considering my knowledge of English and French and the abundance of such evaluation datasets, I will focus on the English-to-French translation task. The specific research question is as follows:

\begin{quote}
    \textit{Do English and French sentences create similar topological structures in the attention maps generated by transformer models during translation? If so, does similarity in the topological structures of attention maps correlate with translation quality?}
\end{quote}

\section{Related Work}

There are past studies that focused on using algebraic topology to analyze neural networks. For example, the paper by \citet{bianchini} is a pioneer study that leveraged topological tools to compare the expressivity of shallow and deep neural networks. They discovered that for deep neural networks, the sum of the Betti numbers, as a metric that measures the topological complexity that the network can express, can grow exponentially with the number of hidden units. Later, \citet{guss} extended this work by empirically applying Betti numbers to measure the topological complexity of real-world datasets and characterize the expressivity of fully-connected neural networks. These studies laid a solid foundation for using topological tools to study neural networks, but the generalization of such techniques to more complex neural structures still remains limited.

In addition to studying neural networks using topology, there are also attempts to apply topological techniques on language modeling. \citet{fitz} introduces the notion of a \textit{word manifold}, which turns \( n \)-gram models on raw texts of various languages into simplicial complexes, allowing for topological analysis. This study differs from my proposed study in that the method is applied to raw texts with no neural models associated with it. More recently, \citet{draganov} makes an effort to study word embeddings generated by large language models by considering the \( d \)-dimensional space that these embeddings are located in as a topological space. Then, they applied persistent homology to extract topological patterns from the embedding spaces formed by 81 languages. Their study suggested statistically significant results that word embeddings carry meaningful linguistic information, but there was no analysis of the underlying neural models.

Perhaps the most closely related study to my proposed topic is the one by \citet{meirom}. In this study, they also looked at embeddings as \citet{draganov} did, but they argue that certain semantics are inherent to the real world and are not language dependent. For example, \textit{dog} and \textit{cat} are both common pets so they often appear in the same context regardless of the language. Under this assumption, they claimed that the embedding spaces of different languages should be isomorphic to each other at the sentence level, and their results supported this. An interesting question therefore arose from this study: since the embedding spaces of different languages at the sentence level are isomorphic, and an NMT system transforms a sentence from the source language to the target language, how does the NMT system preserve such isomorphism during translation? This question is thus the main motivation of my proposed research.

Now, is looking at attention feasible? Previous studies said yes. \citet{ravishankar} studied fully using the attention of multilingual BERT to decode syntactic dependency trees of 18 languages, including English and French, and their results showed that solely using attention can achieve competitive accuracy in dependency parsing, suggesting that attention does encode meaningful syntactic information, which could be helpful in translation as well. Furthermore, \citet{kushnareva} studied the attention mechanism with topology. In the study, they first built weighted graphs from attention maps by treating tokens as nodes and attention weights as edges, followed by applying persistent homology on the graph to construct a filtration. Their topic was on detecting artificially generated texts which is different from mine, but this process is inspiring that I will adopt in my study.

To conclude this section, \citet{uchendu} in their paper of a survey on using TDA to approach NLP problems stated that:

\begin{quote}
    \textit{One glaring application is on multi-lingual tasks... Due to the benefits of TDA which include performing robustly on heterogeneous, imbalanced, and noisy data, its application on multi-lingual tasks is necessary.}
\end{quote}

This statement further motivates my proposed topic.

\section{Methodology}

The proposed methodology is as follows:

\begin{enumerate}
    \item Choose an NMT model. The NLLB (No Language Left Behind) model developed by Meta is a good candidate \citep{nllb}. This model offers translation between 200 languages, including English and French, and is open-sourced. The NLLB models are available on Hugging Face ranging from 600M to 54B parameters, allowing me to experiment with different model sizes considering possible computational constraints.
    \item Pick an evaluation dataset that aligns English and French sentences. The evaluation sets curated by the \textit{Workshop on Machine Translation} (WMT) are good resources to use and are available on Hugging Face, among which WMT14 is a solid choice which was also selected by \citet{vaswani}.
    \item Choose \( >1000 \) sentences from the datasets. Run the NMT model on these sentences to obtain the self-attention maps for both the English encoder and the French decoder.
    \item For each attention map, build a weighted graph by treating tokens as nodes and attention weights as edges, following \citet{kushnareva}. Run persistent homology on the weighted graph to extract topological features (\( \beta_0 = \text{connected components} \) and \( \beta_1 = \text{loops} \)).
    \item Align the self-attention topological features of the encoder and decoder based on sentence pairs. Compute the similarity between the features and find correlations between similarity scores and translation quality, measured by BLEU scores.
    \item Optionally track changes in topological features throughout the transformer layers.
    \item If time permits, experiment with other language pairs such as English-to-Chinese.
\end{enumerate}

The proposed research topic has its significance because of the following reasons:

\begin{itemize}
    \item If we do find that topological features are similar between English and French sentences, then it suggests that the transformer models are capable of preserving topological structures during translation, which helps explain their success in NMT.
    \item If similarity in topological features correlates with translation quality (i.e. BLEU scores), then we can possibly develop topology-based metrics to evaluate translation quality in the future.
    \item However, if the features differ, it still suggests that more work on other perspectives needs to be done to fully interpret an NMT system that uses transformer.
\end{itemize}

I hope this research can shed some light on the interpretability of transformer models in NMT through topological analysis. The findings may also inspire future research that applies topology to other NLP tasks involving transformer models.

\pagebreak

\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
