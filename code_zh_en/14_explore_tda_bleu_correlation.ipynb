{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore TDA-BLEU Correlation\n",
    "\n",
    "Combine topological similarity (Wasserstein distance) with translation quality (BLEU scores) to test our hypothesis:\n",
    "\n",
    "**Does topological similarity between English and Chinese attention patterns predict translation quality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import warnings\n",
    "\n",
    "# Configure matplotlib for Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Suppress warnings about infinite death times in persistence diagrams\n",
    "warnings.filterwarnings('ignore', message='.*non-finite death times.*')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TDA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TDA results\n",
    "tda_path = Path(\"../data/tda_results_zh_en/tda_results_last_layer_filtered.pkl\")\n",
    "\n",
    "print(f\"Loading TDA results from {tda_path}...\")\n",
    "with open(tda_path, 'rb') as f:\n",
    "    tda_results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(tda_results)} TDA results\")\n",
    "\n",
    "# Extract relevant TDA metrics\n",
    "df_tda = pd.DataFrame([{\n",
    "    'idx': r['idx'],\n",
    "    'wasserstein_distance': r['wasserstein_distance'],\n",
    "    'wasserstein_h0': r['wasserstein_h0'],\n",
    "    'wasserstein_h1': r['wasserstein_h1'],\n",
    "    'en_num_tokens': r['en_num_tokens'],\n",
    "    'zh_num_tokens': r['zh_num_tokens'],\n",
    "    'en_h0_features': r['en_h0_features'],\n",
    "    'en_h1_features': r['en_h1_features'],\n",
    "    'zh_h0_features': r['zh_h0_features'],\n",
    "    'zh_h1_features': r['zh_h1_features']\n",
    "} for r in tda_results])\n",
    "\n",
    "print(f\"\\nTDA DataFrame shape: {df_tda.shape}\")\n",
    "print(df_tda.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLEU scores\n",
    "bleu_path = Path(\"../data/bleu_scores_zh_en.csv\")\n",
    "\n",
    "print(f\"Loading BLEU scores from {bleu_path}...\")\n",
    "df_bleu = pd.read_csv(bleu_path)\n",
    "\n",
    "print(f\"✓ Loaded {len(df_bleu)} BLEU scores\")\n",
    "print(f\"\\nBLEU DataFrame shape: {df_bleu.shape}\")\n",
    "print(df_bleu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on idx\n",
    "df = pd.merge(df_tda, df_bleu, on='idx')\n",
    "\n",
    "print(f\"✓ Merged DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"Wasserstein Distance:\")\n",
    "print(f\"  Mean: {df['wasserstein_distance'].mean():.6f}\")\n",
    "print(f\"  Std:  {df['wasserstein_distance'].std():.6f}\")\n",
    "print()\n",
    "\n",
    "print(\"BLEU Scores:\")\n",
    "print(f\"  EN→ZH - Mean: {df['bleu_en_zh'].mean():.2f}, Std: {df['bleu_en_zh'].std():.2f}\")\n",
    "print(f\"  ZH→EN - Mean: {df['bleu_zh_en'].mean():.2f}, Std: {df['bleu_zh_en'].std():.2f}\")\n",
    "print(f\"  Avg   - Mean: {df['bleu_avg'].mean():.2f}, Std: {df['bleu_avg'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between Wasserstein distance and BLEU scores\n",
    "print(\"=\"*70)\n",
    "print(\"CORRELATION ANALYSIS: Wasserstein Distance vs BLEU\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Pearson correlation (linear relationship)\n",
    "pearson_en_zh, p_pearson_en_zh = pearsonr(df['wasserstein_distance'], df['bleu_en_zh'])\n",
    "pearson_zh_en, p_pearson_zh_en = pearsonr(df['wasserstein_distance'], df['bleu_zh_en'])\n",
    "pearson_avg, p_pearson_avg = pearsonr(df['wasserstein_distance'], df['bleu_avg'])\n",
    "\n",
    "print(\"Pearson Correlation (linear):\")\n",
    "print(f\"  Wasserstein vs EN→ZH BLEU: r = {pearson_en_zh:.4f}, p = {p_pearson_en_zh:.2e}\")\n",
    "print(f\"  Wasserstein vs ZH→EN BLEU: r = {pearson_zh_en:.4f}, p = {p_pearson_zh_en:.2e}\")\n",
    "print(f\"  Wasserstein vs Avg BLEU:   r = {pearson_avg:.4f}, p = {p_pearson_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Spearman correlation (monotonic relationship)\n",
    "spearman_en_zh, p_spearman_en_zh = spearmanr(df['wasserstein_distance'], df['bleu_en_zh'])\n",
    "spearman_zh_en, p_spearman_zh_en = spearmanr(df['wasserstein_distance'], df['bleu_zh_en'])\n",
    "spearman_avg, p_spearman_avg = spearmanr(df['wasserstein_distance'], df['bleu_avg'])\n",
    "\n",
    "print(\"Spearman Correlation (monotonic):\")\n",
    "print(f\"  Wasserstein vs EN→ZH BLEU: ρ = {spearman_en_zh:.4f}, p = {p_spearman_en_zh:.2e}\")\n",
    "print(f\"  Wasserstein vs ZH→EN BLEU: ρ = {spearman_zh_en:.4f}, p = {p_spearman_zh_en:.2e}\")\n",
    "print(f\"  Wasserstein vs Avg BLEU:   ρ = {spearman_avg:.4f}, p = {p_spearman_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "print(\"Interpretation:\")\n",
    "if abs(pearson_avg) < 0.1:\n",
    "    strength = \"negligible\"\n",
    "elif abs(pearson_avg) < 0.3:\n",
    "    strength = \"weak\"\n",
    "elif abs(pearson_avg) < 0.5:\n",
    "    strength = \"moderate\"\n",
    "else:\n",
    "    strength = \"strong\"\n",
    "\n",
    "direction = \"negative\" if pearson_avg < 0 else \"positive\"\n",
    "print(f\"  Overall correlation is {strength} and {direction}.\")\n",
    "\n",
    "if pearson_avg < 0:\n",
    "    print(f\"  → Lower Wasserstein distance (more similar topology) is associated with higher BLEU (better translation).\")\n",
    "else:\n",
    "    print(f\"  → Higher Wasserstein distance (more different topology) is associated with higher BLEU (better translation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scatter Plots: Wasserstein Distance vs BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→ZH BLEU\n",
    "axes[0].scatter(df['wasserstein_distance'], df['bleu_en_zh'], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[0].set_ylabel('EN→ZH BLEU Score')\n",
    "axes[0].set_title(f'Wasserstein vs EN→ZH BLEU\\nr = {pearson_en_zh:.3f}, p = {p_pearson_en_zh:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_en_zh'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[0].legend()\n",
    "\n",
    "# ZH→EN BLEU\n",
    "axes[1].scatter(df['wasserstein_distance'], df['bleu_zh_en'], alpha=0.3, s=10)\n",
    "axes[1].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[1].set_ylabel('ZH→EN BLEU Score')\n",
    "axes[1].set_title(f'Wasserstein vs ZH→EN BLEU\\nr = {pearson_zh_en:.3f}, p = {p_pearson_zh_en:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_zh_en'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[1].legend()\n",
    "\n",
    "# Average BLEU\n",
    "axes[2].scatter(df['wasserstein_distance'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[2].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[2].set_ylabel('Average BLEU Score')\n",
    "axes[2].set_title(f'Wasserstein vs Average BLEU\\nr = {pearson_avg:.3f}, p = {p_pearson_avg:.2e}')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[2].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. H0 vs H1 Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze H0 and H1 components separately\n",
    "print(\"=\"*70)\n",
    "print(\"H0 vs H1 CONTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# H0 correlations\n",
    "pearson_h0, p_h0 = pearsonr(df['wasserstein_h0'], df['bleu_avg'])\n",
    "print(f\"H0 (Connected Components) vs Avg BLEU:\")\n",
    "print(f\"  Pearson r = {pearson_h0:.4f}, p = {p_h0:.2e}\")\n",
    "print()\n",
    "\n",
    "# H1 correlations\n",
    "pearson_h1, p_h1 = pearsonr(df['wasserstein_h1'], df['bleu_avg'])\n",
    "print(f\"H1 (Loops/Holes) vs Avg BLEU:\")\n",
    "print(f\"  Pearson r = {pearson_h1:.4f}, p = {p_h1:.2e}\")\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(df['wasserstein_h0'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('H0 Wasserstein Distance')\n",
    "axes[0].set_ylabel('Average BLEU Score')\n",
    "axes[0].set_title(f'H0 vs BLEU\\nr = {pearson_h0:.3f}, p = {p_h0:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_h0'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(df['wasserstein_h0'], p(df['wasserstein_h0']), \"r--\", alpha=0.5)\n",
    "\n",
    "axes[1].scatter(df['wasserstein_h1'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[1].set_xlabel('H1 Wasserstein Distance')\n",
    "axes[1].set_ylabel('Average BLEU Score')\n",
    "axes[1].set_title(f'H1 vs BLEU\\nr = {pearson_h1:.3f}, p = {p_h1:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_h1'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(df['wasserstein_h1'], p(df['wasserstein_h1']), \"r--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. BLEU vs Token Count Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between BLEU scores and token counts\n",
    "print(\"=\"*70)\n",
    "print(\"CORRELATION ANALYSIS: Token Count vs BLEU\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Compute correlations\n",
    "pearson_en_tokens_en_zh, p_en_en_zh = pearsonr(df['en_num_tokens'], df['bleu_en_zh'])\n",
    "pearson_en_tokens_zh_en, p_en_zh_en = pearsonr(df['en_num_tokens'], df['bleu_zh_en'])\n",
    "pearson_en_tokens_avg, p_en_avg = pearsonr(df['en_num_tokens'], df['bleu_avg'])\n",
    "\n",
    "pearson_zh_tokens_en_zh, p_zh_en_zh = pearsonr(df['zh_num_tokens'], df['bleu_en_zh'])\n",
    "pearson_zh_tokens_zh_en, p_zh_zh_en = pearsonr(df['zh_num_tokens'], df['bleu_zh_en'])\n",
    "pearson_zh_tokens_avg, p_zh_avg = pearsonr(df['zh_num_tokens'], df['bleu_avg'])\n",
    "\n",
    "print(\"English Token Count vs BLEU:\")\n",
    "print(f\"  EN tokens vs EN→ZH BLEU: r = {pearson_en_tokens_en_zh:.4f}, p = {p_en_en_zh:.2e}\")\n",
    "print(f\"  EN tokens vs ZH→EN BLEU: r = {pearson_en_tokens_zh_en:.4f}, p = {p_en_zh_en:.2e}\")\n",
    "print(f\"  EN tokens vs Avg BLEU:   r = {pearson_en_tokens_avg:.4f}, p = {p_en_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "print(\"Chinese Token Count vs BLEU:\")\n",
    "print(f\"  ZH tokens vs EN→ZH BLEU: r = {pearson_zh_tokens_en_zh:.4f}, p = {p_zh_en_zh:.2e}\")\n",
    "print(f\"  ZH tokens vs ZH→EN BLEU: r = {pearson_zh_tokens_zh_en:.4f}, p = {p_zh_zh_en:.2e}\")\n",
    "print(f\"  ZH tokens vs Avg BLEU:   r = {pearson_zh_tokens_avg:.4f}, p = {p_zh_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# English tokens vs BLEU scores\n",
    "axes[0, 0].scatter(df['en_num_tokens'], df['bleu_en_zh'], alpha=0.3, s=10, color='blue')\n",
    "en_slope_en_zh, en_int_en_zh, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_en_zh'])\n",
    "en_x = np.array([df['en_num_tokens'].min(), df['en_num_tokens'].max()])\n",
    "axes[0, 0].plot(en_x, en_slope_en_zh * en_x + en_int_en_zh, 'b-', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('English Token Count')\n",
    "axes[0, 0].set_ylabel('EN→ZH BLEU Score')\n",
    "axes[0, 0].set_title(f'EN Tokens vs EN→ZH BLEU\\nr = {pearson_en_tokens_en_zh:.3f}, p = {p_en_en_zh:.2e}')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].scatter(df['en_num_tokens'], df['bleu_zh_en'], alpha=0.3, s=10, color='blue')\n",
    "en_slope_zh_en, en_int_zh_en, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_zh_en'])\n",
    "axes[0, 1].plot(en_x, en_slope_zh_en * en_x + en_int_zh_en, 'b-', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].set_xlabel('English Token Count')\n",
    "axes[0, 1].set_ylabel('ZH→EN BLEU Score')\n",
    "axes[0, 1].set_title(f'EN Tokens vs ZH→EN BLEU\\nr = {pearson_en_tokens_zh_en:.3f}, p = {p_en_zh_en:.2e}')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 2].scatter(df['en_num_tokens'], df['bleu_avg'], alpha=0.3, s=10, color='blue')\n",
    "en_slope_avg, en_int_avg, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_avg'])\n",
    "axes[0, 2].plot(en_x, en_slope_avg * en_x + en_int_avg, 'b-', linewidth=2, alpha=0.8)\n",
    "axes[0, 2].set_xlabel('English Token Count')\n",
    "axes[0, 2].set_ylabel('Average BLEU Score')\n",
    "axes[0, 2].set_title(f'EN Tokens vs Avg BLEU\\nr = {pearson_en_tokens_avg:.3f}, p = {p_en_avg:.2e}')\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# Chinese tokens vs BLEU scores\n",
    "axes[1, 0].scatter(df['zh_num_tokens'], df['bleu_en_zh'], alpha=0.3, s=10, color='green')\n",
    "zh_slope_en_zh, zh_int_en_zh, _, _, _ = stats.linregress(df['zh_num_tokens'], df['bleu_en_zh'])\n",
    "zh_x = np.array([df['zh_num_tokens'].min(), df['zh_num_tokens'].max()])\n",
    "axes[1, 0].plot(zh_x, zh_slope_en_zh * zh_x + zh_int_en_zh, 'g-', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Chinese Token Count')\n",
    "axes[1, 0].set_ylabel('EN→ZH BLEU Score')\n",
    "axes[1, 0].set_title(f'ZH Tokens vs EN→ZH BLEU\\nr = {pearson_zh_tokens_en_zh:.3f}, p = {p_zh_en_zh:.2e}')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].scatter(df['zh_num_tokens'], df['bleu_zh_en'], alpha=0.3, s=10, color='green')\n",
    "zh_slope_zh_en, zh_int_zh_en, _, _, _ = stats.linregress(df['zh_num_tokens'], df['bleu_zh_en'])\n",
    "axes[1, 1].plot(zh_x, zh_slope_zh_en * zh_x + zh_int_zh_en, 'g-', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Chinese Token Count')\n",
    "axes[1, 1].set_ylabel('ZH→EN BLEU Score')\n",
    "axes[1, 1].set_title(f'ZH Tokens vs ZH→EN BLEU\\nr = {pearson_zh_tokens_zh_en:.3f}, p = {p_zh_zh_en:.2e}')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 2].scatter(df['zh_num_tokens'], df['bleu_avg'], alpha=0.3, s=10, color='green')\n",
    "zh_slope_avg, zh_int_avg, _, _, _ = stats.linregress(df['zh_num_tokens'], df['bleu_avg'])\n",
    "axes[1, 2].plot(zh_x, zh_slope_avg * zh_x + zh_int_avg, 'g-', linewidth=2, alpha=0.8)\n",
    "axes[1, 2].set_xlabel('Chinese Token Count')\n",
    "axes[1, 2].set_ylabel('Average BLEU Score')\n",
    "axes[1, 2].set_title(f'ZH Tokens vs Avg BLEU\\nr = {pearson_zh_tokens_avg:.3f}, p = {p_zh_avg:.2e}')\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. Partial Correlation: Wasserstein vs BLEU (Controlling for Token Count)\n",
    "\n",
    "Since both Wasserstein distance and BLEU are correlated with token counts, we need to compute **partial correlation** to determine if the Wasserstein-BLEU relationship is genuine or spurious.\n",
    "\n",
    "**Method**: Use linear regression residuals\n",
    "1. Regress Wasserstein distance on token counts → get residuals\n",
    "2. Regress BLEU on token counts → get residuals  \n",
    "3. Correlate the residuals (= partial correlation, controlling for token count effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare predictor matrix: both English and Chinese token counts\n",
    "X_tokens = df[['en_num_tokens', 'zh_num_tokens']].values\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PARTIAL CORRELATION: Wasserstein vs BLEU (Controlling for Token Counts)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Function to compute partial correlation via residuals\n",
    "def partial_correlation_residuals(y1, y2, X):\n",
    "    \"\"\"\n",
    "    Compute partial correlation between y1 and y2, controlling for X.\n",
    "    \n",
    "    Method: Regress both y1 and y2 on X, then correlate the residuals.\n",
    "    \"\"\"\n",
    "    # Reshape if needed\n",
    "    y1 = np.array(y1).reshape(-1, 1)\n",
    "    y2 = np.array(y2).reshape(-1, 1)\n",
    "    \n",
    "    # Regress y1 on X\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X, y1)\n",
    "    residuals1 = y1.flatten() - model1.predict(X).flatten()\n",
    "    \n",
    "    # Regress y2 on X\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X, y2)\n",
    "    residuals2 = y2.flatten() - model2.predict(X).flatten()\n",
    "    \n",
    "    # Correlate residuals\n",
    "    r, p = pearsonr(residuals1, residuals2)\n",
    "    return r, p, residuals1, residuals2\n",
    "\n",
    "# Compute partial correlations for each BLEU metric\n",
    "print(\"Partial Correlation (controlling for EN and ZH token counts):\")\n",
    "print()\n",
    "\n",
    "# Wasserstein vs EN→ZH BLEU\n",
    "r_partial_en_zh, p_partial_en_zh, w_resid_en_zh, bleu_resid_en_zh = partial_correlation_residuals(\n",
    "    df['wasserstein_distance'], df['bleu_en_zh'], X_tokens\n",
    ")\n",
    "print(f\"Wasserstein vs EN→ZH BLEU:\")\n",
    "print(f\"  Original correlation:  r = {pearson_en_zh:.4f}, p = {p_pearson_en_zh:.2e}\")\n",
    "print(f\"  Partial correlation:   r = {r_partial_en_zh:.4f}, p = {p_partial_en_zh:.2e}\")\n",
    "print()\n",
    "\n",
    "# Wasserstein vs ZH→EN BLEU\n",
    "r_partial_zh_en, p_partial_zh_en, w_resid_zh_en, bleu_resid_zh_en = partial_correlation_residuals(\n",
    "    df['wasserstein_distance'], df['bleu_zh_en'], X_tokens\n",
    ")\n",
    "print(f\"Wasserstein vs ZH→EN BLEU:\")\n",
    "print(f\"  Original correlation:  r = {pearson_zh_en:.4f}, p = {p_pearson_zh_en:.2e}\")\n",
    "print(f\"  Partial correlation:   r = {r_partial_zh_en:.4f}, p = {p_partial_zh_en:.2e}\")\n",
    "print()\n",
    "\n",
    "# Wasserstein vs Average BLEU\n",
    "r_partial_avg, p_partial_avg, w_resid_avg, bleu_resid_avg = partial_correlation_residuals(\n",
    "    df['wasserstein_distance'], df['bleu_avg'], X_tokens\n",
    ")\n",
    "print(f\"Wasserstein vs Average BLEU:\")\n",
    "print(f\"  Original correlation:  r = {pearson_avg:.4f}, p = {p_pearson_avg:.2e}\")\n",
    "print(f\"  Partial correlation:   r = {r_partial_avg:.4f}, p = {p_partial_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "print(\"Interpretation:\")\n",
    "change_en_zh = abs(r_partial_en_zh) - abs(pearson_en_zh)\n",
    "change_zh_en = abs(r_partial_zh_en) - abs(pearson_zh_en)\n",
    "change_avg = abs(r_partial_avg) - abs(pearson_avg)\n",
    "\n",
    "print(f\"  EN→ZH: Correlation changed by {change_en_zh:+.4f} after controlling for token counts\")\n",
    "print(f\"  ZH→EN: Correlation changed by {change_zh_en:+.4f} after controlling for token counts\")\n",
    "print(f\"  Avg:   Correlation changed by {change_avg:+.4f} after controlling for token counts\")\n",
    "print()\n",
    "\n",
    "if abs(change_avg) < 0.05:\n",
    "    print(\"  → Token count has minimal confounding effect. The Wasserstein-BLEU relationship is genuine.\")\n",
    "elif abs(r_partial_avg) < abs(pearson_avg) * 0.5:\n",
    "    print(\"  → Token count is a major confounder. Much of the Wasserstein-BLEU correlation is explained by token count.\")\n",
    "else:\n",
    "    print(\"  → Token count has some confounding effect, but the Wasserstein-BLEU relationship persists.\")\n",
    "\n",
    "# Visualize partial correlation (residuals plot)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→ZH\n",
    "axes[0].scatter(w_resid_en_zh, bleu_resid_en_zh, alpha=0.3, s=10)\n",
    "z = np.polyfit(w_resid_en_zh, bleu_resid_en_zh, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "axes[0].plot(w_resid_en_zh, p_fit(w_resid_en_zh), \"r--\", alpha=0.5)\n",
    "axes[0].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\n",
    "axes[0].set_ylabel('EN→ZH BLEU\\n(residuals after removing token count effect)')\n",
    "axes[0].set_title(f'Partial Correlation: Wasserstein vs EN→ZH BLEU\\nr_partial = {r_partial_en_zh:.3f}, p = {p_partial_en_zh:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# ZH→EN\n",
    "axes[1].scatter(w_resid_zh_en, bleu_resid_zh_en, alpha=0.3, s=10)\n",
    "z = np.polyfit(w_resid_zh_en, bleu_resid_zh_en, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "axes[1].plot(w_resid_zh_en, p_fit(w_resid_zh_en), \"r--\", alpha=0.5)\n",
    "axes[1].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\n",
    "axes[1].set_ylabel('ZH→EN BLEU\\n(residuals after removing token count effect)')\n",
    "axes[1].set_title(f'Partial Correlation: Wasserstein vs ZH→EN BLEU\\nr_partial = {r_partial_zh_en:.3f}, p = {p_partial_zh_en:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Average\n",
    "axes[2].scatter(w_resid_avg, bleu_resid_avg, alpha=0.3, s=10)\n",
    "z = np.polyfit(w_resid_avg, bleu_resid_avg, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "axes[2].plot(w_resid_avg, p_fit(w_resid_avg), \"r--\", alpha=0.5)\n",
    "axes[2].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[2].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[2].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\n",
    "axes[2].set_ylabel('Average BLEU\\n(residuals after removing token count effect)')\n",
    "axes[2].set_title(f'Partial Correlation: Wasserstein vs Average BLEU\\nr_partial = {r_partial_avg:.3f}, p = {p_partial_avg:.2e}')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Binned Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin by Wasserstein distance and compare mean BLEU scores\n",
    "df['w_bin'] = pd.qcut(df['wasserstein_distance'], q=5, labels=['Very Similar', 'Similar', 'Moderate', 'Dissimilar', 'Very Dissimilar'])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BINNED ANALYSIS: Mean BLEU by Topological Similarity\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "binned_stats = df.groupby('w_bin').agg({\n",
    "    'bleu_avg': ['mean', 'std', 'count'],\n",
    "    'wasserstein_distance': ['mean', 'std']\n",
    "})\n",
    "\n",
    "print(binned_stats)\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bin_means = df.groupby('w_bin')['bleu_avg'].mean()\n",
    "bin_stds = df.groupby('w_bin')['bleu_avg'].std()\n",
    "\n",
    "ax.bar(range(len(bin_means)), bin_means, yerr=bin_stds, capsize=5, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(range(len(bin_means)))\n",
    "ax.set_xticklabels(bin_means.index, rotation=45, ha='right')\n",
    "ax.set_xlabel('Topological Similarity (Wasserstein Distance Bins)')\n",
    "ax.set_ylabel('Mean BLEU Score')\n",
    "ax.set_title('Translation Quality by Topological Similarity')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Examples: High Similarity vs Low Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Wasserstein distance\n",
    "df_sorted = df.sort_values('wasserstein_distance')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HIGH TOPOLOGICAL SIMILARITY (Low Wasserstein Distance)\")\n",
    "print(\"=\"*70)\n",
    "for i in range(5):\n",
    "    row = df_sorted.iloc[i]\n",
    "    print(f\"\\n[{i+1}] Pair {int(row['idx'])}: W = {row['wasserstein_distance']:.4f}, BLEU = {row['bleu_avg']:.2f}\")\n",
    "    \n",
    "    # Get original text from TDA results\n",
    "    original = tda_results[int(row['idx'])]\n",
    "    print(f\"    EN: {original['en_text']}\")\n",
    "    print(f\"    ZH: {original['zh_text']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOW TOPOLOGICAL SIMILARITY (High Wasserstein Distance)\")\n",
    "print(\"=\"*70)\n",
    "for i in range(5):\n",
    "    row = df_sorted.iloc[-(i+1)]\n",
    "    print(f\"\\n[{i+1}] Pair {int(row['idx'])}: W = {row['wasserstein_distance']:.4f}, BLEU = {row['bleu_avg']:.2f}\")\n",
    "    \n",
    "    original = tda_results[int(row['idx'])]\n",
    "    print(f\"    EN: {original['en_text']}\")\n",
    "    print(f\"    ZH: {original['zh_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Findings:**\n",
    "- Correlation between topological similarity (Wasserstein distance) and translation quality (BLEU)\n",
    "- H0 vs H1 contribution to the relationship\n",
    "- Binned analysis showing trend across similarity levels\n",
    "\n",
    "**Hypothesis Test:**\n",
    "Does lower Wasserstein distance (more topologically similar attention patterns) predict higher BLEU scores (better translation quality)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
