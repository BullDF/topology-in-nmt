{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Explore TDA-BLEU Correlation\n",
    "\n",
    "Combine topological similarity (Wasserstein distance) with translation quality (BLEU scores) to test our hypothesis:\n",
    "\n",
    "**Does topological similarity between English and Chinese attention patterns predict translation quality?**\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path in code_zh_en\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell and run from \"Verify Working Directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only needed for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # IMPORTANT: Set this to your code_zh_en directory path\n",
    "    # This should point to where THIS notebook is located\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_zh_en\"\n",
    "    \n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"✓ Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab, using local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Verify Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify working directory and required files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check required files\n",
    "tda_file = \"../data/tda_results_zh_en/tda_results_last_layer_filtered.pkl\"\n",
    "bleu_file = \"../data/bleu_scores_zh_en.csv\"\n",
    "\n",
    "if os.path.exists(tda_file):\n",
    "    print(f\"✓ TDA results file exists: {tda_file}\")\n",
    "    print(f\"  File size: {Path(tda_file).stat().st_size / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    print(f\"✗ TDA results file NOT found: {tda_file}\")\n",
    "\n",
    "if os.path.exists(bleu_file):\n",
    "    print(f\"✓ BLEU scores file exists: {bleu_file}\")\n",
    "else:\n",
    "    print(f\"✗ BLEU scores file NOT found: {bleu_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import spearmanr, pearsonr\nimport warnings\n\n# Configure matplotlib for Chinese font support\nplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\nplt.rcParams['axes.unicode_minus'] = False\n\n# Suppress warnings about infinite death times in persistence diagrams\nwarnings.filterwarnings('ignore', message='.*non-finite death times.*')\n\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"✓ Libraries imported\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Load TDA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TDA results\n",
    "tda_path = Path(\"../data/tda_results_zh_en/tda_results_last_layer_filtered.pkl\")\n",
    "\n",
    "print(f\"Loading TDA results from {tda_path}...\")\n",
    "with open(tda_path, 'rb') as f:\n",
    "    tda_results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(tda_results)} TDA results\")\n",
    "\n",
    "# Extract relevant TDA metrics\n",
    "df_tda = pd.DataFrame([{\n",
    "    'idx': r['idx'],\n",
    "    'wasserstein_distance': r['wasserstein_distance'],\n",
    "    'wasserstein_h0': r['wasserstein_h0'],\n",
    "    'wasserstein_h1': r['wasserstein_h1'],\n",
    "    'en_num_tokens': r['en_num_tokens'],\n",
    "    'zh_num_tokens': r['zh_num_tokens'],\n",
    "    'en_h0_features': r['en_h0_features'],\n",
    "    'en_h1_features': r['en_h1_features'],\n",
    "    'zh_h0_features': r['zh_h0_features'],\n",
    "    'zh_h1_features': r['zh_h1_features']\n",
    "} for r in tda_results])\n",
    "\n",
    "print(f\"\\nTDA DataFrame shape: {df_tda.shape}\")\n",
    "print(df_tda.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Load BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLEU scores\n",
    "bleu_path = Path(\"../data/bleu_scores_zh_en.csv\")\n",
    "\n",
    "print(f\"Loading BLEU scores from {bleu_path}...\")\n",
    "df_bleu = pd.read_csv(bleu_path)\n",
    "\n",
    "print(f\"✓ Loaded {len(df_bleu)} BLEU scores\")\n",
    "print(f\"\\nBLEU DataFrame shape: {df_bleu.shape}\")\n",
    "print(df_bleu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on idx\n",
    "df = pd.merge(df_tda, df_bleu, on='idx')\n",
    "\n",
    "print(f\"✓ Merged DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"Wasserstein Distance:\")\n",
    "print(f\"  Mean: {df['wasserstein_distance'].mean():.6f}\")\n",
    "print(f\"  Std:  {df['wasserstein_distance'].std():.6f}\")\n",
    "print()\n",
    "\n",
    "print(\"BLEU Scores:\")\n",
    "print(f\"  EN→ZH - Mean: {df['bleu_en_zh'].mean():.2f}, Std: {df['bleu_en_zh'].std():.2f}\")\n",
    "print(f\"  ZH→EN - Mean: {df['bleu_zh_en'].mean():.2f}, Std: {df['bleu_zh_en'].std():.2f}\")\n",
    "print(f\"  Avg   - Mean: {df['bleu_avg'].mean():.2f}, Std: {df['bleu_avg'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between Wasserstein distance and BLEU scores\n",
    "print(\"=\" * 70)\n",
    "print(\"CORRELATION ANALYSIS: Wasserstein Distance vs BLEU\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Pearson correlation (linear relationship)\n",
    "pearson_en_zh, p_pearson_en_zh = pearsonr(df['wasserstein_distance'], df['bleu_en_zh'])\n",
    "pearson_zh_en, p_pearson_zh_en = pearsonr(df['wasserstein_distance'], df['bleu_zh_en'])\n",
    "pearson_avg, p_pearson_avg = pearsonr(df['wasserstein_distance'], df['bleu_avg'])\n",
    "\n",
    "print(\"Pearson Correlation (linear):\")\n",
    "print(f\"  Wasserstein vs EN→ZH BLEU: r = {pearson_en_zh:.4f}, p = {p_pearson_en_zh:.2e}\")\n",
    "print(f\"  Wasserstein vs ZH→EN BLEU: r = {pearson_zh_en:.4f}, p = {p_pearson_zh_en:.2e}\")\n",
    "print(f\"  Wasserstein vs Avg BLEU:   r = {pearson_avg:.4f}, p = {p_pearson_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Spearman correlation (monotonic relationship)\n",
    "spearman_en_zh, p_spearman_en_zh = spearmanr(df['wasserstein_distance'], df['bleu_en_zh'])\n",
    "spearman_zh_en, p_spearman_zh_en = spearmanr(df['wasserstein_distance'], df['bleu_zh_en'])\n",
    "spearman_avg, p_spearman_avg = spearmanr(df['wasserstein_distance'], df['bleu_avg'])\n",
    "\n",
    "print(\"Spearman Correlation (monotonic):\")\n",
    "print(f\"  Wasserstein vs EN→ZH BLEU: ρ = {spearman_en_zh:.4f}, p = {p_spearman_en_zh:.2e}\")\n",
    "print(f\"  Wasserstein vs ZH→EN BLEU: ρ = {spearman_zh_en:.4f}, p = {p_spearman_zh_en:.2e}\")\n",
    "print(f\"  Wasserstein vs Avg BLEU:   ρ = {spearman_avg:.4f}, p = {p_spearman_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "print(\"Interpretation:\")\n",
    "if abs(pearson_avg) < 0.1:\n",
    "    strength = \"negligible\"\n",
    "elif abs(pearson_avg) < 0.3:\n",
    "    strength = \"weak\"\n",
    "elif abs(pearson_avg) < 0.5:\n",
    "    strength = \"moderate\"\n",
    "else:\n",
    "    strength = \"strong\"\n",
    "\n",
    "direction = \"negative\" if pearson_avg < 0 else \"positive\"\n",
    "print(f\"  Overall correlation is {strength} and {direction}.\")\n",
    "\n",
    "if pearson_avg < 0:\n",
    "    print(f\"  → Lower Wasserstein distance (more similar topology) is associated with higher BLEU (better translation).\")\n",
    "else:\n",
    "    print(f\"  → Higher Wasserstein distance (more different topology) is associated with higher BLEU (better translation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Scatter Plots: Wasserstein Distance vs BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→ZH BLEU\n",
    "axes[0].scatter(df['wasserstein_distance'], df['bleu_en_zh'], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[0].set_ylabel('EN→ZH BLEU Score')\n",
    "axes[0].set_title(f'Wasserstein vs EN→ZH BLEU\\nr = {pearson_en_zh:.3f}, p = {p_pearson_en_zh:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_en_zh'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[0].legend()\n",
    "\n",
    "# ZH→EN BLEU\n",
    "axes[1].scatter(df['wasserstein_distance'], df['bleu_zh_en'], alpha=0.3, s=10)\n",
    "axes[1].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[1].set_ylabel('ZH→EN BLEU Score')\n",
    "axes[1].set_title(f'Wasserstein vs ZH→EN BLEU\\nr = {pearson_zh_en:.3f}, p = {p_pearson_zh_en:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_zh_en'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[1].legend()\n",
    "\n",
    "# Average BLEU\n",
    "axes[2].scatter(df['wasserstein_distance'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[2].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[2].set_ylabel('Average BLEU Score')\n",
    "axes[2].set_title(f'Wasserstein vs Average BLEU\\nr = {pearson_avg:.3f}, p = {p_pearson_avg:.2e}')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[2].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. H0 vs H1 Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze H0 and H1 components separately\n",
    "print(\"=\" * 70)\n",
    "print(\"H0 vs H1 CONTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# H0 correlations\n",
    "pearson_h0, p_h0 = pearsonr(df['wasserstein_h0'], df['bleu_avg'])\n",
    "print(f\"H0 (Connected Components) vs Avg BLEU:\")\n",
    "print(f\"  Pearson r = {pearson_h0:.4f}, p = {p_h0:.2e}\")\n",
    "print()\n",
    "\n",
    "# H1 correlations\n",
    "pearson_h1, p_h1 = pearsonr(df['wasserstein_h1'], df['bleu_avg'])\n",
    "print(f\"H1 (Loops/Holes) vs Avg BLEU:\")\n",
    "print(f\"  Pearson r = {pearson_h1:.4f}, p = {p_h1:.2e}\")\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(df['wasserstein_h0'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('H0 Wasserstein Distance')\n",
    "axes[0].set_ylabel('Average BLEU Score')\n",
    "axes[0].set_title(f'H0 vs BLEU\\nr = {pearson_h0:.3f}, p = {p_h0:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_h0'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(df['wasserstein_h0'], p(df['wasserstein_h0']), \"r--\", alpha=0.5)\n",
    "\n",
    "axes[1].scatter(df['wasserstein_h1'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[1].set_xlabel('H1 Wasserstein Distance')\n",
    "axes[1].set_ylabel('Average BLEU Score')\n",
    "axes[1].set_title(f'H1 vs BLEU\\nr = {pearson_h1:.3f}, p = {p_h1:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_h1'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(df['wasserstein_h1'], p(df['wasserstein_h1']), \"r--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8.5. BLEU vs Token Count Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze correlation between BLEU scores and token counts\nprint(\"=\" * 70)\nprint(\"CORRELATION ANALYSIS: Token Count vs BLEU\")\nprint(\"=\" * 70)\nprint()\n\n# Compute correlations\npearson_en_tokens_en_zh, p_en_en_zh = pearsonr(df['en_num_tokens'], df['bleu_en_zh'])\npearson_en_tokens_zh_en, p_en_zh_en = pearsonr(df['en_num_tokens'], df['bleu_zh_en'])\npearson_en_tokens_avg, p_en_avg = pearsonr(df['en_num_tokens'], df['bleu_avg'])\n\npearson_zh_tokens_en_zh, p_zh_en_zh = pearsonr(df['zh_num_tokens'], df['bleu_en_zh'])\npearson_zh_tokens_zh_en, p_zh_zh_en = pearsonr(df['zh_num_tokens'], df['bleu_zh_en'])\npearson_zh_tokens_avg, p_zh_avg = pearsonr(df['zh_num_tokens'], df['bleu_avg'])\n\nprint(\"English Token Count vs BLEU:\")\nprint(f\"  EN tokens vs EN→ZH BLEU: r = {pearson_en_tokens_en_zh:.4f}, p = {p_en_en_zh:.2e}\")\nprint(f\"  EN tokens vs ZH→EN BLEU: r = {pearson_en_tokens_zh_en:.4f}, p = {p_en_zh_en:.2e}\")\nprint(f\"  EN tokens vs Avg BLEU:   r = {pearson_en_tokens_avg:.4f}, p = {p_en_avg:.2e}\")\nprint()\n\nprint(\"Chinese Token Count vs BLEU:\")\nprint(f\"  ZH tokens vs EN→ZH BLEU: r = {pearson_zh_tokens_en_zh:.4f}, p = {p_zh_en_zh:.2e}\")\nprint(f\"  ZH tokens vs ZH→EN BLEU: r = {pearson_zh_tokens_zh_en:.4f}, p = {p_zh_zh_en:.2e}\")\nprint(f\"  ZH tokens vs Avg BLEU:   r = {pearson_zh_tokens_avg:.4f}, p = {p_zh_avg:.2e}\")\nprint()\n\n# Visualize\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# English tokens vs BLEU scores\naxes[0, 0].scatter(df['en_num_tokens'], df['bleu_en_zh'], alpha=0.3, s=10, color='blue')\nen_slope_en_zh, en_int_en_zh, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_en_zh'])\nen_x = np.array([df['en_num_tokens'].min(), df['en_num_tokens'].max()])\naxes[0, 0].plot(en_x, en_slope_en_zh * en_x + en_int_en_zh, 'b-', linewidth=2, alpha=0.8)\naxes[0, 0].set_xlabel('English Token Count')\naxes[0, 0].set_ylabel('EN→ZH BLEU Score')\naxes[0, 0].set_title(f'EN Tokens vs EN→ZH BLEU\\nr = {pearson_en_tokens_en_zh:.3f}, p = {p_en_en_zh:.2e}')\naxes[0, 0].grid(alpha=0.3)\n\naxes[0, 1].scatter(df['en_num_tokens'], df['bleu_zh_en'], alpha=0.3, s=10, color='blue')\nen_slope_zh_en, en_int_zh_en, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_zh_en'])\naxes[0, 1].plot(en_x, en_slope_zh_en * en_x + en_int_zh_en, 'b-', linewidth=2, alpha=0.8)\naxes[0, 1].set_xlabel('English Token Count')\naxes[0, 1].set_ylabel('ZH→EN BLEU Score')\naxes[0, 1].set_title(f'EN Tokens vs ZH→EN BLEU\\nr = {pearson_en_tokens_zh_en:.3f}, p = {p_en_zh_en:.2e}')\naxes[0, 1].grid(alpha=0.3)\n\naxes[0, 2].scatter(df['en_num_tokens'], df['bleu_avg'], alpha=0.3, s=10, color='blue')\nen_slope_avg, en_int_avg, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_avg'])\naxes[0, 2].plot(en_x, en_slope_avg * en_x + en_int_avg, 'b-', linewidth=2, alpha=0.8)\naxes[0, 2].set_xlabel('English Token Count')\naxes[0, 2].set_ylabel('Average BLEU Score')\naxes[0, 2].set_title(f'EN Tokens vs Avg BLEU\\nr = {pearson_en_tokens_avg:.3f}, p = {p_en_avg:.2e}')\naxes[0, 2].grid(alpha=0.3)\n\n# Chinese tokens vs BLEU scores\naxes[1, 0].scatter(df['zh_num_tokens'], df['bleu_en_zh'], alpha=0.3, s=10, color='green')\nzh_slope_en_zh, zh_int_en_zh, _, _, _ = stats.linregress(df['zh_num_tokens'], df['bleu_en_zh'])\nzh_x = np.array([df['zh_num_tokens'].min(), df['zh_num_tokens'].max()])\naxes[1, 0].plot(zh_x, zh_slope_en_zh * zh_x + zh_int_en_zh, 'g-', linewidth=2, alpha=0.8)\naxes[1, 0].set_xlabel('Chinese Token Count')\naxes[1, 0].set_ylabel('EN→ZH BLEU Score')\naxes[1, 0].set_title(f'ZH Tokens vs EN→ZH BLEU\\nr = {pearson_zh_tokens_en_zh:.3f}, p = {p_zh_en_zh:.2e}')\naxes[1, 0].grid(alpha=0.3)\n\naxes[1, 1].scatter(df['zh_num_tokens'], df['bleu_zh_en'], alpha=0.3, s=10, color='green')\nzh_slope_zh_en, zh_int_zh_en, _, _, _ = stats.linregress(df['zh_num_tokens'], df['bleu_zh_en'])\naxes[1, 1].plot(zh_x, zh_slope_zh_en * zh_x + zh_int_zh_en, 'g-', linewidth=2, alpha=0.8)\naxes[1, 1].set_xlabel('Chinese Token Count')\naxes[1, 1].set_ylabel('ZH→EN BLEU Score')\naxes[1, 1].set_title(f'ZH Tokens vs ZH→EN BLEU\\nr = {pearson_zh_tokens_zh_en:.3f}, p = {p_zh_zh_en:.2e}')\naxes[1, 1].grid(alpha=0.3)\n\naxes[1, 2].scatter(df['zh_num_tokens'], df['bleu_avg'], alpha=0.3, s=10, color='green')\nzh_slope_avg, zh_int_avg, _, _, _ = stats.linregress(df['zh_num_tokens'], df['bleu_avg'])\naxes[1, 2].plot(zh_x, zh_slope_avg * zh_x + zh_int_avg, 'g-', linewidth=2, alpha=0.8)\naxes[1, 2].set_xlabel('Chinese Token Count')\naxes[1, 2].set_ylabel('Average BLEU Score')\naxes[1, 2].set_title(f'ZH Tokens vs Avg BLEU\\nr = {pearson_zh_tokens_avg:.3f}, p = {p_zh_avg:.2e}')\naxes[1, 2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8.6. Partial Correlation: Wasserstein vs BLEU (Controlling for Token Count)\n",
    "\n",
    "Since both Wasserstein distance and BLEU are correlated with token counts, we need to compute **partial correlation** to determine if the Wasserstein-BLEU relationship is genuine or spurious.\n",
    "\n",
    "**Method**: Use linear regression residuals\n",
    "1. Regress Wasserstein distance on token counts → get residuals\n",
    "2. Regress BLEU on token counts → get residuals  \n",
    "3. Correlate the residuals (= partial correlation, controlling for token count effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare predictor matrix: both English and Chinese token counts\n",
    "X_tokens = df[['en_num_tokens', 'zh_num_tokens']].values\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PARTIAL CORRELATION: Wasserstein vs BLEU (Controlling for Token Counts)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Function to compute partial correlation via residuals\n",
    "def partial_correlation_residuals(y1, y2, X):\n",
    "    \"\"\"\n",
    "    Compute partial correlation between y1 and y2, controlling for X.\n",
    "    \n",
    "    Method: Regress both y1 and y2 on X, then correlate the residuals.\n",
    "    \"\"\"\n",
    "    # Reshape if needed\n",
    "    y1 = np.array(y1).reshape(-1, 1)\n",
    "    y2 = np.array(y2).reshape(-1, 1)\n",
    "    \n",
    "    # Regress y1 on X\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X, y1)\n",
    "    residuals1 = y1.flatten() - model1.predict(X).flatten()\n",
    "    \n",
    "    # Regress y2 on X\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X, y2)\n",
    "    residuals2 = y2.flatten() - model2.predict(X).flatten()\n",
    "    \n",
    "    # Correlate residuals\n",
    "    r, p = pearsonr(residuals1, residuals2)\n",
    "    return r, p, residuals1, residuals2\n",
    "\n",
    "# Compute partial correlations for each BLEU metric\n",
    "print(\"Partial Correlation (controlling for EN and ZH token counts):\")\n",
    "print()\n",
    "\n",
    "# Wasserstein vs EN→ZH BLEU\n",
    "r_partial_en_zh, p_partial_en_zh, w_resid_en_zh, bleu_resid_en_zh = partial_correlation_residuals(\n",
    "    df['wasserstein_distance'], df['bleu_en_zh'], X_tokens\n",
    ")\n",
    "print(f\"Wasserstein vs EN→ZH BLEU:\")\n",
    "print(f\"  Original correlation:  r = {pearson_en_zh:.4f}, p = {p_pearson_en_zh:.2e}\")\n",
    "print(f\"  Partial correlation:   r = {r_partial_en_zh:.4f}, p = {p_partial_en_zh:.2e}\")\n",
    "print()\n",
    "\n",
    "# Wasserstein vs ZH→EN BLEU\n",
    "r_partial_zh_en, p_partial_zh_en, w_resid_zh_en, bleu_resid_zh_en = partial_correlation_residuals(\n",
    "    df['wasserstein_distance'], df['bleu_zh_en'], X_tokens\n",
    ")\n",
    "print(f\"Wasserstein vs ZH→EN BLEU:\")\n",
    "print(f\"  Original correlation:  r = {pearson_zh_en:.4f}, p = {p_pearson_zh_en:.2e}\")\n",
    "print(f\"  Partial correlation:   r = {r_partial_zh_en:.4f}, p = {p_partial_zh_en:.2e}\")\n",
    "print()\n",
    "\n",
    "# Wasserstein vs Average BLEU\n",
    "r_partial_avg, p_partial_avg, w_resid_avg, bleu_resid_avg = partial_correlation_residuals(\n",
    "    df['wasserstein_distance'], df['bleu_avg'], X_tokens\n",
    ")\n",
    "print(f\"Wasserstein vs Average BLEU:\")\n",
    "print(f\"  Original correlation:  r = {pearson_avg:.4f}, p = {p_pearson_avg:.2e}\")\n",
    "print(f\"  Partial correlation:   r = {r_partial_avg:.4f}, p = {p_partial_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "print(\"Interpretation:\")\n",
    "change_en_zh = abs(r_partial_en_zh) - abs(pearson_en_zh)\n",
    "change_zh_en = abs(r_partial_zh_en) - abs(pearson_zh_en)\n",
    "change_avg = abs(r_partial_avg) - abs(pearson_avg)\n",
    "\n",
    "print(f\"  EN→ZH: Correlation changed by {change_en_zh:+.4f} after controlling for token counts\")\n",
    "print(f\"  ZH→EN: Correlation changed by {change_zh_en:+.4f} after controlling for token counts\")\n",
    "print(f\"  Avg:   Correlation changed by {change_avg:+.4f} after controlling for token counts\")\n",
    "print()\n",
    "\n",
    "if abs(change_avg) < 0.05:\n",
    "    print(\"  → Token count has minimal confounding effect. The Wasserstein-BLEU relationship is genuine.\")\n",
    "elif abs(r_partial_avg) < abs(pearson_avg) * 0.5:\n",
    "    print(\"  → Token count is a major confounder. Much of the Wasserstein-BLEU correlation is explained by token count.\")\n",
    "else:\n",
    "    print(\"  → Token count has some confounding effect, but the Wasserstein-BLEU relationship persists.\")\n",
    "\n",
    "# Visualize partial correlation (residuals plot)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→ZH\n",
    "axes[0].scatter(w_resid_en_zh, bleu_resid_en_zh, alpha=0.3, s=10)\n",
    "z = np.polyfit(w_resid_en_zh, bleu_resid_en_zh, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "axes[0].plot(w_resid_en_zh, p_fit(w_resid_en_zh), \"r--\", alpha=0.5)\n",
    "axes[0].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[0].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\n",
    "axes[0].set_ylabel('EN→ZH BLEU\\n(residuals after removing token count effect)')\n",
    "axes[0].set_title(f'Partial Correlation: Wasserstein vs EN→ZH BLEU\\nr_partial = {r_partial_en_zh:.3f}, p = {p_partial_en_zh:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# ZH→EN\n",
    "axes[1].scatter(w_resid_zh_en, bleu_resid_zh_en, alpha=0.3, s=10)\n",
    "z = np.polyfit(w_resid_zh_en, bleu_resid_zh_en, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "axes[1].plot(w_resid_zh_en, p_fit(w_resid_zh_en), \"r--\", alpha=0.5)\n",
    "axes[1].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\n",
    "axes[1].set_ylabel('ZH→EN BLEU\\n(residuals after removing token count effect)')\n",
    "axes[1].set_title(f'Partial Correlation: Wasserstein vs ZH→EN BLEU\\nr_partial = {r_partial_zh_en:.3f}, p = {p_partial_zh_en:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Average\n",
    "axes[2].scatter(w_resid_avg, bleu_resid_avg, alpha=0.3, s=10)\n",
    "z = np.polyfit(w_resid_avg, bleu_resid_avg, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "axes[2].plot(w_resid_avg, p_fit(w_resid_avg), \"r--\", alpha=0.5)\n",
    "axes[2].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[2].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[2].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\n",
    "axes[2].set_ylabel('Average BLEU\\n(residuals after removing token count effect)')\n",
    "axes[2].set_title(f'Partial Correlation: Wasserstein vs Average BLEU\\nr_partial = {r_partial_avg:.3f}, p = {p_partial_avg:.2e}')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Binned Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin by Wasserstein distance and compare mean BLEU scores\n",
    "df['w_bin'] = pd.qcut(df['wasserstein_distance'], q=5, labels=['Very Similar', 'Similar', 'Moderate', 'Dissimilar', 'Very Dissimilar'])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BINNED ANALYSIS: Mean BLEU by Topological Similarity\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "binned_stats = df.groupby('w_bin').agg({\n",
    "    'bleu_avg': ['mean', 'std', 'count'],\n",
    "    'wasserstein_distance': ['mean', 'std']\n",
    "})\n",
    "\n",
    "print(binned_stats)\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bin_means = df.groupby('w_bin')['bleu_avg'].mean()\n",
    "bin_stds = df.groupby('w_bin')['bleu_avg'].std()\n",
    "\n",
    "ax.bar(range(len(bin_means)), bin_means, yerr=bin_stds, capsize=5, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(range(len(bin_means)))\n",
    "ax.set_xticklabels(bin_means.index, rotation=45, ha='right')\n",
    "ax.set_xlabel('Topological Similarity (Wasserstein Distance Bins)')\n",
    "ax.set_ylabel('Mean BLEU Score')\n",
    "ax.set_title('Translation Quality by Topological Similarity')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. Examples: High Similarity vs Low Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": "# Sort by Wasserstein distance\ndf_sorted = df.sort_values('wasserstein_distance')\n\nprint(\"=\" * 70)\nprint(\"HIGH TOPOLOGICAL SIMILARITY (Low Wasserstein Distance)\")\nprint(\"=\" * 70)\nfor i in range(5):\n    row = df_sorted.iloc[i]\n    print(f\"\\n[{i+1}] Pair {int(row['idx'])}: W = {row['wasserstein_distance']:.4f}, BLEU = {row['bleu_avg']:.2f}\")\n    \n    # Get original text from TDA results\n    original = tda_results[int(row['idx'])]\n    print(f\"    EN: {original['en_text']}\")\n    print(f\"    ZH: {original['zh_text']}\")\n    print(f\"    Generated ZH: {original['en_translation']}\")\n    print(f\"    Generated EN: {original['zh_translation']}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"LOW TOPOLOGICAL SIMILARITY (High Wasserstein Distance)\")\nprint(\"=\" * 70)\nfor i in range(5):\n    row = df_sorted.iloc[-(i+1)]\n    print(f\"\\n[{i+1}] Pair {int(row['idx'])}: W = {row['wasserstein_distance']:.4f}, BLEU = {row['bleu_avg']:.2f}\")\n    \n    original = tda_results[int(row['idx'])]\n    print(f\"    EN: {original['en_text']}\")\n    print(f\"    ZH: {original['zh_text']}\")\n    print(f\"    Generated ZH: {original['en_translation']}\")\n    print(f\"    Generated EN: {original['zh_translation']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **TDA-BLEU Correlation Analysis Complete!**\n",
    "\n",
    "**Key Findings:**\n",
    "- Correlation between topological similarity (Wasserstein distance) and translation quality (BLEU)\n",
    "- H0 vs H1 contribution to the relationship\n",
    "- Token count confounding analysis via partial correlation\n",
    "- Binned analysis showing trend across similarity levels\n",
    "\n",
    "**Hypothesis Test:**\n",
    "Does lower Wasserstein distance (more topologically similar attention patterns) predict higher BLEU scores (better translation quality)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}