{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Extracted Attention Maps\n",
    "\n",
    "Load and verify the extracted encoder attention maps from all 2000 sentence pairs.\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path in code_zh_en\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell and run from \"Import Libraries\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only needed for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # IMPORTANT: Set this to your code_zh_en directory path\n",
    "    # This should point to where THIS notebook is located\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_zh_en\"\n",
    "    \n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"✓ Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab, using local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify working directory and required files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check data file\n",
    "data_path = \"../data/attention_maps_zh_en/all_encoder_attention_last_layer.pkl\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"✓ Data file exists: {data_path}\")\n",
    "    print(f\"  File size: {Path(data_path).stat().st_size / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(f\"✗ Data file NOT found: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\n# Configure matplotlib for Chinese font support\n# Check if running on Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\nif IN_COLAB:\n    # On Colab: Download and configure SimHei font for Chinese support\n    import urllib.request\n    import matplotlib\n    import matplotlib.font_manager as fm\n    import shutil\n    import os\n    \n    # Download SimHei font from a reliable source\n    font_url = \"https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf\"\n    font_path = \"/usr/share/fonts/truetype/SimHei.ttf\"\n    \n    if not os.path.exists(font_path):\n        print(\"Downloading SimHei font...\")\n        try:\n            urllib.request.urlretrieve(font_url, font_path)\n            print(f\"✓ Downloaded font to {font_path}\")\n        except Exception as e:\n            print(f\"⚠ Download failed: {e}\")\n            # Try alternative source\n            font_url = \"https://github.com/kosho2013/DFPlayer/raw/master/SimHei.ttf\"\n            try:\n                urllib.request.urlretrieve(font_url, font_path)\n                print(f\"✓ Downloaded from alternative source\")\n            except Exception as e2:\n                print(f\"⚠ Alternative download also failed: {e2}\")\n    else:\n        print(f\"✓ Font already exists at {font_path}\")\n    \n    # Clear matplotlib cache\n    cache_dir = matplotlib.get_cachedir()\n    if os.path.exists(cache_dir):\n        shutil.rmtree(cache_dir)\n        print(\"✓ Cleared matplotlib cache\")\n    \n    # Register the font if it exists\n    if os.path.exists(font_path):\n        fm.fontManager.addfont(font_path)\n        print(\"✓ Font registered with matplotlib\")\n        \n    # Set SimHei as default font\n    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n    plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display\n    \n    print(\"✓ Font configuration complete: SimHei\")\n    \nelse:\n    # On local: use Arial Unicode MS (macOS)\n    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']\n    plt.rcParams['axes.unicode_minus'] = False\n    print(\"✓ Running locally: using Arial Unicode MS for Chinese support\")\n\nprint(\"Libraries loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Attention Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted attention maps\n",
    "data_path = Path(\"../data/attention_maps_zh_en/all_encoder_attention_last_layer.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "print(f\"File size: {data_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "print()\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first result\n",
    "sample = results[0]\n",
    "\n",
    "print(\"Data structure for each sentence pair:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"{key:20s}: {type(value).__name__:15s} shape {value.shape}, dtype {value.dtype}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key:20s}: {type(value).__name__:15s} length {len(value)}\")\n",
    "    else:\n",
    "        print(f\"{key:20s}: {type(value).__name__:15s}\")\n",
    "\n",
    "print()\n",
    "print(\"Sample content:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Index: {sample['idx']}\")\n",
    "print(f\"English: {sample['en_text']}\")\n",
    "print(f\"Chinese: {sample['zh_text']}\")\n",
    "print(f\"\\nEnglish tokens ({len(sample['en_tokens'])}): {sample['en_tokens']}\")\n",
    "print(f\"Chinese tokens ({len(sample['zh_tokens'])}): {sample['zh_tokens']}\")\n",
    "print(f\"\\nEnglish → Chinese translation: {sample['en_translation']}\")\n",
    "print(f\"Chinese → English translation: {sample['zh_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check attention matrix shapes across all samples\n",
    "en_shapes = [r['en_attention'].shape for r in results[:100]]  # Check first 100\n",
    "zh_shapes = [r['zh_attention'].shape for r in results[:100]]\n",
    "\n",
    "# Extract heads (should be consistent)\n",
    "en_heads = [shape[0] for shape in en_shapes]\n",
    "zh_heads = [shape[0] for shape in zh_shapes]\n",
    "\n",
    "print(\"Model Architecture (from attention tensors):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: NLLB-1.3B with 24 encoder layers\")\n",
    "print(f\"Extracted: LAST LAYER ONLY (layer 23)\")\n",
    "print(f\"Number of attention heads: {en_heads[0]} (consistent: {len(set(en_heads)) == 1})\")\n",
    "print()\n",
    "print(f\"Attention shape format: (num_heads, seq_len, seq_len)\")\n",
    "print(f\"Sample English attention: {results[0]['en_attention'].shape}\")\n",
    "print(f\"Sample Chinese attention: {results[0]['zh_attention'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics on sequence lengths\n",
    "en_seq_lens = [r['en_attention'].shape[1] for r in results]  # shape is (num_heads, seq_len, seq_len)\n",
    "zh_seq_lens = [r['zh_attention'].shape[1] for r in results]\n",
    "\n",
    "print(\"Sequence Length Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"English tokens:\")\n",
    "print(f\"  Min:  {min(en_seq_lens)}\")\n",
    "print(f\"  Max:  {max(en_seq_lens)}\")\n",
    "print(f\"  Mean: {np.mean(en_seq_lens):.1f}\")\n",
    "print(f\"  Median: {np.median(en_seq_lens):.1f}\")\n",
    "print()\n",
    "print(f\"Chinese tokens:\")\n",
    "print(f\"  Min:  {min(zh_seq_lens)}\")\n",
    "print(f\"  Max:  {max(zh_seq_lens)}\")\n",
    "print(f\"  Mean: {np.mean(zh_seq_lens):.1f}\")\n",
    "print(f\"  Median: {np.median(zh_seq_lens):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sequence length distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.hist(en_seq_lens, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.axvline(np.mean(en_seq_lens), color='red', linestyle='--', label=f'Mean: {np.mean(en_seq_lens):.1f}')\n",
    "ax1.set_xlabel('Sequence Length (tokens)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('English Sequence Lengths')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.hist(zh_seq_lens, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.axvline(np.mean(zh_seq_lens), color='red', linestyle='--', label=f'Mean: {np.mean(zh_seq_lens):.1f}')\n",
    "ax2.set_xlabel('Sequence Length (tokens)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Chinese Sequence Lengths')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Attention Maps\n",
    "\n",
    "Plot attention maps from different examples and layers to verify correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoder_attention(attention, tokens, head=0, title=\"Encoder Self-Attention (Last Layer)\", filter_special=True):\n",
    "    \"\"\"\n",
    "    Plot encoder self-attention heatmap from the last layer.\n",
    "    \n",
    "    Args:\n",
    "        attention: Attention weights (num_heads, seq_len, seq_len) - LAST LAYER ONLY\n",
    "        tokens: List of token strings\n",
    "        head: Which attention head to visualize\n",
    "        title: Plot title\n",
    "        filter_special: Whether to filter out special tokens\n",
    "    \"\"\"\n",
    "    # Extract specified head (no layer dimension since we only have last layer)\n",
    "    attn = attention[head]  # (seq_len, seq_len)\n",
    "    \n",
    "    # Filter special tokens if requested\n",
    "    if filter_special:\n",
    "        # Keep only content tokens (filter out special tokens and language tags)\n",
    "        special_tokens = {'</s>', '<s>', '<pad>', 'eng_Latn', 'zho_Hans'}\n",
    "        content_mask = [tok not in special_tokens for tok in tokens]\n",
    "        \n",
    "        if sum(content_mask) > 0:  # Only filter if there are content tokens\n",
    "            attn = attn[content_mask][:, content_mask]\n",
    "            tokens = [tok for tok, keep in zip(tokens, content_mask) if keep]\n",
    "            \n",
    "            # Renormalize attention weights after filtering\n",
    "            attn = attn / attn.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        square=True\n",
    "    )\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.title(f\"{title}\\nHead {head}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: First sentence pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first example\n",
    "idx = 0\n",
    "example = results[idx]\n",
    "\n",
    "print(f\"Example {idx}:\")\n",
    "print(f\"English: {example['en_text']}\")\n",
    "print(f\"Chinese: {example['zh_text']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot English encoder attention - head 0\n",
    "plot_encoder_attention(\n",
    "    attention=example['en_attention'],\n",
    "    tokens=example['en_tokens'],\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot English encoder attention - head 8\n",
    "plot_encoder_attention(\n",
    "    attention=example['en_attention'],\n",
    "    tokens=example['en_tokens'],\n",
    "    head=8,\n",
    "    title=f\"English Encoder Attention - Head 8 (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Chinese encoder attention - head 0\n",
    "plot_encoder_attention(\n",
    "    attention=example['zh_attention'],\n",
    "    tokens=example['zh_tokens'],\n",
    "    head=0,\n",
    "    title=f\"Chinese Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Attention Properties\n",
    "\n",
    "Check that attention weights have expected properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few examples for attention properties\n",
    "print(\"Verifying attention weight properties:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in [0, 100, 500, 1000, 1999]:\n",
    "    example_check = results[i]\n",
    "    en_attn = example_check['en_attention']\n",
    "    zh_attn = example_check['zh_attention']\n",
    "    \n",
    "    # Check that attention weights sum to ~1 along last dimension (softmax property)\n",
    "    en_sums = en_attn.sum(axis=-1)  # Sum over keys for each query\n",
    "    zh_sums = zh_attn.sum(axis=-1)\n",
    "    \n",
    "    # Use atol=1e-3 for float32 precision (typical deviations ~3e-4)\n",
    "    en_sum_ok = np.allclose(en_sums, 1.0, atol=1e-3)\n",
    "    zh_sum_ok = np.allclose(zh_sums, 1.0, atol=1e-3)\n",
    "    \n",
    "    # Check that all values are in [0, 1]\n",
    "    en_range_ok = (en_attn >= 0).all() and (en_attn <= 1).all()\n",
    "    zh_range_ok = (zh_attn >= 0).all() and (zh_attn <= 1).all()\n",
    "    \n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  EN - Sums to 1: {en_sum_ok}, Range [0,1]: {en_range_ok}\")\n",
    "    print(f\"  ZH - Sums to 1: {zh_sum_ok}, Range [0,1]: {zh_range_ok}\")\n",
    "\n",
    "print()\n",
    "print(\"✓ All attention weights have correct properties!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Data successfully loaded and verified!**\n",
    "\n",
    "- Loaded 2000 sentence pairs\n",
    "- Each pair has English and Chinese encoder attention from **last layer only (layer 23 out of 24)**\n",
    "- Model: NLLB-1.3B with 24 encoder layers, 16 attention heads per layer\n",
    "- Attention matrices have correct shape: **(16 heads, seq_len, seq_len)** - last layer only\n",
    "- Attention weights sum to 1 (softmax property)\n",
    "- Visualizations show expected patterns\n",
    "- File size: ~300-400 MB (24x smaller than storing all layers)\n",
    "\n",
    "**Next steps:**\n",
    "1. Build attention graphs (tokens as nodes, weights as edges)\n",
    "2. Compute persistent homology (β₀, β₁) using last layer attention\n",
    "3. Compare topological structure across languages\n",
    "4. Correlate with translation quality (BLEU scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}