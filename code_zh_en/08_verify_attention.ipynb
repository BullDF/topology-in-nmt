{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Extracted Attention Maps\n",
    "\n",
    "Load and verify the extracted encoder attention maps from all 2000 sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport seaborn as sns\nfrom pathlib import Path\n\n# Configure matplotlib for Chinese font support\nmatplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']\nmatplotlib.rcParams['axes.unicode_minus'] = False\n\n# Force matplotlib to rebuild font cache if needed\ntry:\n    fm._rebuild()\nexcept:\n    pass\n\nprint(\"Libraries loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Attention Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted attention maps\n",
    "data_path = Path(\"../data/attention_maps_zh_en/all_encoder_attention.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "print(f\"File size: {data_path.stat().st_size / (1024**3):.2f} GB\")\n",
    "print()\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first result\n",
    "sample = results[0]\n",
    "\n",
    "print(\"Data structure for each sentence pair:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"{key:20s}: {type(value).__name__:15s} shape {value.shape}, dtype {value.dtype}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key:20s}: {type(value).__name__:15s} length {len(value)}\")\n",
    "    else:\n",
    "        print(f\"{key:20s}: {type(value).__name__:15s}\")\n",
    "\n",
    "print()\n",
    "print(\"Sample content:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Index: {sample['idx']}\")\n",
    "print(f\"English: {sample['en_text']}\")\n",
    "print(f\"Chinese: {sample['zh_text']}\")\n",
    "print(f\"\\nEnglish tokens ({len(sample['en_tokens'])}): {sample['en_tokens']}\")\n",
    "print(f\"Chinese tokens ({len(sample['zh_tokens'])}): {sample['zh_tokens']}\")\n",
    "print(f\"\\nEnglish → Chinese translation: {sample['en_translation']}\")\n",
    "print(f\"Chinese → English translation: {sample['zh_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check attention matrix shapes across all samples\n",
    "en_shapes = [r['en_attention'].shape for r in results[:100]]  # Check first 100\n",
    "zh_shapes = [r['zh_attention'].shape for r in results[:100]]\n",
    "\n",
    "# Extract layers and heads (should be consistent)\n",
    "en_layers = [shape[0] for shape in en_shapes]\n",
    "en_heads = [shape[1] for shape in en_shapes]\n",
    "\n",
    "print(\"Model Architecture (from attention tensors):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of encoder layers: {en_layers[0]} (consistent: {len(set(en_layers)) == 1})\")\n",
    "print(f\"Number of attention heads: {en_heads[0]} (consistent: {len(set(en_heads)) == 1})\")\n",
    "print()\n",
    "print(f\"Attention shape format: (num_layers, num_heads, seq_len, seq_len)\")\n",
    "print(f\"Sample English attention: {results[0]['en_attention'].shape}\")\n",
    "print(f\"Sample Chinese attention: {results[0]['zh_attention'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics on sequence lengths\n",
    "en_seq_lens = [r['en_attention'].shape[2] for r in results]\n",
    "zh_seq_lens = [r['zh_attention'].shape[2] for r in results]\n",
    "\n",
    "print(\"Sequence Length Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"English tokens:\")\n",
    "print(f\"  Min:  {min(en_seq_lens)}\")\n",
    "print(f\"  Max:  {max(en_seq_lens)}\")\n",
    "print(f\"  Mean: {np.mean(en_seq_lens):.1f}\")\n",
    "print(f\"  Median: {np.median(en_seq_lens):.1f}\")\n",
    "print()\n",
    "print(f\"Chinese tokens:\")\n",
    "print(f\"  Min:  {min(zh_seq_lens)}\")\n",
    "print(f\"  Max:  {max(zh_seq_lens)}\")\n",
    "print(f\"  Mean: {np.mean(zh_seq_lens):.1f}\")\n",
    "print(f\"  Median: {np.median(zh_seq_lens):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sequence length distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.hist(en_seq_lens, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.axvline(np.mean(en_seq_lens), color='red', linestyle='--', label=f'Mean: {np.mean(en_seq_lens):.1f}')\n",
    "ax1.set_xlabel('Sequence Length (tokens)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('English Sequence Lengths')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.hist(zh_seq_lens, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.axvline(np.mean(zh_seq_lens), color='red', linestyle='--', label=f'Mean: {np.mean(zh_seq_lens):.1f}')\n",
    "ax2.set_xlabel('Sequence Length (tokens)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Chinese Sequence Lengths')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Attention Maps\n",
    "\n",
    "Plot attention maps from different examples and layers to verify correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoder_attention(attention, tokens, layer=0, head=0, title=\"Encoder Self-Attention\", filter_special=True):\n",
    "    \"\"\"\n",
    "    Plot encoder self-attention heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention: Attention weights (num_layers, num_heads, seq_len, seq_len)\n",
    "        tokens: List of token strings\n",
    "        layer: Which layer to visualize\n",
    "        head: Which attention head to visualize\n",
    "        title: Plot title\n",
    "        filter_special: Whether to filter out special tokens\n",
    "    \"\"\"\n",
    "    # Extract specified layer and head\n",
    "    attn = attention[layer, head]  # (seq_len, seq_len)\n",
    "    \n",
    "    # Filter special tokens if requested\n",
    "    if filter_special:\n",
    "        # Keep only content tokens (filter out special tokens and language tags)\n",
    "        special_tokens = {'</s>', '<s>', '<pad>', 'eng_Latn', 'zho_Hans'}\n",
    "        content_mask = [tok not in special_tokens for tok in tokens]\n",
    "        \n",
    "        if sum(content_mask) > 0:  # Only filter if there are content tokens\n",
    "            attn = attn[content_mask][:, content_mask]\n",
    "            tokens = [tok for tok, keep in zip(tokens, content_mask) if keep]\n",
    "            \n",
    "            # Renormalize attention weights after filtering\n",
    "            attn = attn / attn.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        square=True\n",
    "    )\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.title(f\"{title}\\nLayer {layer}, Head {head}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: First sentence pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first example\n",
    "idx = 0\n",
    "example = results[idx]\n",
    "\n",
    "print(f\"Example {idx}:\")\n",
    "print(f\"English: {example['en_text']}\")\n",
    "print(f\"Chinese: {example['zh_text']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot English encoder attention - first layer\n",
    "plot_encoder_attention(\n",
    "    attention=example['en_attention'],\n",
    "    tokens=example['en_tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot English encoder attention - last layer\n",
    "last_layer = example['en_attention'].shape[0] - 1\n",
    "plot_encoder_attention(\n",
    "    attention=example['en_attention'],\n",
    "    tokens=example['en_tokens'],\n",
    "    layer=last_layer,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention - Last Layer (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Chinese encoder attention - first layer\n",
    "plot_encoder_attention(\n",
    "    attention=example['zh_attention'],\n",
    "    tokens=example['zh_tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"Chinese Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Middle sentence pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select middle example\n",
    "idx = len(results) // 2\n",
    "example = results[idx]\n",
    "\n",
    "print(f\"Example {idx}:\")\n",
    "print(f\"English: {example['en_text']}\")\n",
    "print(f\"Chinese: {example['zh_text']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot English encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=example['en_attention'],\n",
    "    tokens=example['en_tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Chinese encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=example['zh_attention'],\n",
    "    tokens=example['zh_tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"Chinese Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Last sentence pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select last example\n",
    "idx = len(results) - 1\n",
    "example = results[idx]\n",
    "\n",
    "print(f\"Example {idx}:\")\n",
    "print(f\"English: {example['en_text']}\")\n",
    "print(f\"Chinese: {example['zh_text']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot English encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=example['en_attention'],\n",
    "    tokens=example['en_tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Chinese encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=example['zh_attention'],\n",
    "    tokens=example['zh_tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"Chinese Encoder Attention (Example {idx})\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Attention Properties\n",
    "\n",
    "Check that attention weights have expected properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few examples for attention properties\n",
    "print(\"Verifying attention weight properties:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in [0, 100, 500, 1000, 1999]:\n",
    "    example = results[i]\n",
    "    en_attn = example['en_attention']\n",
    "    zh_attn = example['zh_attention']\n",
    "    \n",
    "    # Check that attention weights sum to ~1 along last dimension (softmax property)\n",
    "    en_sums = en_attn.sum(axis=-1)  # Sum over keys for each query\n",
    "    zh_sums = zh_attn.sum(axis=-1)\n",
    "    \n",
    "    en_sum_ok = np.allclose(en_sums, 1.0, atol=1e-5)\n",
    "    zh_sum_ok = np.allclose(zh_sums, 1.0, atol=1e-5)\n",
    "    \n",
    "    # Check that all values are in [0, 1]\n",
    "    en_range_ok = (en_attn >= 0).all() and (en_attn <= 1).all()\n",
    "    zh_range_ok = (zh_attn >= 0).all() and (zh_attn <= 1).all()\n",
    "    \n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  EN - Sums to 1: {en_sum_ok}, Range [0,1]: {en_range_ok}\")\n",
    "    print(f\"  ZH - Sums to 1: {zh_sum_ok}, Range [0,1]: {zh_range_ok}\")\n",
    "\n",
    "print()\n",
    "print(\"✓ All attention weights have correct properties!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Data successfully loaded and verified!**\n",
    "\n",
    "- Loaded 2000 sentence pairs\n",
    "- Each pair has English and Chinese encoder attention\n",
    "- Attention matrices have correct shape: (12 layers, 16 heads, seq_len, seq_len)\n",
    "- Attention weights sum to 1 (softmax property)\n",
    "- Visualizations show expected patterns\n",
    "\n",
    "**Next steps:**\n",
    "1. Build attention graphs (tokens as nodes, weights as edges)\n",
    "2. Compute persistent homology (β₀, β₁)\n",
    "3. Compare topological structure across languages\n",
    "4. Correlate with translation quality (BLEU scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}