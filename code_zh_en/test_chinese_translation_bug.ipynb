{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Chinese Translation Truncation Issue\n",
    "\n",
    "**Problem:** Chinese translations are being truncated at the first comma (，)\n",
    "\n",
    "**Goal:** Diagnose whether the issue is in:\n",
    "1. Model generation (stopping too early)\n",
    "2. Token decoding (decode function issue)\n",
    "3. Tokenizer configuration (comma treated as EOS)\n",
    "\n",
    "**Test cases from user:**\n",
    "- Pair 702: EN → ZH truncates at comma\n",
    "- Pair 1107: EN → ZH truncates at comma  \n",
    "- Pair 616: EN → ZH truncates at comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_dir = \"../models/nllb-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_dir,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases from User Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences from user\n",
    "test_cases = [\n",
    "    {\n",
    "        'id': 702,\n",
    "        'en': \"At 10:00pm, Sun Yijie, who had been pregnant for four months, was released on bail of NT$200,000.\",\n",
    "        'zh_reference': \"晚间10时许，怀有4月身孕的孙颐婕20万元交保，先行离开。\",\n",
    "        'zh_truncated': \"苏伊杰已经怀孕四个月,\"\n",
    "    },\n",
    "    {\n",
    "        'id': 1107,\n",
    "        'en': \"It is boundless. If you need and are brave enough to initiate crowdfunding, everything will become possible.\",\n",
    "        'zh_reference': \"它没有界限，只要你有需求，只要你敢于发起众筹，那么一切皆有可能。\",\n",
    "        'zh_truncated': \"如果您需要并且有勇气启动众筹,\"\n",
    "    },\n",
    "    {\n",
    "        'id': 616,\n",
    "        'en': \"It was Mr. Dong's real intention to sign the \\\"agreement\\\", which was legal and effective after being signed and sealed by all the parties concerned.\",\n",
    "        'zh_reference': \"签订"协议书"是董某的真实意思表示,经过各方签字盖章合法有效。\",\n",
    "        'zh_truncated': \"东先生的真正意图是签署"协议",\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Current Implementation (max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_current(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Test with current parameters (max_length=128)\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 1: Current Implementation (max_length=128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    print(f\"Input text: {text}\")\n",
    "    print(f\"Input tokens: {len(inputs.input_ids[0])} tokens\")\n",
    "    print()\n",
    "    \n",
    "    # Generate with current parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    # Show raw output\n",
    "    print(\"Raw output sequence:\")\n",
    "    print(f\"  Length: {len(outputs[0])} tokens\")\n",
    "    print(f\"  Token IDs: {outputs[0].tolist()[:20]}... (showing first 20)\")\n",
    "    print()\n",
    "    \n",
    "    # Show tokens\n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    print(f\"Output tokens ({len(output_tokens)} total):\")\n",
    "    for i, token in enumerate(output_tokens[:30]):  # Show first 30\n",
    "        print(f\"  [{i}] {repr(token)}\")\n",
    "    if len(output_tokens) > 30:\n",
    "        print(f\"  ... ({len(output_tokens) - 30} more tokens)\")\n",
    "    print()\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    print(f\"Translation length: {len(translation)} characters\")\n",
    "    \n",
    "    # Check for truncation\n",
    "    if translation.endswith(',') or translation.endswith('，'):\n",
    "        print(\"⚠️  WARNING: Translation ends with comma - likely truncated!\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Test first case\n",
    "test_case = test_cases[0]\n",
    "result = test_translation_current(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Increased max_length (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_longer(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Test with increased max_length=256\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 2: Increased max_length (256)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=256\n",
    "        )\n",
    "    \n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.endswith(',') or translation.endswith('，'):\n",
    "        print(\"⚠️  WARNING: Still ends with comma!\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result2 = test_translation_longer(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Use max_new_tokens instead of max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_max_new_tokens(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Test with max_new_tokens=200 (independent of input length)\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 3: max_new_tokens=200 (better for variable input lengths)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    print(f\"Input length: {len(inputs.input_ids[0])} tokens\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200  # Generate up to 200 NEW tokens\n",
    "        )\n",
    "    \n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens (input + new)\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.endswith(',') or translation.endswith('，'):\n",
    "        print(\"⚠️  WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"✓ Translation appears complete (no trailing comma)\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result3 = test_translation_max_new_tokens(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Check for EOS token issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eos_tokens(tokenizer):\n",
    "    \"\"\"\n",
    "    Check what tokens are treated as EOS\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 4: EOS Token Investigation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check EOS token\n",
    "    print(f\"EOS token: {repr(tokenizer.eos_token)}\")\n",
    "    print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Check Chinese comma\n",
    "    chinese_comma = \"，\"\n",
    "    chinese_comma_ids = tokenizer.encode(chinese_comma, add_special_tokens=False)\n",
    "    print(f\"Chinese comma (，):\")\n",
    "    print(f\"  Token IDs: {chinese_comma_ids}\")\n",
    "    print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(chinese_comma_ids)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check English comma\n",
    "    english_comma = \",\"\n",
    "    english_comma_ids = tokenizer.encode(english_comma, add_special_tokens=False)\n",
    "    print(f\"English comma (,):\")\n",
    "    print(f\"  Token IDs: {english_comma_ids}\")\n",
    "    print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(english_comma_ids)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if comma matches EOS\n",
    "    if tokenizer.eos_token_id in chinese_comma_ids:\n",
    "        print(\"⚠️  CRITICAL: Chinese comma contains EOS token ID!\")\n",
    "    elif tokenizer.eos_token_id in english_comma_ids:\n",
    "        print(\"⚠️  CRITICAL: English comma contains EOS token ID!\")\n",
    "    else:\n",
    "        print(\"✓ Commas do not match EOS token ID\")\n",
    "    \n",
    "    # Check language tokens\n",
    "    print()\n",
    "    print(\"Language tokens:\")\n",
    "    print(f\"  eng_Latn: {tokenizer.convert_tokens_to_ids('eng_Latn')}\")\n",
    "    print(f\"  zho_Hans: {tokenizer.convert_tokens_to_ids('zho_Hans')}\")\n",
    "\n",
    "test_eos_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Disable early stopping and add generation constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_no_early_stop(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Test with early_stopping disabled and other constraints\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 5: Disable early stopping + explicit constraints\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1,  # Greedy decoding\n",
    "            do_sample=False,  # No sampling\n",
    "            early_stopping=False,  # Don't stop early\n",
    "        )\n",
    "    \n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens\")\n",
    "    print(f\"Last 5 tokens: {output_tokens[-5:]}\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.endswith(',') or translation.endswith('，'):\n",
    "        print(\"⚠️  WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"✓ Translation appears complete\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result5 = test_translation_no_early_stop(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All Cases with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING ALL CASES WITH BEST PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for test_case in test_cases:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case {test_case['id']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"English: {test_case['en']}\")\n",
    "    print(f\"Reference Chinese: {test_case['zh_reference']}\")\n",
    "    print(f\"Previous (truncated): {test_case['zh_truncated']}\")\n",
    "    print()\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(test_case['en'], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            early_stopping=False\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"New translation: {translation}\")\n",
    "    print(f\"Length: {len(outputs[0])} tokens, {len(translation)} characters\")\n",
    "    \n",
    "    # Compare\n",
    "    if translation == test_case['zh_truncated']:\n",
    "        print(\"❌ Still truncated (identical to before)\")\n",
    "    elif len(translation) > len(test_case['zh_truncated']):\n",
    "        print(f\"✓ IMPROVED: {len(translation) - len(test_case['zh_truncated'])} more characters\")\n",
    "    else:\n",
    "        print(\"❓ Different but not necessarily better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Tests performed:\")\n",
    "print(\"1. Current implementation (max_length=128)\")\n",
    "print(\"2. Increased max_length (256)\")\n",
    "print(\"3. Using max_new_tokens instead of max_length\")\n",
    "print(\"4. EOS token investigation\")\n",
    "print(\"5. Disabled early stopping with constraints\")\n",
    "print()\n",
    "print(\"RECOMMENDED FIX for notebook 07:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Replace:\")\n",
    "print(\"    outputs = model.generate(\")\n",
    "print(\"        **inputs,\")\n",
    "print(\"        forced_bos_token_id=tgt_lang_id,\")\n",
    "print(\"        output_attentions=True,\")\n",
    "print(\"        return_dict_in_generate=True,\")\n",
    "print(\"        max_length=128\")\n",
    "print(\"    )\")\n",
    "print()\n",
    "print(\"With:\")\n",
    "print(\"    outputs = model.generate(\")\n",
    "print(\"        **inputs,\")\n",
    "print(\"        forced_bos_token_id=tgt_lang_id,\")\n",
    "print(\"        output_attentions=True,\")\n",
    "print(\"        return_dict_in_generate=True,\")\n",
    "print(\"        max_new_tokens=200,  # Allow 200 new tokens regardless of input length\")\n",
    "print(\"        num_beams=1,         # Greedy decoding\")\n",
    "print(\"        do_sample=False,     # Deterministic\")\n",
    "print(\"        early_stopping=False # Don't stop prematurely\")\n",
    "print(\"    )\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"⚠️  NOTE: After fixing, you will need to re-run notebook 07 to regenerate\")\n",
    "print(\"    all translations for the 2000 sentence pairs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
