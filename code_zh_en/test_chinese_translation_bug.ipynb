{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Chinese Translation Truncation Issue\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell\n",
    "\n",
    "---\n",
    "\n",
    "**Problem:** Chinese translations truncate at first comma\n",
    "\n",
    "**Goal:** Diagnose the root cause\n",
    "\n",
    "**Test cases:** Pairs 702, 1107, 616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_zh_en\"\n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths\n",
    "import os\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Model exists: {os.path.exists('../models/nllb-1.3B')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_dir = \"../models/nllb-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir, attn_implementation=\"eager\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_cases = [\n",
    "    {'id': 702, 'en': \"At 10:00pm, Sun Yijie, who had been pregnant for four months, was released on bail of NT$200,000.\"},\n",
    "    {'id': 1107, 'en': \"It is boundless. If you need and are brave enough to initiate crowdfunding, everything will become possible.\"},\n",
    "    {'id': 616, 'en': \"It was Mr. Dong's real intention to sign the agreement, which was legal and effective after being signed and sealed by all the parties concerned.\"}\n",
    "]\n",
    "print(f\"Loaded {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRITICAL DIAGNOSTIC: Token-by-Token Analysis\n",
    "\n",
    "This will show exactly what the model generates and where it stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep diagnostic on first test case\n",
    "test_case = test_cases[0]\n",
    "\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "inputs = tokenizer(test_case['en'], return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"English: {test_case['en']}\")\n",
    "print(f\"Input tokens: {len(inputs.input_ids[0])}\")\n",
    "print()\n",
    "\n",
    "# Generate with max_new_tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "        max_new_tokens=200,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOKEN-BY-TOKEN GENERATION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_ids = outputs.sequences[0].tolist()\n",
    "output_tokens = tokenizer.convert_ids_to_tokens(outputs.sequences[0])\n",
    "\n",
    "for i, (token_id, token) in enumerate(zip(output_ids, output_tokens)):\n",
    "    is_eos = (token_id == tokenizer.eos_token_id)\n",
    "    marker = \" <-- EOS TOKEN\" if is_eos else \"\"\n",
    "    print(f\"[{i:3d}] ID={token_id:6d} Token={repr(token):30s}{marker}\")\n",
    "    if is_eos:\n",
    "        print(f\"\\n*** Generation stopped at position {i} ***\")\n",
    "        break\n",
    "\n",
    "print()\n",
    "decoded = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Length: {len(decoded)} chars, {len(outputs.sequences[0])} tokens\")\n",
    "print(f\"Ends with comma: {decoded.rstrip().endswith(',')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Reverse Direction: ZH→EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if ZH→EN has the same problem\n",
    "chinese_test = \"晚间10时许，怀有4月身孕的孙颐婕20万元交保，先行离开。\"\n",
    "\n",
    "print(f\"Chinese input: {chinese_test}\")\n",
    "print()\n",
    "\n",
    "tokenizer.src_lang = \"zho_Hans\"\n",
    "inputs = tokenizer(chinese_test, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"),\n",
    "        max_new_tokens=200\n",
    "    )\n",
    "\n",
    "translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"English output: {translation}\")\n",
    "print(f\"Length: {len(translation)} chars, {len(outputs[0])} tokens\")\n",
    "print()\n",
    "print(\"Does ZH→EN work correctly or also truncate?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All Parameter Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different parameter combinations\n",
    "configs = [\n",
    "    {'name': 'Original', 'params': {'max_length': 128}},\n",
    "    {'name': 'Longer max_length', 'params': {'max_length': 256}},\n",
    "    {'name': 'max_new_tokens', 'params': {'max_new_tokens': 200}},\n",
    "    {'name': 'max_new_tokens + constraints', 'params': {'max_new_tokens': 200, 'num_beams': 1, 'do_sample': False, 'early_stopping': False}}\n",
    "]\n",
    "\n",
    "test_case = test_cases[0]\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "inputs = tokenizer(test_case['en'], return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Testing: {test_case['en'][:60]}...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for config in configs:\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            **config['params']\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"{config['name']:30s}: {translation[:80]}...\" if len(translation) > 80 else f\"{config['name']:30s}: {translation}\")\n",
    "    print(f\"{'':30s}  ({len(outputs[0])} tokens, {len(translation)} chars)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check EOS Token Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizer configuration:\")\n",
    "print(f\"EOS token: {repr(tokenizer.eos_token)}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print()\n",
    "\n",
    "# Check Chinese comma\n",
    "ch_comma = \"\\uff0c\"\n",
    "ch_ids = tokenizer.encode(ch_comma, add_special_tokens=False)\n",
    "print(f\"Chinese comma (\\uff0c): IDs={ch_ids}, Tokens={tokenizer.convert_ids_to_tokens(ch_ids)}\")\n",
    "\n",
    "# Check English comma\n",
    "en_comma = \",\"\n",
    "en_ids = tokenizer.encode(en_comma, add_special_tokens=False)\n",
    "print(f\"English comma (,): IDs={en_ids}, Tokens={tokenizer.convert_ids_to_tokens(en_ids)}\")\n",
    "print()\n",
    "\n",
    "if tokenizer.eos_token_id in ch_ids or tokenizer.eos_token_id in en_ids:\n",
    "    print(\"WARNING: Comma includes EOS token!\")\n",
    "else:\n",
    "    print(\"Commas do not match EOS token ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Based on the diagnostics above, the root cause should be visible in the token-by-token output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY QUESTIONS TO ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"1. In the token-by-token output, does EOS appear right after the comma?\")\n",
    "print(\"2. Does the ZH→EN translation work correctly?\")\n",
    "print(\"3. Do any parameter combinations produce longer outputs?\")\n",
    "print()\n",
    "print(\"If EOS appears after the comma, this is a model/tokenizer bug.\")\n",
    "print(\"If ZH→EN works but EN→ZH doesn't, it's specific to Chinese generation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
