{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Chinese Translation Truncation Issue\n",
    "\n",
    "**Problem:** Chinese translations are being truncated at the first comma\n",
    "\n",
    "**Goal:** Diagnose whether the issue is in:\n",
    "1. Model generation (stopping too early)\n",
    "2. Token decoding (decode function issue)\n",
    "3. Tokenizer configuration (comma treated as EOS)\n",
    "\n",
    "**Test cases:** Pairs 702, 1107, 616 from user examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_dir = \"../models/nllb-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_dir,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases from User Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences from user (simplified to avoid quote escaping issues)\n",
    "test_cases = [\n",
    "    {\n",
    "        'id': 702,\n",
    "        'en': \"At 10:00pm, Sun Yijie, who had been pregnant for four months, was released on bail of NT$200,000.\"\n",
    "    },\n",
    "    {\n",
    "        'id': 1107,\n",
    "        'en': \"It is boundless. If you need and are brave enough to initiate crowdfunding, everything will become possible.\"\n",
    "    },\n",
    "    {\n",
    "        'id': 616,\n",
    "        'en': \"It was Mr. Dong's real intention to sign the agreement, which was legal and effective after being signed and sealed by all the parties concerned.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Current Implementation (max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_current(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 1: Current Implementation (max_length=128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    print(f\"Input text: {text}\")\n",
    "    print(f\"Input tokens: {len(inputs.input_ids[0])} tokens\")\n",
    "    print()\n",
    "    \n",
    "    # Generate with current parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    # Show raw output\n",
    "    print(\"Raw output sequence:\")\n",
    "    print(f\"  Length: {len(outputs[0])} tokens\")\n",
    "    print(f\"  Token IDs (first 20): {outputs[0].tolist()[:20]}\")\n",
    "    print()\n",
    "    \n",
    "    # Show tokens\n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    print(f\"Output tokens ({len(output_tokens)} total):\")\n",
    "    for i, token in enumerate(output_tokens[:30]):\n",
    "        print(f\"  [{i}] {repr(token)}\")\n",
    "    if len(output_tokens) > 30:\n",
    "        print(f\"  ... ({len(output_tokens) - 30} more tokens)\")\n",
    "    print()\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    print(f\"Translation length: {len(translation)} characters\")\n",
    "    \n",
    "    # Check for truncation (ends with comma)\n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Translation ends with comma - likely truncated!\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Test first case\n",
    "test_case = test_cases[0]\n",
    "result = test_translation_current(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Increased max_length (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_longer(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 2: Increased max_length (256)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=256\n",
    "        )\n",
    "    \n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"OK: Does not end with comma\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result2 = test_translation_longer(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Use max_new_tokens instead of max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_max_new_tokens(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 3: max_new_tokens=200 (better for variable input lengths)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    print(f\"Input length: {len(inputs.input_ids[0])} tokens\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200\n",
    "        )\n",
    "    \n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens (input + new)\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"OK: Translation appears complete\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result3 = test_translation_max_new_tokens(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Check for EOS token issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eos_tokens(tokenizer):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 4: EOS Token Investigation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check EOS token\n",
    "    print(f\"EOS token: {repr(tokenizer.eos_token)}\")\n",
    "    print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Check Chinese comma (full-width)\n",
    "    chinese_comma = \"\\uff0c\"  # Unicode for full-width comma\n",
    "    chinese_comma_ids = tokenizer.encode(chinese_comma, add_special_tokens=False)\n",
    "    print(f\"Chinese comma (full-width):\")\n",
    "    print(f\"  Character: {chinese_comma}\")\n",
    "    print(f\"  Token IDs: {chinese_comma_ids}\")\n",
    "    print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(chinese_comma_ids)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check English comma\n",
    "    english_comma = \",\"\n",
    "    english_comma_ids = tokenizer.encode(english_comma, add_special_tokens=False)\n",
    "    print(f\"English comma:\")\n",
    "    print(f\"  Token IDs: {english_comma_ids}\")\n",
    "    print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(english_comma_ids)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if comma matches EOS\n",
    "    if tokenizer.eos_token_id in chinese_comma_ids:\n",
    "        print(\"CRITICAL: Chinese comma contains EOS token ID!\")\n",
    "    elif tokenizer.eos_token_id in english_comma_ids:\n",
    "        print(\"CRITICAL: English comma contains EOS token ID!\")\n",
    "    else:\n",
    "        print(\"OK: Commas do not match EOS token ID\")\n",
    "    \n",
    "    # Check language tokens\n",
    "    print()\n",
    "    print(\"Language tokens:\")\n",
    "    print(f\"  eng_Latn: {tokenizer.convert_tokens_to_ids('eng_Latn')}\")\n",
    "    print(f\"  zho_Hans: {tokenizer.convert_tokens_to_ids('zho_Hans')}\")\n",
    "\n",
    "test_eos_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Disable early stopping and add generation constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_no_early_stop(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 5: Disable early stopping + explicit constraints\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            early_stopping=False\n",
    "        )\n",
    "    \n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens\")\n",
    "    print(f\"Last 5 tokens: {output_tokens[-5:]}\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"OK: Translation appears complete\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result5 = test_translation_no_early_stop(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All Cases with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING ALL CASES WITH BEST PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for test_case in test_cases:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case {test_case['id']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"English: {test_case['en']}\")\n",
    "    print()\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(test_case['en'], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Current implementation\n",
    "    with torch.no_grad():\n",
    "        outputs_old = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=128\n",
    "        )\n",
    "    translation_old = tokenizer.decode(outputs_old[0], skip_special_tokens=True)\n",
    "    \n",
    "    # New implementation\n",
    "    with torch.no_grad():\n",
    "        outputs_new = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            early_stopping=False\n",
    "        )\n",
    "    translation_new = tokenizer.decode(outputs_new[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"OLD (max_length=128):   {translation_old}\")\n",
    "    print(f\"NEW (max_new_tokens=200): {translation_new}\")\n",
    "    print()\n",
    "    print(f\"OLD length: {len(outputs_old[0])} tokens, {len(translation_old)} chars\")\n",
    "    print(f\"NEW length: {len(outputs_new[0])} tokens, {len(translation_new)} chars\")\n",
    "    \n",
    "    # Compare\n",
    "    if len(translation_new) > len(translation_old):\n",
    "        print(f\"IMPROVED: +{len(translation_new) - len(translation_old)} characters\")\n",
    "    elif translation_new == translation_old:\n",
    "        print(\"NO CHANGE\")\n",
    "    else:\n",
    "        print(\"DIFFERENT (not necessarily better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Tests performed:\")\n",
    "print(\"1. Current implementation (max_length=128)\")\n",
    "print(\"2. Increased max_length (256)\")\n",
    "print(\"3. Using max_new_tokens instead of max_length\")\n",
    "print(\"4. EOS token investigation\")\n",
    "print(\"5. Disabled early stopping with constraints\")\n",
    "print()\n",
    "print(\"RECOMMENDED FIX for notebook 07:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Replace:\")\n",
    "print(\"    outputs = model.generate(\")\n",
    "print(\"        **inputs,\")\n",
    "print(\"        forced_bos_token_id=tgt_lang_id,\")\n",
    "print(\"        output_attentions=True,\")\n",
    "print(\"        return_dict_in_generate=True,\")\n",
    "print(\"        max_length=128\")\n",
    "print(\"    )\")\n",
    "print()\n",
    "print(\"With:\")\n",
    "print(\"    outputs = model.generate(\")\n",
    "print(\"        **inputs,\")\n",
    "print(\"        forced_bos_token_id=tgt_lang_id,\")\n",
    "print(\"        output_attentions=True,\")\n",
    "print(\"        return_dict_in_generate=True,\")\n",
    "print(\"        max_new_tokens=200,\")\n",
    "print(\"        num_beams=1,\")\n",
    "print(\"        do_sample=False,\")\n",
    "print(\"        early_stopping=False\")\n",
    "print(\"    )\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"NOTE: After fixing, you will need to re-run notebook 07 to regenerate\")\n",
    "print(\"      all translations for the 2000 sentence pairs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
