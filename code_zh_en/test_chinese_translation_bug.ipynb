{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Chinese Translation Truncation Issue\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path in code_zh_en\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell and run from Import Libraries\n",
    "\n",
    "---\n",
    "\n",
    "**Problem:** Chinese translations are being truncated at the first comma\n",
    "\n",
    "**Goal:** Diagnose whether the issue is in:\n",
    "1. Model generation (stopping too early)\n",
    "2. Token decoding (decode function issue)\n",
    "3. Tokenizer configuration (comma treated as EOS)\n",
    "\n",
    "**Test cases:** Pairs 702, 1107, 616 from user examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only needed for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # IMPORTANT: Set this to your code_zh_en directory path\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_zh_en\"\n",
    "    \n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab, using local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify working directory and model\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "model_path = \"../models/nllb-1.3B\"\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model directory exists: {model_path}\")\n",
    "else:\n",
    "    print(f\"WARNING: Model directory NOT found: {model_path}\")\n",
    "    print(\"Make sure the model has been downloaded via notebook 03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"../models/nllb-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_dir,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases from User Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences from user\n",
    "test_cases = [\n",
    "    {\n",
    "        'id': 702,\n",
    "        'en': \"At 10:00pm, Sun Yijie, who had been pregnant for four months, was released on bail of NT$200,000.\"\n",
    "    },\n",
    "    {\n",
    "        'id': 1107,\n",
    "        'en': \"It is boundless. If you need and are brave enough to initiate crowdfunding, everything will become possible.\"\n",
    "    },\n",
    "    {\n",
    "        'id': 616,\n",
    "        'en': \"It was Mr. Dong's real intention to sign the agreement, which was legal and effective after being signed and sealed by all the parties concerned.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Current Implementation (max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_current(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 1: Current Implementation (max_length=128)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    print(f\"Input text: {text}\")\n",
    "    print(f\"Input tokens: {len(inputs.input_ids[0])} tokens\")\n",
    "    print()\n",
    "    \n",
    "    # Generate with current parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    # Show raw output\n",
    "    print(\"Raw output sequence:\")\n",
    "    print(f\"  Length: {len(outputs[0])} tokens\")\n",
    "    print(f\"  Token IDs (first 20): {outputs[0].tolist()[:20]}\")\n",
    "    print()\n",
    "    \n",
    "    # Show tokens\n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    print(f\"Output tokens ({len(output_tokens)} total):\")\n",
    "    for i, token in enumerate(output_tokens[:30]):\n",
    "        print(f\"  [{i}] {repr(token)}\")\n",
    "    if len(output_tokens) > 30:\n",
    "        print(f\"  ... ({len(output_tokens) - 30} more tokens)\")\n",
    "    print()\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    print(f\"Translation length: {len(translation)} characters\")\n",
    "    \n",
    "    # Check for truncation\n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Translation ends with comma - likely truncated!\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Test first case\n",
    "test_case = test_cases[0]\n",
    "result = test_translation_current(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Increased max_length (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_longer(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 2: Increased max_length (256)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=256\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"OK: Does not end with comma\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result2 = test_translation_longer(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Use max_new_tokens instead of max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_max_new_tokens(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 3: max_new_tokens=200 (better for variable input lengths)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    print(f\"Input length: {len(inputs.input_ids[0])} tokens\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens (input + new)\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"OK: Translation appears complete\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result3 = test_translation_max_new_tokens(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Check for EOS token issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eos_tokens(tokenizer):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 4: EOS Token Investigation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"EOS token: {repr(tokenizer.eos_token)}\")\n",
    "    print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Check Chinese comma (full-width)\n",
    "    chinese_comma = \"\\uff0c\"\n",
    "    chinese_comma_ids = tokenizer.encode(chinese_comma, add_special_tokens=False)\n",
    "    print(f\"Chinese comma (full-width):\")\n",
    "    print(f\"  Character: {chinese_comma}\")\n",
    "    print(f\"  Token IDs: {chinese_comma_ids}\")\n",
    "    print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(chinese_comma_ids)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check English comma\n",
    "    english_comma = \",\"\n",
    "    english_comma_ids = tokenizer.encode(english_comma, add_special_tokens=False)\n",
    "    print(f\"English comma:\")\n",
    "    print(f\"  Token IDs: {english_comma_ids}\")\n",
    "    print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(english_comma_ids)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if comma matches EOS\n",
    "    if tokenizer.eos_token_id in chinese_comma_ids:\n",
    "        print(\"CRITICAL: Chinese comma contains EOS token ID!\")\n",
    "    elif tokenizer.eos_token_id in english_comma_ids:\n",
    "        print(\"CRITICAL: English comma contains EOS token ID!\")\n",
    "    else:\n",
    "        print(\"OK: Commas do not match EOS token ID\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Language tokens:\")\n",
    "    print(f\"  eng_Latn: {tokenizer.convert_tokens_to_ids('eng_Latn')}\")\n",
    "    print(f\"  zho_Hans: {tokenizer.convert_tokens_to_ids('zho_Hans')}\")\n",
    "\n",
    "test_eos_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Disable early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_no_early_stop(text, tokenizer, model, device):\n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 5: Disable early stopping + explicit constraints\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            early_stopping=False\n",
    "        )\n",
    "    \n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output length: {len(outputs[0])} tokens\")\n",
    "    print(f\"Last 5 tokens: {output_tokens[-5:]}\")\n",
    "    print(f\"Decoded translation: {translation}\")\n",
    "    \n",
    "    if translation.rstrip().endswith(','):\n",
    "        print(\"WARNING: Still ends with comma!\")\n",
    "    else:\n",
    "        print(\"OK: Translation appears complete\")\n",
    "    \n",
    "    return translation\n",
    "\n",
    "result5 = test_translation_no_early_stop(test_case['en'], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All Cases - Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING ALL CASES - OLD vs NEW PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for test_case in test_cases:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case {test_case['id']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"English: {test_case['en']}\")\n",
    "    print()\n",
    "    \n",
    "    tokenizer.src_lang = \"eng_Latn\"\n",
    "    inputs = tokenizer(test_case['en'], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # OLD: Current implementation\n",
    "    with torch.no_grad():\n",
    "        outputs_old = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_length=128\n",
    "        )\n",
    "    translation_old = tokenizer.decode(outputs_old[0], skip_special_tokens=True)\n",
    "    \n",
    "    # NEW: Recommended fix\n",
    "    with torch.no_grad():\n",
    "        outputs_new = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            early_stopping=False\n",
    "        )\n",
    "    translation_new = tokenizer.decode(outputs_new[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"OLD (max_length=128):\")\n",
    "    print(f\"  {translation_old}\")\n",
    "    print(f\"  Length: {len(outputs_old[0])} tokens, {len(translation_old)} chars\")\n",
    "    print()\n",
    "    print(f\"NEW (max_new_tokens=200):\")\n",
    "    print(f\"  {translation_new}\")\n",
    "    print(f\"  Length: {len(outputs_new[0])} tokens, {len(translation_new)} chars\")\n",
    "    print()\n",
    "    \n",
    "    # Compare\n",
    "    if len(translation_new) > len(translation_old):\n",
    "        print(f\"RESULT: IMPROVED (+{len(translation_new) - len(translation_old)} characters)\")\n",
    "    elif translation_new == translation_old:\n",
    "        print(\"RESULT: NO CHANGE\")\n",
    "    else:\n",
    "        print(\"RESULT: DIFFERENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Tests performed:\")\n",
    "print(\"1. Current implementation (max_length=128)\")\n",
    "print(\"2. Increased max_length (256)\")\n",
    "print(\"3. Using max_new_tokens instead of max_length\")\n",
    "print(\"4. EOS token investigation\")\n",
    "print(\"5. Disabled early stopping with constraints\")\n",
    "print()\n",
    "print(\"RECOMMENDED FIX for notebook 07:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Replace:\")\n",
    "print(\"    outputs = model.generate(\")\n",
    "print(\"        **inputs,\")\n",
    "print(\"        forced_bos_token_id=tgt_lang_id,\")\n",
    "print(\"        output_attentions=True,\")\n",
    "print(\"        return_dict_in_generate=True,\")\n",
    "print(\"        max_length=128\")\n",
    "print(\"    )\")\n",
    "print()\n",
    "print(\"With:\")\n",
    "print(\"    outputs = model.generate(\")\n",
    "print(\"        **inputs,\")\n",
    "print(\"        forced_bos_token_id=tgt_lang_id,\")\n",
    "print(\"        output_attentions=True,\")\n",
    "print(\"        return_dict_in_generate=True,\")\n",
    "print(\"        max_new_tokens=200,\")\n",
    "print(\"        num_beams=1,\")\n",
    "print(\"        do_sample=False,\")\n",
    "print(\"        early_stopping=False\")\n",
    "print(\"    )\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"NOTE: After fixing, re-run notebook 07 to regenerate all 2000 translations.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Test ZH→EN translation with a Chinese sentence that has commas\nchinese_test = \"晚间10时许，怀有4月身孕的孙颐婕20万元交保，先行离开。\"\n\nprint(f\"Chinese input: {chinese_test}\")\nprint()\n\ntokenizer.src_lang = \"zho_Hans\"\ninputs = tokenizer(chinese_test, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"),\n        max_new_tokens=200\n    )\n\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"English translation: {translation}\")\nprint(f\"Translation length: {len(translation)} characters\")\nprint(f\"Number of output tokens: {len(outputs[0])}\")\nprint()\nprint(\"Does ZH→EN work correctly? Or is it also truncated?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Test Reverse Direction: ZH→EN\n\nTest if the issue is specific to generating Chinese, or if it affects both directions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Deep diagnostic: analyze token-by-token generation\ntest_case = test_cases[0]\n\ntokenizer.src_lang = \"eng_Latn\"\ninputs = tokenizer(test_case['en'], return_tensors=\"pt\").to(device)\n\nprint(f\"Test: {test_case['en']}\")\nprint(f\"Input IDs: {inputs.input_ids[0].tolist()}\")\nprint()\n\n# Generate with verbose output\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n        max_new_tokens=200,\n        output_scores=True,\n        return_dict_in_generate=True\n    )\n\nprint(\"=\"*80)\nprint(\"GENERATED TOKEN SEQUENCE:\")\nprint(\"=\"*80)\n\noutput_ids = outputs.sequences[0].tolist()\noutput_tokens = tokenizer.convert_ids_to_tokens(outputs.sequences[0])\n\nfor i, (token_id, token) in enumerate(zip(output_ids, output_tokens)):\n    is_eos = token_id == tokenizer.eos_token_id\n    marker = \" <-- EOS!\" if is_eos else \"\"\n    print(f\"[{i:3d}] ID={token_id:6d} Token={repr(token):20s}{marker}\")\n    \n    # Stop printing after EOS\n    if is_eos:\n        print(\"\\\\nGeneration stopped at EOS token\")\n        break\n\nprint()\nprint(\"=\"*80)\ndecoded = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\nprint(f\"Decoded output: {decoded}\")\nprint(f\"Output ends with comma: {decoded.rstrip().endswith(',')}\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Deep Diagnostic: Check Token Generation\n\nSince all parameters produce truncated output, let's examine the raw token sequence to see what the model is actually generating.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}