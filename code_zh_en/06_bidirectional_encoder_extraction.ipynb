{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Attention Extraction\n",
    "\n",
    "This notebook demonstrates extracting encoder self-attention maps from **both** English and Chinese sentences by running the NLLB model in both translation directions:\n",
    "\n",
    "1. **EN → ZH**: English source → Extract English encoder attention\n",
    "2. **ZH → EN**: Chinese source → Extract Chinese encoder attention\n",
    "\n",
    "This bidirectional extraction allows us to compare the topological structure of encoder attention patterns across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport seaborn as sns\nfrom pathlib import Path\n\n# Configure matplotlib for Chinese font support\nmatplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']\nmatplotlib.rcParams['axes.unicode_minus'] = False\n\n# Force matplotlib to rebuild font cache if needed\ntry:\n    fm._rebuild()\nexcept:\n    pass\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load model and tokenizer with eager attention (required for output_attentions=True)\n",
    "model_path = \"../models/nllb-600M\"\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_path,\n",
    "    attn_implementation=\"eager\"  # Required for extracting attention weights\n",
    ").to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "data_path = Path(\"../data/sentence_pairs_zh_en.pkl\")\n",
    "data = pd.read_pickle(data_path)\n",
    "\n",
    "# Convert list of dicts to DataFrame and rename columns\n",
    "df = pd.DataFrame(data)\n",
    "df = df.rename(columns={'english': 'en', 'chinese': 'zh'})\n",
    "\n",
    "print(f\"Loaded {len(df)} sentence pairs\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Extract Encoder Attention in Both Directions\n",
    "\n",
    "For each sentence pair, we'll:\n",
    "1. Run **EN → ZH** translation and extract **English encoder attention**\n",
    "2. Run **ZH → EN** translation and extract **Chinese encoder attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder_attention(text, src_lang, tgt_lang, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Extract encoder self-attention for a given source text.\n",
    "    \n",
    "    Args:\n",
    "        text: Source text string\n",
    "        src_lang: Source language code (e.g., 'eng_Latn', 'zho_Hans')\n",
    "        tgt_lang: Target language code (e.g., 'zho_Hans', 'eng_Latn')\n",
    "        tokenizer: NLLB tokenizer\n",
    "        model: NLLB model\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - tokens: List of source tokens\n",
    "            - encoder_attention: Encoder self-attention (num_layers, num_heads, seq_len, seq_len)\n",
    "            - translation: Generated translation text\n",
    "    \"\"\"\n",
    "    # Set source language\n",
    "    tokenizer.src_lang = src_lang\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get target language BOS token\n",
    "    tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "    \n",
    "    # Generate translation with attention output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_id,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    # Extract encoder attention (available in encoder_attentions)\n",
    "    # Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "    encoder_attention = outputs.encoder_attentions\n",
    "    encoder_attention = torch.stack([layer.squeeze(0) for layer in encoder_attention])  # (num_layers, num_heads, seq_len, seq_len)\n",
    "    \n",
    "    # Decode tokens\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "    translation = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'tokens': input_tokens,\n",
    "        'encoder_attention': encoder_attention.cpu().numpy(),\n",
    "        'translation': translation\n",
    "    }\n",
    "\n",
    "print(\"Function defined: extract_encoder_attention()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Test on Sample Sentence Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample sentence pair\n",
    "idx = 10\n",
    "en_text = df.iloc[idx]['en']\n",
    "zh_text = df.iloc[idx]['zh']\n",
    "\n",
    "print(f\"Sample {idx}:\")\n",
    "print(f\"English: {en_text}\")\n",
    "print(f\"Chinese: {zh_text}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract English encoder attention (EN → ZH)\n",
    "print(\"Extracting English encoder attention (EN → ZH)...\")\n",
    "en_result = extract_encoder_attention(\n",
    "    text=en_text,\n",
    "    src_lang='eng_Latn',\n",
    "    tgt_lang='zho_Hans',\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"English tokens: {en_result['tokens']}\")\n",
    "print(f\"English encoder attention shape: {en_result['encoder_attention'].shape}\")\n",
    "print(f\"Translation to Chinese: {en_result['translation']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Chinese encoder attention (ZH → EN)\n",
    "print(\"Extracting Chinese encoder attention (ZH → EN)...\")\n",
    "zh_result = extract_encoder_attention(\n",
    "    text=zh_text,\n",
    "    src_lang='zho_Hans',\n",
    "    tgt_lang='eng_Latn',\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Chinese tokens: {zh_result['tokens']}\")\n",
    "print(f\"Chinese encoder attention shape: {zh_result['encoder_attention'].shape}\")\n",
    "print(f\"Translation to English: {zh_result['translation']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Visualize Encoder Attention Maps\n",
    "\n",
    "Compare encoder attention patterns from English and Chinese for the same sentence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoder_attention(attention, tokens, layer=0, head=0, title=\"Encoder Self-Attention\", filter_special=True):\n",
    "    \"\"\"\n",
    "    Plot encoder self-attention heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention: Attention weights (num_layers, num_heads, seq_len, seq_len)\n",
    "        tokens: List of token strings\n",
    "        layer: Which layer to visualize\n",
    "        head: Which attention head to visualize\n",
    "        title: Plot title\n",
    "        filter_special: Whether to filter out special tokens\n",
    "    \"\"\"\n",
    "    # Extract specified layer and head\n",
    "    attn = attention[layer, head]  # (seq_len, seq_len)\n",
    "    \n",
    "    # Filter special tokens if requested\n",
    "    if filter_special:\n",
    "        # Keep only content tokens (filter out special tokens and language tags)\n",
    "        special_tokens = {'</s>', '<s>', '<pad>', 'eng_Latn', 'zho_Hans'}\n",
    "        content_mask = [tok not in special_tokens for tok in tokens]\n",
    "        \n",
    "        if sum(content_mask) > 0:  # Only filter if there are content tokens\n",
    "            attn = attn[content_mask][:, content_mask]\n",
    "            tokens = [tok for tok, keep in zip(tokens, content_mask) if keep]\n",
    "            \n",
    "            # Renormalize attention weights after filtering\n",
    "            attn = attn / attn.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        square=True\n",
    "    )\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.title(f\"{title}\\nLayer {layer}, Head {head}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Function defined: plot_encoder_attention()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize English encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=en_result['encoder_attention'],\n",
    "    tokens=en_result['tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention\\n'{en_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize English encoder attention - middle layer\n",
    "middle_layer = en_result['encoder_attention'].shape[0] // 2\n",
    "plot_encoder_attention(\n",
    "    attention=en_result['encoder_attention'],\n",
    "    tokens=en_result['tokens'],\n",
    "    layer=middle_layer,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention (Middle Layer)\\n'{en_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize English encoder attention - last layer\n",
    "last_layer = en_result['encoder_attention'].shape[0] - 1\n",
    "plot_encoder_attention(\n",
    "    attention=en_result['encoder_attention'],\n",
    "    tokens=en_result['tokens'],\n",
    "    layer=last_layer,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention (Last Layer)\\n'{en_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Chinese encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=zh_result['encoder_attention'],\n",
    "    tokens=zh_result['tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"Chinese Encoder Attention\\n'{zh_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Chinese encoder attention - middle layer\n",
    "middle_layer = zh_result['encoder_attention'].shape[0] // 2\n",
    "plot_encoder_attention(\n",
    "    attention=zh_result['encoder_attention'],\n",
    "    tokens=zh_result['tokens'],\n",
    "    layer=middle_layer,\n",
    "    head=0,\n",
    "    title=f\"Chinese Encoder Attention (Middle Layer)\\n'{zh_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Chinese encoder attention - last layer\n",
    "last_layer = zh_result['encoder_attention'].shape[0] - 1\n",
    "plot_encoder_attention(\n",
    "    attention=zh_result['encoder_attention'],\n",
    "    tokens=zh_result['tokens'],\n",
    "    layer=last_layer,\n",
    "    head=0,\n",
    "    title=f\"Chinese Encoder Attention (Last Layer)\\n'{zh_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 5. Compare Attention Statistics\n",
    "\n",
    "Compute basic statistics to compare English and Chinese encoder attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average attention across all layers and heads\n",
    "en_avg_attention = en_result['encoder_attention'].mean(axis=(0, 1))  # (seq_len, seq_len)\n",
    "zh_avg_attention = zh_result['encoder_attention'].mean(axis=(0, 1))  # (seq_len, seq_len)\n",
    "\n",
    "print(\"English encoder attention statistics:\")\n",
    "print(f\"  Shape: {en_avg_attention.shape}\")\n",
    "print(f\"  Mean:  {en_avg_attention.mean():.4f}\")\n",
    "print(f\"  Std:   {en_avg_attention.std():.4f}\")\n",
    "print(f\"  Min:   {en_avg_attention.min():.4f}\")\n",
    "print(f\"  Max:   {en_avg_attention.max():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Chinese encoder attention statistics:\")\n",
    "print(f\"  Shape: {zh_avg_attention.shape}\")\n",
    "print(f\"  Mean:  {zh_avg_attention.mean():.4f}\")\n",
    "print(f\"  Std:   {zh_avg_attention.std():.4f}\")\n",
    "print(f\"  Min:   {zh_avg_attention.min():.4f}\")\n",
    "print(f\"  Max:   {zh_avg_attention.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 6. Process Multiple Samples\n",
    "\n",
    "Extract encoder attention for a few more sentence pairs to verify the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process first 5 sentence pairs\n",
    "num_samples = 5\n",
    "results = []\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    en_text = df.iloc[idx]['en']\n",
    "    zh_text = df.iloc[idx]['zh']\n",
    "    \n",
    "    print(f\"\\nProcessing pair {idx}...\")\n",
    "    print(f\"  EN: {en_text[:60]}...\")\n",
    "    print(f\"  ZH: {zh_text[:60]}...\")\n",
    "    \n",
    "    # Extract encoder attention for both directions\n",
    "    en_result = extract_encoder_attention(en_text, 'eng_Latn', 'zho_Hans', tokenizer, model, device)\n",
    "    zh_result = extract_encoder_attention(zh_text, 'zho_Hans', 'eng_Latn', tokenizer, model, device)\n",
    "    \n",
    "    results.append({\n",
    "        'idx': idx,\n",
    "        'en_text': en_text,\n",
    "        'zh_text': zh_text,\n",
    "        'en_tokens': en_result['tokens'],\n",
    "        'zh_tokens': zh_result['tokens'],\n",
    "        'en_attention': en_result['encoder_attention'],\n",
    "        'zh_attention': zh_result['encoder_attention'],\n",
    "        'en_translation': en_result['translation'],\n",
    "        'zh_translation': zh_result['translation']\n",
    "    })\n",
    "    \n",
    "    print(f\"  EN attention shape: {en_result['encoder_attention'].shape}\")\n",
    "    print(f\"  ZH attention shape: {zh_result['encoder_attention'].shape}\")\n",
    "\n",
    "print(f\"\\n✓ Processed {len(results)} sentence pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of extracted data\n",
    "print(\"Summary of extracted encoder attention:\")\n",
    "print(f\"Total pairs processed: {len(results)}\")\n",
    "print()\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Pair {result['idx']}:\")\n",
    "    print(f\"  English: {len(result['en_tokens'])} tokens, attention shape {result['en_attention'].shape}\")\n",
    "    print(f\"  Chinese: {len(result['zh_tokens'])} tokens, attention shape {result['zh_attention'].shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates bidirectional encoder attention extraction:\n",
    "\n",
    "1. ✅ **EN → ZH**: Extract English encoder attention\n",
    "2. ✅ **ZH → EN**: Extract Chinese encoder attention\n",
    "3. ✅ Visualize and compare attention patterns\n",
    "4. ✅ Process multiple sentence pairs\n",
    "\n",
    "**Next Steps:**\n",
    "- Scale to all 2000 sentence pairs\n",
    "- Build attention graphs (tokens as nodes, attention weights as edges)\n",
    "- Compute persistent homology (β₀, β₁) using TDA\n",
    "- Compare topological structure across languages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}