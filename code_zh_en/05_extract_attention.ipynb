{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Extract Attention Maps from NLLB Model\n",
    "Explore and extract encoder/decoder attention weights for English → Chinese translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Import all required libraries\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom datasets import load_from_disk\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport seaborn as sns\n\n# Configure matplotlib for Chinese font support\nmatplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']\nmatplotlib.rcParams['axes.unicode_minus'] = False\n\n# Force matplotlib to rebuild font cache if needed\ntry:\n    fm._rebuild()\nexcept:\n    pass"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with eager attention implementation (required for attention output)\n",
    "model_dir = \"../models/nllb-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_dir,\n",
    "    attn_implementation=\"eager\"  # Required for output_attentions=True\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "print(f\"Attention implementation: eager\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(\"../data/wmt17_zh_en_validation_2000\")\n",
    "print(f\"\\nLoaded {len(dataset)} sentence pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Extract attention from a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first example\n",
    "example = dataset[0][\"translation\"]\n",
    "english = example[\"en\"]\n",
    "chinese = example[\"zh\"]\n",
    "\n",
    "print(f\"English: {english}\")\n",
    "print(f\"Chinese: {chinese}\")\n",
    "\n",
    "# Tokenize English input\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "inputs = tokenizer(english, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"\\nInput shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translation with attention output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "        max_length=100,\n",
    "        output_attentions=True,  # IMPORTANT: Enable attention output\n",
    "        return_dict_in_generate=True  # Return structured output\n",
    "    )\n",
    "\n",
    "# Decode translation\n",
    "translation = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"\\nOutput tokens: {tokenizer.convert_ids_to_tokens(outputs.sequences[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Understand attention structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output structure\n",
    "print(\"Output keys:\", outputs.keys())\n",
    "print(\"\\nAttention types available:\")\n",
    "if hasattr(outputs, 'encoder_attentions') and outputs.encoder_attentions is not None:\n",
    "    print(f\"  - Encoder self-attention: {len(outputs.encoder_attentions)} layers\")\n",
    "if hasattr(outputs, 'decoder_attentions') and outputs.decoder_attentions is not None:\n",
    "    print(f\"  - Decoder self-attention: {len(outputs.decoder_attentions)} timesteps\")\n",
    "if hasattr(outputs, 'cross_attentions') and outputs.cross_attentions is not None:\n",
    "    print(f\"  - Cross-attention: {len(outputs.cross_attentions)} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine encoder attention structure\n",
    "if outputs.encoder_attentions is not None:\n",
    "    encoder_attn = outputs.encoder_attentions\n",
    "    print(f\"Encoder attention:\")\n",
    "    print(f\"  Number of layers: {len(encoder_attn)}\")\n",
    "    print(f\"  Shape per layer: {encoder_attn[0].shape}\")  # (batch, heads, seq_len, seq_len)\n",
    "    print(f\"  Format: (batch_size, num_heads, seq_length, seq_length)\")\n",
    "    \n",
    "    # Get last layer attention\n",
    "    last_layer_attn = encoder_attn[-1][0]  # Remove batch dimension\n",
    "    print(f\"\\n  Last layer shape: {last_layer_attn.shape}\")\n",
    "    print(f\"  Number of attention heads: {last_layer_attn.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine decoder and cross-attention structure\n",
    "if outputs.decoder_attentions is not None:\n",
    "    print(f\"\\nDecoder self-attention:\")\n",
    "    print(f\"  Number of timesteps: {len(outputs.decoder_attentions)}\")\n",
    "    print(f\"  Each timestep contains {len(outputs.decoder_attentions[0])} layers\")\n",
    "    print(f\"  Shape format: (batch_size, num_heads, query_length, key_length)\")\n",
    "    print(f\"  Shape varies per timestep due to causal masking:\")\n",
    "    # Show first few timesteps to illustrate the pattern\n",
    "    for t in range(min(3, len(outputs.decoder_attentions))):\n",
    "        shape = outputs.decoder_attentions[t][0].shape\n",
    "        print(f\"    Timestep {t}: {shape}\")\n",
    "\n",
    "if outputs.cross_attentions is not None:\n",
    "    print(f\"\\nCross-attention (decoder attending to encoder):\")\n",
    "    print(f\"  Number of timesteps: {len(outputs.cross_attentions)}\")\n",
    "    print(f\"  Each timestep contains {len(outputs.cross_attentions[0])} layers\")\n",
    "    print(f\"  Shape format: (batch_size, num_heads, decoder_length, encoder_length)\")\n",
    "    print(f\"  Shape varies per timestep:\")\n",
    "    for t in range(min(3, len(outputs.cross_attentions))):\n",
    "        shape = outputs.cross_attentions[t][0].shape\n",
    "        print(f\"    Timestep {t}: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Visualize encoder self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoder attention from last layer\n",
    "encoder_attn_last = outputs.encoder_attentions[-1][0].cpu().numpy()  # (heads, seq_len, seq_len)\n",
    "\n",
    "# Average over all attention heads\n",
    "encoder_attn_avg = encoder_attn_last.mean(axis=0)  # (seq_len, seq_len)\n",
    "\n",
    "# Get tokens for axis labels\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(encoder_attn_avg, \n",
    "            xticklabels=input_tokens, \n",
    "            yticklabels=input_tokens,\n",
    "            cmap='viridis',\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Encoder Self-Attention (Last Layer, Averaged over Heads)\\nEnglish Sentence')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Attention matrix shape: {encoder_attn_avg.shape}\")\n",
    "print(f\"Min attention: {encoder_attn_avg.min():.4f}\")\n",
    "print(f\"Max attention: {encoder_attn_avg.max():.4f}\")\n",
    "print(f\"Mean attention: {encoder_attn_avg.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Visualize individual attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 4 attention heads from last encoder layer\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(min(4, encoder_attn_last.shape[0])):\n",
    "    attn_head = encoder_attn_last[head_idx]\n",
    "    \n",
    "    sns.heatmap(attn_head, \n",
    "                xticklabels=input_tokens, \n",
    "                yticklabels=input_tokens,\n",
    "                cmap='viridis',\n",
    "                ax=axes[head_idx],\n",
    "                cbar_kws={'label': 'Weight'})\n",
    "    axes[head_idx].set_title(f'Attention Head {head_idx}')\n",
    "    axes[head_idx].set_xlabel('Key')\n",
    "    axes[head_idx].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Encoder Self-Attention Heads (Last Layer)', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Visualize cross-attention (decoder attending to encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output tokens (Chinese translation) for cross-attention visualization\n",
    "output_tokens = tokenizer.convert_ids_to_tokens(outputs.sequences[0].cpu())\n",
    "print(f\"Generated {len(output_tokens)} Chinese tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build complete cross-attention matrix by stacking all timesteps\n",
    "# At each timestep, decoder attends to encoder with query_length=1\n",
    "# We need to stack all timesteps to get the full (decoder_length, encoder_length) matrix\n",
    "if outputs.cross_attentions is not None:\n",
    "    print(f\"Number of timesteps: {len(outputs.cross_attentions)}\")\n",
    "    print(f\"Number of output tokens: {len(output_tokens)}\")\n",
    "    print(f\"Note: output_tokens[0] = </s> (BOS), output_tokens[1] = zho_Hans, length = timesteps + 1\")\n",
    "    \n",
    "    # Stack cross-attention from all timesteps\n",
    "    # Each timestep has shape (batch, heads, 1, encoder_length)\n",
    "    cross_attn_all_timesteps = []\n",
    "    \n",
    "    for t in range(len(outputs.cross_attentions)):\n",
    "        # Get last layer cross-attention at timestep t\n",
    "        cross_attn_t = outputs.cross_attentions[t][-1][0].cpu().numpy()  # (heads, 1, encoder_length)\n",
    "        \n",
    "        # Average over heads\n",
    "        cross_attn_t_avg = cross_attn_t.mean(axis=0)  # (1, encoder_length)\n",
    "        \n",
    "        cross_attn_all_timesteps.append(cross_attn_t_avg)\n",
    "    \n",
    "    # Stack to create full matrix (num_timesteps, encoder_length)\n",
    "    cross_attn_full = np.vstack(cross_attn_all_timesteps)\n",
    "    \n",
    "    print(f\"\\nStacked cross-attention shape: {cross_attn_full.shape}\")\n",
    "    print(f\"  Rows (decoder tokens): {cross_attn_full.shape[0]}\")\n",
    "    print(f\"  Cols (encoder tokens): {cross_attn_full.shape[1]}\")\n",
    "    \n",
    "    # Get decoder tokens (skip first BOS token since it wasn't generated)\n",
    "    # output_tokens[0] is '</s>' (BOS), output_tokens[1:] are zho_Hans + generated tokens\n",
    "    decoder_tokens_generated = output_tokens[1:]  # Skip BOS token\n",
    "    \n",
    "    print(f\"\\nDecoder tokens (generated, excluding BOS): {len(decoder_tokens_generated)}\")\n",
    "    print(f\"Tokens: {decoder_tokens_generated}\")\n",
    "    \n",
    "    # Plot complete cross-attention heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(cross_attn_full,\n",
    "                xticklabels=input_tokens,\n",
    "                yticklabels=decoder_tokens_generated,\n",
    "                cmap='viridis',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title('Cross-Attention (All Timesteps, Last Layer)\\nChinese tokens attending to English tokens')\n",
    "    plt.xlabel('Encoder (English)')\n",
    "    plt.ylabel('Decoder (Chinese)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Complete cross-attention matrix showing how each Chinese token attends to English tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross-attention with only content tokens (remove special tokens)\n",
    "if outputs.cross_attentions is not None:\n",
    "    # Filter encoder tokens: remove eng_Latn (first) and </s> (last)\n",
    "    input_tokens_content = input_tokens[1:-1]\n",
    "    \n",
    "    # Filter decoder tokens: remove zho_Hans (first) and </s> (last) from decoder_tokens_generated\n",
    "    # decoder_tokens_generated = output_tokens[1:] = [zho_Hans, char1, char2, ..., </s>]\n",
    "    decoder_tokens_content = decoder_tokens_generated[1:-1]\n",
    "    \n",
    "    # Filter cross-attention matrix accordingly\n",
    "    # Remove first and last columns (encoder special tokens)\n",
    "    # Remove first and last rows (decoder special tokens)\n",
    "    cross_attn_content = cross_attn_full[1:-1, 1:-1]\n",
    "    \n",
    "    print(f\"Content-only tokens:\")\n",
    "    print(f\"  English (encoder): {len(input_tokens_content)} tokens\")\n",
    "    print(f\"  Chinese (decoder): {len(decoder_tokens_content)} tokens\")\n",
    "    print(f\"  Cross-attention shape: {cross_attn_content.shape}\")\n",
    "    \n",
    "    # Plot content-only cross-attention heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cross_attn_content,\n",
    "                xticklabels=input_tokens_content,\n",
    "                yticklabels=decoder_tokens_content,\n",
    "                cmap='viridis',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title('Cross-Attention (Content Tokens Only)\\nChinese characters attending to English words')\n",
    "    plt.xlabel('Encoder (English - content only)')\n",
    "    plt.ylabel('Decoder (Chinese - content only)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Cross-attention showing only content words (special tokens removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Extract and save attention for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_maps(text, tokenizer, model, device, src_lang=\"eng_Latn\", tgt_lang=\"zho_Hans\"):\n",
    "    \"\"\"\n",
    "    Extract encoder self-attention for a given text.\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - 'tokens': list of tokens\n",
    "            - 'encoder_attention': numpy array (layers, heads, seq_len, seq_len)\n",
    "            - 'encoder_attention_avg': numpy array (seq_len, seq_len) - averaged over layers and heads\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokenizer.src_lang = src_lang\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "    \n",
    "    # Generate with attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "            max_length=100,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    # Extract encoder attention\n",
    "    encoder_attn_all = torch.stack([layer[0] for layer in outputs.encoder_attentions]).cpu().numpy()\n",
    "    # Shape: (layers, heads, seq_len, seq_len)\n",
    "    \n",
    "    # Average over layers and heads\n",
    "    encoder_attn_avg = encoder_attn_all.mean(axis=(0, 1))  # (seq_len, seq_len)\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'encoder_attention': encoder_attn_all,\n",
    "        'encoder_attention_avg': encoder_attn_avg\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_result = extract_attention_maps(english, tokenizer, model, device)\n",
    "print(f\"Extracted attention for: {english}\")\n",
    "print(f\"Tokens: {test_result['tokens']}\")\n",
    "print(f\"Encoder attention shape: {test_result['encoder_attention'].shape}\")\n",
    "print(f\"Averaged attention shape: {test_result['encoder_attention_avg'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention for first 5 examples\n",
    "num_examples = 5\n",
    "attention_data = []\n",
    "\n",
    "print(f\"Extracting attention for {num_examples} examples...\\n\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    example = dataset[i][\"translation\"]\n",
    "    english = example[\"en\"]\n",
    "    chinese = example[\"zh\"]\n",
    "    \n",
    "    result = extract_attention_maps(english, tokenizer, model, device)\n",
    "    \n",
    "    attention_data.append({\n",
    "        'index': i,\n",
    "        'english': english,\n",
    "        'chinese': chinese,\n",
    "        'tokens': result['tokens'],\n",
    "        'attention_avg': result['encoder_attention_avg']\n",
    "    })\n",
    "    \n",
    "    print(f\"[{i+1}/{num_examples}] Extracted attention for: {english[:50]}...\")\n",
    "\n",
    "print(f\"\\n✓ Extracted attention for {len(attention_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for multiple examples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i in range(min(3, len(attention_data))):\n",
    "    data = attention_data[i]\n",
    "    \n",
    "    sns.heatmap(data['attention_avg'],\n",
    "                xticklabels=data['tokens'],\n",
    "                yticklabels=data['tokens'],\n",
    "                cmap='viridis',\n",
    "                ax=axes[i],\n",
    "                cbar_kws={'label': 'Weight'})\n",
    "    axes[i].set_title(f\"Example {i+1}\\n{data['english'][:40]}...\", fontsize=10)\n",
    "    axes[i].set_xlabel('Key')\n",
    "    axes[i].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Encoder Self-Attention (Averaged)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Successfully extracted:**\n",
    "- Encoder self-attention maps (English sentence structure)\n",
    "- Cross-attention maps (decoder attending to encoder)\n",
    "- Attention weights across all layers and heads\n",
    "- Averaged attention for graph construction\n",
    "\n",
    "**Next steps:**\n",
    "1. Build attention graphs (tokens as nodes, attention weights as edges)\n",
    "2. Compute persistent homology on the graphs\n",
    "3. Compare English vs Chinese topological structures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}