{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Explore BLEU Score Computation\n",
    "\n",
    "Compute BLEU scores for translation quality assessment.\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path in code_zh_en\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell and run from \"Verify Working Directory\"\n",
    "\n",
    "---\n",
    "\n",
    "We have:\n",
    "- `en_translation`: Generated Chinese (EN → ZH)\n",
    "- `zh_translation`: Generated English (ZH → EN)\n",
    "- `zh_text`: Reference Chinese\n",
    "- `en_text`: Reference English\n",
    "\n",
    "We'll compute:\n",
    "- **EN→ZH BLEU**: Compare `en_translation` with `zh_text`\n",
    "- **ZH→EN BLEU**: Compare `zh_translation` with `en_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only needed for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # IMPORTANT: Set this to your code_zh_en directory path\n",
    "    # This should point to where THIS notebook is located\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_zh_en\"\n",
    "    \n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"✓ Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab, using local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Verify Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify working directory and required files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check TDA results file\n",
    "tda_file = \"../data/tda_results_zh_en/tda_results_last_layer_filtered.pkl\"\n",
    "if os.path.exists(tda_file):\n",
    "    print(f\"✓ TDA results file exists: {tda_file}\")\n",
    "    print(f\"  File size: {Path(tda_file).stat().st_size / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    print(f\"✗ TDA results file NOT found: {tda_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sacrebleu (standard BLEU implementation)\n",
    "# Note: sacrebleu is not installed by default on Google Colab\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport pandas as pd\nfrom sacrebleu import sentence_bleu\n\n# Configure matplotlib for Chinese font support\n# Check if running on Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\nif IN_COLAB:\n    # On Colab: Download and configure SimHei font for Chinese support\n    import urllib.request\n    import matplotlib\n    import matplotlib.font_manager as fm\n    import shutil\n    import os\n    \n    # Download SimHei font from a reliable source\n    font_url = \"https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf\"\n    font_path = \"/usr/share/fonts/truetype/SimHei.ttf\"\n    \n    if not os.path.exists(font_path):\n        print(\"Downloading SimHei font...\")\n        try:\n            urllib.request.urlretrieve(font_url, font_path)\n            print(f\"✓ Downloaded font to {font_path}\")\n        except Exception as e:\n            print(f\"⚠ Download failed: {e}\")\n            # Try alternative source\n            font_url = \"https://github.com/kosho2013/DFPlayer/raw/master/SimHei.ttf\"\n            try:\n                urllib.request.urlretrieve(font_url, font_path)\n                print(f\"✓ Downloaded from alternative source\")\n            except Exception as e2:\n                print(f\"⚠ Alternative download also failed: {e2}\")\n    else:\n        print(f\"✓ Font already exists at {font_path}\")\n    \n    # Clear matplotlib cache\n    cache_dir = matplotlib.get_cachedir()\n    if os.path.exists(cache_dir):\n        shutil.rmtree(cache_dir)\n        print(\"✓ Cleared matplotlib cache\")\n    \n    # Register the font if it exists\n    if os.path.exists(font_path):\n        fm.fontManager.addfont(font_path)\n        print(\"✓ Font registered with matplotlib\")\n        \n    # Set SimHei as default font\n    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n    plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display\n    \n    print(\"✓ Font configuration complete: SimHei\")\n    \nelse:\n    # On local: use Arial Unicode MS (macOS)\n    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']\n    plt.rcParams['axes.unicode_minus'] = False\n    print(\"✓ Running locally: using Arial Unicode MS for Chinese support\")\n\nsns.set_style(\"whitegrid\")\nprint(\"✓ Libraries imported\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Load TDA Results\n",
    "\n",
    "Load results that contain both translations and original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TDA results (contains translations)\n",
    "data_path = Path(\"../data/tda_results_zh_en/tda_results_last_layer_filtered.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")\n",
    "print()\n",
    "\n",
    "# Examine first result\n",
    "print(\"Data structure:\")\n",
    "print(f\"Keys: {list(results[0].keys())}\")\n",
    "print()\n",
    "print(\"Sample:\")\n",
    "print(f\"EN text:         {results[0]['en_text']}\")\n",
    "print(f\"ZH text:         {results[0]['zh_text']}\")\n",
    "print(f\"EN→ZH (generated): {results[0]['en_translation']}\")\n",
    "print(f\"ZH→EN (generated): {results[0]['zh_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Compute BLEU Scores for Sample Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "def compute_bleu_scores(en_text, zh_text, en_translation, zh_translation):\n    \"\"\"\n    Compute BLEU scores for both translation directions.\n    \n    Args:\n        en_text: Original English text (reference for ZH→EN)\n        zh_text: Original Chinese text (reference for EN→ZH)\n        en_translation: Generated Chinese from English (hypothesis for EN→ZH)\n        zh_translation: Generated English from Chinese (hypothesis for ZH→EN)\n    \n    Returns:\n        dict with BLEU scores\n    \n    Note:\n        Uses character-level tokenization (tokenize='char') for Chinese text\n        to avoid word segmentation issues. This is standard practice for Chinese BLEU.\n    \"\"\"\n    # EN→ZH: Compare generated Chinese with reference Chinese\n    # Use character-level tokenization for Chinese\n    bleu_en_zh = sentence_bleu(en_translation, [zh_text], tokenize='char').score\n    \n    # ZH→EN: Compare generated English with reference English\n    # Use default tokenization for English\n    bleu_zh_en = sentence_bleu(zh_translation, [en_text]).score\n    \n    # Average BLEU\n    bleu_avg = (bleu_en_zh + bleu_zh_en) / 2\n    \n    return {\n        'bleu_en_zh': bleu_en_zh,\n        'bleu_zh_en': bleu_zh_en,\n        'bleu_avg': bleu_avg\n    }\n\nprint(\"✓ Function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first 10 examples\n",
    "print(\"Testing BLEU computation on first 10 examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(10):\n",
    "    example = results[i]\n",
    "    scores = compute_bleu_scores(\n",
    "        en_text=example['en_text'],\n",
    "        zh_text=example['zh_text'],\n",
    "        en_translation=example['en_translation'],\n",
    "        zh_translation=example['zh_translation']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[{i}] EN→ZH: {scores['bleu_en_zh']:.2f}, ZH→EN: {scores['bleu_zh_en']:.2f}, Avg: {scores['bleu_avg']:.2f}\")\n",
    "    print(f\"    EN: {example['en_text'][:70]}...\")\n",
    "    print(f\"    ZH: {example['zh_text'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Compute BLEU for All Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU scores for all examples\n",
    "print(f\"Computing BLEU scores for {len(results)} sentence pairs...\")\n",
    "\n",
    "bleu_results = []\n",
    "for i, example in enumerate(results):\n",
    "    scores = compute_bleu_scores(\n",
    "        en_text=example['en_text'],\n",
    "        zh_text=example['zh_text'],\n",
    "        en_translation=example['en_translation'],\n",
    "        zh_translation=example['zh_translation']\n",
    "    )\n",
    "    \n",
    "    bleu_results.append({\n",
    "        'idx': i,\n",
    "        **scores\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(results)}\")\n",
    "\n",
    "print(f\"✓ Computed BLEU scores for all {len(bleu_results)} pairs\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_bleu = pd.DataFrame(bleu_results)\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df_bleu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BLEU SCORE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"EN→ZH BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_en_zh'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_en_zh'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_en_zh'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_en_zh'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_en_zh'].std():.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"ZH→EN BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_zh_en'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_zh_en'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_zh_en'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_zh_en'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_zh_en'].std():.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"Average BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_avg'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_avg'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_avg'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_avg'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_avg'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Visualize BLEU Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→ZH BLEU\n",
    "axes[0].hist(df_bleu['bleu_en_zh'], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(df_bleu['bleu_en_zh'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_en_zh\"].mean():.2f}')\n",
    "axes[0].set_xlabel('BLEU Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('EN→ZH BLEU Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# ZH→EN BLEU\n",
    "axes[1].hist(df_bleu['bleu_zh_en'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(df_bleu['bleu_zh_en'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_zh_en\"].mean():.2f}')\n",
    "axes[1].set_xlabel('BLEU Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('ZH→EN BLEU Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Average BLEU\n",
    "axes[2].hist(df_bleu['bleu_avg'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[2].axvline(df_bleu['bleu_avg'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_avg\"].mean():.2f}')\n",
    "axes[2].set_xlabel('BLEU Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Average BLEU Distribution')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Best and Worst Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by average BLEU\n",
    "df_bleu_sorted = df_bleu.sort_values('bleu_avg', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BEST TRANSLATIONS (Highest BLEU)\")\n",
    "print(\"=\"*70)\n",
    "for i in range(5):\n",
    "    idx = int(df_bleu_sorted.iloc[i]['idx'])\n",
    "    example = results[idx]\n",
    "    scores = df_bleu_sorted.iloc[i]\n",
    "    \n",
    "    print(f\"\\n[{i+1}] Pair {idx}: Avg BLEU = {scores['bleu_avg']:.2f} (EN→ZH: {scores['bleu_en_zh']:.2f}, ZH→EN: {scores['bleu_zh_en']:.2f})\")\n",
    "    print(f\"    EN: {example['en_text']}\")\n",
    "    print(f\"    ZH: {example['zh_text']}\")\n",
    "    print(f\"    Generated ZH: {example['en_translation']}\")\n",
    "    print(f\"    Generated EN: {example['zh_translation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WORST TRANSLATIONS (Lowest BLEU)\")\n",
    "print(\"=\"*70)\n",
    "for i in range(5):\n",
    "    idx = int(df_bleu_sorted.iloc[-(i+1)]['idx'])\n",
    "    example = results[idx]\n",
    "    scores = df_bleu_sorted.iloc[-(i+1)]\n",
    "    \n",
    "    print(f\"\\n[{i+1}] Pair {idx}: Avg BLEU = {scores['bleu_avg']:.2f} (EN→ZH: {scores['bleu_en_zh']:.2f}, ZH→EN: {scores['bleu_zh_en']:.2f})\")\n",
    "    print(f\"    EN: {example['en_text']}\")\n",
    "    print(f\"    ZH: {example['zh_text']}\")\n",
    "    print(f\"    Generated ZH: {example['en_translation']}\")\n",
    "    print(f\"    Generated EN: {example['zh_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Correlation Between Translation Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(df_bleu['bleu_en_zh'], df_bleu['bleu_zh_en'], alpha=0.3, s=10)\n",
    "plt.xlabel('EN→ZH BLEU')\n",
    "plt.ylabel('ZH→EN BLEU')\n",
    "plt.title('Correlation Between Translation Directions')\n",
    "plt.plot([0, 100], [0, 100], 'r--', alpha=0.5, label='y=x')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation\n",
    "corr = df_bleu['bleu_en_zh'].corr(df_bleu['bleu_zh_en'])\n",
    "print(f\"Correlation between EN→ZH and ZH→EN BLEU: r = {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BLEU scores to CSV for later analysis\n",
    "output_path = Path(\"../data/bleu_scores_zh_en.csv\")\n",
    "\n",
    "print(f\"Saving BLEU scores to {output_path}...\")\n",
    "df_bleu.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved to {output_path}\")\n",
    "print(f\"  Shape: {df_bleu.shape}\")\n",
    "print(f\"  Columns: {list(df_bleu.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": "## Summary\n\n✅ **BLEU Score Computation Complete!**\n\n**What we computed:**\n- BLEU scores for all 2000 sentence pairs (both translation directions)\n- BLEU score distributions (EN→ZH, ZH→EN, Average)\n- Best and worst translations by BLEU score\n- Correlation between translation directions\n- Saved results to CSV for correlation analysis\n\n**Next steps:**\n- Correlate BLEU scores with topological similarity (notebook 13)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}