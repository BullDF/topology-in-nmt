{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Data Analysis - Exploration\n",
    "\n",
    "Explore building attention graphs and computing persistent homology on encoder attention patterns.\n",
    "\n",
    "**Methodology:**\n",
    "1. Extract last layer attention\n",
    "2. Average across attention heads\n",
    "3. (Optional) Filter special tokens and renormalize\n",
    "4. Symmetrize attention matrix\n",
    "5. Convert to distance: `d_ij = 1 - attention_ij`\n",
    "6. Compute Vietoris-Rips persistent homology\n",
    "7. Extract persistence diagrams (β₀, β₁)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TDA libraries if not already installed\n",
    "# !pip install ripser persim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\n\n# Configure matplotlib for Chinese font support\nplt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\nplt.rcParams['axes.unicode_minus'] = False\n\n# Suppress warnings about infinite death times in persistence diagrams\n# (This is expected for H0 diagrams - one component persists forever)\nwarnings.filterwarnings('ignore', message='.*non-finite death times.*')\n\n# TDA libraries (Scikit-TDA)\nfrom ripser import ripser\nfrom persim import plot_diagrams, wasserstein\n\nprint(\"✓ Libraries imported\")"
  },
  {
   "cell_type": "code",
   "source": "import matplotlib\n\ndef plot_diagrams_with_font(diagrams, show=True, **kwargs):\n    \"\"\"\n    Wrapper around persim.plot_diagrams that ensures Chinese font is applied.\n    Uses matplotlib rc_context to override font settings during plot creation.\n    \n    Args:\n        diagrams: Persistence diagrams from ripser\n        show: Whether to call plt.show()\n        **kwargs: Additional arguments passed to plot_diagrams\n    \"\"\"\n    # Use rc_context to temporarily override font settings\n    with matplotlib.rc_context({'font.sans-serif': ['Arial Unicode MS'], \n                                 'axes.unicode_minus': False}):\n        plot_diagrams(diagrams, show=show, **kwargs)\n\nprint(\"✓ Helper function defined: plot_diagrams_with_font()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Attention Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted attention maps\n",
    "data_path = Path(\"../data/attention_maps_zh_en/all_encoder_attention.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")\n",
    "print(f\"\\nModel architecture: {results[0]['en_attention'].shape[0]} layers, {results[0]['en_attention'].shape[1]} heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Graph Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distance_matrix(attention, tokens, layer=-1, filter_special=True):\n",
    "    \"\"\"\n",
    "    Build distance matrix from attention weights.\n",
    "    \n",
    "    Args:\n",
    "        attention: Attention tensor (num_layers, num_heads, seq_len, seq_len)\n",
    "        tokens: List of token strings\n",
    "        layer: Which layer to use (-1 for last layer)\n",
    "        filter_special: Whether to filter out special tokens\n",
    "    \n",
    "    Returns:\n",
    "        distance_matrix: (N, N) array where N = number of tokens (or content tokens if filtered)\n",
    "        filtered_tokens: List of token strings (content tokens if filtered, all tokens otherwise)\n",
    "    \"\"\"\n",
    "    # 1. Extract last layer and average over heads\n",
    "    attn = attention[layer].mean(axis=0)  # (seq_len, seq_len)\n",
    "    \n",
    "    # 2. Filter special tokens (optional)\n",
    "    if filter_special:\n",
    "        special_tokens = {'</s>', '<s>', '<pad>', 'eng_Latn', 'zho_Hans'}\n",
    "        content_mask = np.array([tok not in special_tokens for tok in tokens])\n",
    "        \n",
    "        if sum(content_mask) > 0:  # Only filter if there are content tokens\n",
    "            # Filter attention matrix\n",
    "            attn_filtered = attn[content_mask][:, content_mask]\n",
    "            filtered_tokens = [tok for tok, keep in zip(tokens, content_mask) if keep]\n",
    "            \n",
    "            # 3. Renormalize\n",
    "            row_sums = attn_filtered.sum(axis=1, keepdims=True)\n",
    "            attn_filtered = attn_filtered / row_sums\n",
    "        else:\n",
    "            # No content tokens, use original\n",
    "            attn_filtered = attn\n",
    "            filtered_tokens = tokens\n",
    "    else:\n",
    "        # No filtering, use all tokens\n",
    "        attn_filtered = attn\n",
    "        filtered_tokens = tokens\n",
    "    \n",
    "    # 4. Symmetrize (make undirected)\n",
    "    attn_sym = (attn_filtered + attn_filtered.T) / 2\n",
    "    \n",
    "    # 5. Convert to distance: d = 1 - attention\n",
    "    distance_matrix = 1 - attn_sym\n",
    "    \n",
    "    # Ensure diagonal is 0 and symmetric\n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    \n",
    "    return distance_matrix, filtered_tokens\n",
    "\n",
    "\n",
    "print(\"✓ Function defined: build_distance_matrix()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test on Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first example\n",
    "idx = 0\n",
    "example = results[idx]\n",
    "\n",
    "print(f\"Example {idx}:\")\n",
    "print(f\"English: {example['en_text']}\")\n",
    "print(f\"Chinese: {example['zh_text']}\")\n",
    "print()\n",
    "\n",
    "# Build distance matrices\n",
    "en_dist, en_tokens = build_distance_matrix(example['en_attention'], example['en_tokens'])\n",
    "zh_dist, zh_tokens = build_distance_matrix(example['zh_attention'], example['zh_tokens'])\n",
    "\n",
    "print(f\"English distance matrix: {en_dist.shape}\")\n",
    "print(f\"  Content tokens: {en_tokens}\")\n",
    "print(f\"  Min distance: {en_dist.min():.4f}\")\n",
    "print(f\"  Max distance: {en_dist.max():.4f}\")\n",
    "print(f\"  Mean distance: {en_dist.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Chinese distance matrix: {zh_dist.shape}\")\n",
    "print(f\"  Content tokens: {zh_tokens}\")\n",
    "print(f\"  Min distance: {zh_dist.min():.4f}\")\n",
    "print(f\"  Max distance: {zh_dist.max():.4f}\")\n",
    "print(f\"  Mean distance: {zh_dist.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distance Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distance matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# English\n",
    "sns.heatmap(en_dist, xticklabels=en_tokens, yticklabels=en_tokens, \n",
    "            cmap='RdYlBu_r', ax=ax1, square=True,\n",
    "            cbar_kws={'label': 'Distance (1 - attention)'})\n",
    "ax1.set_title(f'English Distance Matrix\\n{example[\"en_text\"][:50]}...')\n",
    "ax1.set_xlabel('Tokens')\n",
    "ax1.set_ylabel('Tokens')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Chinese\n",
    "sns.heatmap(zh_dist, xticklabels=zh_tokens, yticklabels=zh_tokens, \n",
    "            cmap='RdYlBu_r', ax=ax2, square=True,\n",
    "            cbar_kws={'label': 'Distance (1 - attention)'})\n",
    "ax2.set_title(f'Chinese Distance Matrix\\n{example[\"zh_text\"][:50]}...')\n",
    "ax2.set_xlabel('Tokens')\n",
    "ax2.set_ylabel('Tokens')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Persistent Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Vietoris-Rips persistent homology using ripser\n",
    "# maxdim=1 computes β₀ (connected components) and β₁ (loops)\n",
    "\n",
    "print(\"✓ Computing Vietoris-Rips persistence using ripser\")\n",
    "print(f\"  Computing: β₀ (connected components) and β₁ (1-dimensional holes/loops)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute persistence for English\n",
    "# ripser expects distance matrix, returns dict with 'dgms' key\n",
    "en_result = ripser(en_dist, maxdim=1, distance_matrix=True)\n",
    "en_diagrams = en_result['dgms']  # List of diagrams: [H0, H1]\n",
    "\n",
    "print(\"English persistence diagrams:\")\n",
    "print(f\"  H0 (β₀) shape: {en_diagrams[0].shape}\")\n",
    "print(f\"  H1 (β₁) shape: {en_diagrams[1].shape}\")\n",
    "print(f\"  Format: (birth, death)\")\n",
    "print(f\"\\nFirst 5 H0 features:\")\n",
    "print(en_diagrams[0][:5])\n",
    "print(f\"\\nFirst 5 H1 features:\")\n",
    "print(en_diagrams[1][:5] if len(en_diagrams[1]) > 0 else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute persistence for Chinese\n",
    "zh_result = ripser(zh_dist, maxdim=1, distance_matrix=True)\n",
    "zh_diagrams = zh_result['dgms']\n",
    "\n",
    "print(\"Chinese persistence diagrams:\")\n",
    "print(f\"  H0 (β₀) shape: {zh_diagrams[0].shape}\")\n",
    "print(f\"  H1 (β₁) shape: {zh_diagrams[1].shape}\")\n",
    "print(f\"\\nFirst 5 H0 features:\")\n",
    "print(zh_diagrams[0][:5])\n",
    "print(f\"\\nFirst 5 H1 features:\")\n",
    "print(zh_diagrams[1][:5] if len(zh_diagrams[1]) > 0 else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Persistence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count features by dimension\n",
    "def analyze_diagram(diagrams, language=\"\"):\n",
    "    \"\"\"\n",
    "    Analyze persistence diagrams from ripser.\n",
    "    diagrams is a list: [H0, H1] where each is (n_features, 2) array\n",
    "    \"\"\"\n",
    "    dim0 = diagrams[0]  # H0 features (birth, death)\n",
    "    dim1 = diagrams[1]  # H1 features\n",
    "    \n",
    "    # Remove infinite persistence (last feature in H0 is typically infinite)\n",
    "    dim0_finite = dim0[np.isfinite(dim0).all(axis=1)]\n",
    "    dim1_finite = dim1[np.isfinite(dim1).all(axis=1)]\n",
    "    \n",
    "    # Persistence = death - birth\n",
    "    persistence0 = dim0_finite[:, 1] - dim0_finite[:, 0]\n",
    "    persistence1 = dim1_finite[:, 1] - dim1_finite[:, 0]\n",
    "    \n",
    "    print(f\"{language} Persistence Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"β₀ features (connected components): {len(dim0_finite)}\")\n",
    "    if len(persistence0) > 0:\n",
    "        print(f\"  Max persistence: {persistence0.max():.4f}\")\n",
    "        print(f\"  Mean persistence: {persistence0.mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nβ₁ features (loops/holes): {len(dim1_finite)}\")\n",
    "    if len(persistence1) > 0:\n",
    "        print(f\"  Max persistence: {persistence1.max():.4f}\")\n",
    "        print(f\"  Mean persistence: {persistence1.mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"  No H1 features detected\")\n",
    "    print()\n",
    "\n",
    "analyze_diagram(en_diagrams, \"English\")\n",
    "analyze_diagram(zh_diagrams, \"Chinese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Visualize Persistence Diagrams"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot English persistence diagram\nplot_diagrams_with_font(en_diagrams, show=False)\nplt.title(f'English Persistence Diagram\\n{example[\"en_text\"][:60]}...')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot Chinese persistence diagram\nplot_diagrams_with_font(zh_diagrams, show=False)\nplt.title(f'Chinese Persistence Diagram\\n{example[\"zh_text\"][:60]}...')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Persistence Diagrams\n",
    "\n",
    "Use Wasserstein distance to measure similarity between English and Chinese topologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Wasserstein distance between diagrams using persim\n",
    "# We compute for both H0 and H1 separately, then can combine\n",
    "\n",
    "# Wasserstein distance for H0 (connected components)\n",
    "w_dist_h0 = wasserstein(en_diagrams[0], zh_diagrams[0])\n",
    "\n",
    "# Wasserstein distance for H1 (loops) - handle case where one diagram is empty\n",
    "if len(en_diagrams[1]) > 0 and len(zh_diagrams[1]) > 0:\n",
    "    w_dist_h1 = wasserstein(en_diagrams[1], zh_diagrams[1])\n",
    "else:\n",
    "    # If one has no H1 features, compute distance to empty diagram\n",
    "    w_dist_h1 = wasserstein(en_diagrams[1], zh_diagrams[1])\n",
    "\n",
    "# Total distance (can weight differently, but simple sum for now)\n",
    "total_w_dist = w_dist_h0 + w_dist_h1\n",
    "\n",
    "print(\"Wasserstein Distances:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"H0 (connected components): {w_dist_h0:.6f}\")\n",
    "print(f\"H1 (loops/holes):          {w_dist_h1:.6f}\")\n",
    "print(f\"Total:                     {total_w_dist:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process Multiple Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process first 10 examples\n",
    "num_examples = 10\n",
    "wasserstein_distances = []\n",
    "\n",
    "print(f\"Computing persistent homology for {num_examples} sentence pairs...\\n\")\n",
    "\n",
    "for idx in range(num_examples):\n",
    "    example = results[idx]\n",
    "    \n",
    "    # Build distance matrices\n",
    "    en_dist, _ = build_distance_matrix(example['en_attention'], example['en_tokens'])\n",
    "    zh_dist, _ = build_distance_matrix(example['zh_attention'], example['zh_tokens'])\n",
    "    \n",
    "    # Compute persistence\n",
    "    en_result = ripser(en_dist, maxdim=1, distance_matrix=True)\n",
    "    zh_result = ripser(zh_dist, maxdim=1, distance_matrix=True)\n",
    "    \n",
    "    en_diagrams = en_result['dgms']\n",
    "    zh_diagrams = zh_result['dgms']\n",
    "    \n",
    "    # Compute Wasserstein distances\n",
    "    w_dist_h0 = wasserstein(en_diagrams[0], zh_diagrams[0])\n",
    "    w_dist_h1 = wasserstein(en_diagrams[1], zh_diagrams[1])\n",
    "    total_w_dist = w_dist_h0 + w_dist_h1\n",
    "    \n",
    "    wasserstein_distances.append({\n",
    "        'idx': idx,\n",
    "        'en_text': example['en_text'],\n",
    "        'zh_text': example['zh_text'],\n",
    "        'wasserstein_distance': total_w_dist,\n",
    "        'wasserstein_h0': w_dist_h0,\n",
    "        'wasserstein_h1': w_dist_h1,\n",
    "        'en_diagrams': en_diagrams,\n",
    "        'zh_diagrams': zh_diagrams\n",
    "    })\n",
    "    \n",
    "    print(f\"[{idx}] W-dist: {total_w_dist:.6f} (H0: {w_dist_h0:.4f}, H1: {w_dist_h1:.4f}) | EN: {example['en_text'][:40]}...\")\n",
    "\n",
    "print(f\"\\n✓ Processed {num_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "w_dists = [d['wasserstein_distance'] for d in wasserstein_distances]\n",
    "\n",
    "print(\"Wasserstein Distance Statistics (first 10 pairs):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Min:    {np.min(w_dists):.6f}\")\n",
    "print(f\"Max:    {np.max(w_dists):.6f}\")\n",
    "print(f\"Mean:   {np.mean(w_dists):.6f}\")\n",
    "print(f\"Median: {np.median(w_dists):.6f}\")\n",
    "print(f\"Std:    {np.std(w_dists):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(w_dists, bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.axvline(np.mean(w_dists), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(w_dists):.6f}')\n",
    "plt.xlabel('Wasserstein Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Topological Similarity (English vs Chinese)\\nLower = More Similar')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Compare High vs Low Similarity Examples",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Find most and least topologically similar pairs\nsorted_by_similarity = sorted(wasserstein_distances, key=lambda x: x['wasserstein_distance'])\n\nprint(\"Most topologically similar (lowest Wasserstein distance):\")\nprint(\"=\"*70)\nmost_similar = sorted_by_similarity[0]\nprint(f\"Pair {most_similar['idx']}: W-dist = {most_similar['wasserstein_distance']:.6f}\")\nprint(f\"  EN: {most_similar['en_text']}\")\nprint(f\"  ZH: {most_similar['zh_text']}\")\nprint()\n\nprint(\"Least topologically similar (highest Wasserstein distance):\")\nprint(\"=\"*70)\nleast_similar = sorted_by_similarity[-1]\nprint(f\"Pair {least_similar['idx']}: W-dist = {least_similar['wasserstein_distance']:.6f}\")\nprint(f\"  EN: {least_similar['en_text']}\")\nprint(f\"  ZH: {least_similar['zh_text']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot persistence diagrams for most similar pair\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# English\nplt.sca(ax1)\nplot_diagrams_with_font(most_similar['en_diagrams'], show=False)\nax1.set_title('English')\n\n# Chinese\nplt.sca(ax2)\nplot_diagrams_with_font(most_similar['zh_diagrams'], show=False)\nax2.set_title('Chinese')\n\nfig.suptitle(f\"Most Topologically Similar Pair (W-dist: {most_similar['wasserstein_distance']:.6f})\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Successfully computed topological features!**\n",
    "\n",
    "**What we learned:**\n",
    "- Built distance matrices from attention weights\n",
    "- Computed Vietoris-Rips persistent homology (β₀, β₁)\n",
    "- Visualized persistence diagrams\n",
    "- Measured topological similarity using Wasserstein distance\n",
    "- Analyzed first 10 sentence pairs\n",
    "\n",
    "**Next steps:**\n",
    "1. Scale to all 2000 sentence pairs\n",
    "2. Compute BLEU scores for translation quality\n",
    "3. Correlate Wasserstein distance with BLEU scores\n",
    "4. Statistical analysis and visualization\n",
    "\n",
    "**Key findings so far:**\n",
    "- Wasserstein distances vary across sentence pairs\n",
    "- Both languages show β₀ and β₁ features\n",
    "- Topological structure varies across sentence pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}