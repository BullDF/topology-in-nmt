{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore TDA-BLEU Correlation\n",
    "\n",
    "Combine topological similarity (Wasserstein distance) with translation quality (BLEU scores) to test our hypothesis:\n",
    "\n",
    "**Does topological similarity between English and French attention patterns predict translation quality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import spearmanr, pearsonr\nimport warnings\n\n# Suppress warnings about infinite death times in persistence diagrams\nwarnings.filterwarnings('ignore', message='.*non-finite death times.*')\n\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"✓ Libraries imported\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TDA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TDA results\n",
    "tda_path = Path(\"../data/tda_results_fr_en/tda_results_last_layer_filtered.pkl\")\n",
    "\n",
    "print(f\"Loading TDA results from {tda_path}...\")\n",
    "with open(tda_path, 'rb') as f:\n",
    "    tda_results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(tda_results)} TDA results\")\n",
    "\n",
    "# Extract relevant TDA metrics\n",
    "df_tda = pd.DataFrame([{\n",
    "    'idx': r['idx'],\n",
    "    'wasserstein_distance': r['wasserstein_distance'],\n",
    "    'wasserstein_h0': r['wasserstein_h0'],\n",
    "    'wasserstein_h1': r['wasserstein_h1'],\n",
    "    'en_num_tokens': r['en_num_tokens'],\n",
    "    'fr_num_tokens': r['fr_num_tokens'],\n",
    "    'en_h0_features': r['en_h0_features'],\n",
    "    'en_h1_features': r['en_h1_features'],\n",
    "    'fr_h0_features': r['fr_h0_features'],\n",
    "    'fr_h1_features': r['fr_h1_features']\n",
    "} for r in tda_results])\n",
    "\n",
    "print(f\"\\nTDA DataFrame shape: {df_tda.shape}\")\n",
    "print(df_tda.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLEU scores\n",
    "bleu_path = Path(\"../data/bleu_scores_fr_en.csv\")\n",
    "\n",
    "print(f\"Loading BLEU scores from {bleu_path}...\")\n",
    "df_bleu = pd.read_csv(bleu_path)\n",
    "\n",
    "print(f\"✓ Loaded {len(df_bleu)} BLEU scores\")\n",
    "print(f\"\\nBLEU DataFrame shape: {df_bleu.shape}\")\n",
    "print(df_bleu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on idx\n",
    "df = pd.merge(df_tda, df_bleu, on='idx')\n",
    "\n",
    "print(f\"✓ Merged DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"Wasserstein Distance:\")\n",
    "print(f\"  Mean: {df['wasserstein_distance'].mean():.6f}\")\n",
    "print(f\"  Std:  {df['wasserstein_distance'].std():.6f}\")\n",
    "print()\n",
    "\n",
    "print(\"BLEU Scores:\")\n",
    "print(f\"  EN→FR - Mean: {df['bleu_en_fr'].mean():.2f}, Std: {df['bleu_en_fr'].std():.2f}\")\n",
    "print(f\"  FR→EN - Mean: {df['bleu_fr_en'].mean():.2f}, Std: {df['bleu_fr_en'].std():.2f}\")\n",
    "print(f\"  Avg   - Mean: {df['bleu_avg'].mean():.2f}, Std: {df['bleu_avg'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between Wasserstein distance and BLEU scores\n",
    "print(\"=\" * 70)\n",
    "print(\"CORRELATION ANALYSIS: Wasserstein Distance vs BLEU\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Pearson correlation (linear relationship)\n",
    "pearson_en_fr, p_pearson_en_fr = pearsonr(df['wasserstein_distance'], df['bleu_en_fr'])\n",
    "pearson_fr_en, p_pearson_fr_en = pearsonr(df['wasserstein_distance'], df['bleu_fr_en'])\n",
    "pearson_avg, p_pearson_avg = pearsonr(df['wasserstein_distance'], df['bleu_avg'])\n",
    "\n",
    "print(\"Pearson Correlation (linear):\")\n",
    "print(f\"  Wasserstein vs EN→FR BLEU: r = {pearson_en_fr:.4f}, p = {p_pearson_en_fr:.2e}\")\n",
    "print(f\"  Wasserstein vs FR→EN BLEU: r = {pearson_fr_en:.4f}, p = {p_pearson_fr_en:.2e}\")\n",
    "print(f\"  Wasserstein vs Avg BLEU:   r = {pearson_avg:.4f}, p = {p_pearson_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Spearman correlation (monotonic relationship)\n",
    "spearman_en_fr, p_spearman_en_fr = spearmanr(df['wasserstein_distance'], df['bleu_en_fr'])\n",
    "spearman_fr_en, p_spearman_fr_en = spearmanr(df['wasserstein_distance'], df['bleu_fr_en'])\n",
    "spearman_avg, p_spearman_avg = spearmanr(df['wasserstein_distance'], df['bleu_avg'])\n",
    "\n",
    "print(\"Spearman Correlation (monotonic):\")\n",
    "print(f\"  Wasserstein vs EN→FR BLEU: ρ = {spearman_en_fr:.4f}, p = {p_spearman_en_fr:.2e}\")\n",
    "print(f\"  Wasserstein vs FR→EN BLEU: ρ = {spearman_fr_en:.4f}, p = {p_spearman_fr_en:.2e}\")\n",
    "print(f\"  Wasserstein vs Avg BLEU:   ρ = {spearman_avg:.4f}, p = {p_spearman_avg:.2e}\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "print(\"Interpretation:\")\n",
    "if abs(pearson_avg) < 0.1:\n",
    "    strength = \"negligible\"\n",
    "elif abs(pearson_avg) < 0.3:\n",
    "    strength = \"weak\"\n",
    "elif abs(pearson_avg) < 0.5:\n",
    "    strength = \"moderate\"\n",
    "else:\n",
    "    strength = \"strong\"\n",
    "\n",
    "direction = \"negative\" if pearson_avg < 0 else \"positive\"\n",
    "print(f\"  Overall correlation is {strength} and {direction}.\")\n",
    "\n",
    "if pearson_avg < 0:\n",
    "    print(f\"  → Lower Wasserstein distance (more similar topology) is associated with higher BLEU (better translation).\")\n",
    "else:\n",
    "    print(f\"  → Higher Wasserstein distance (more different topology) is associated with higher BLEU (better translation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scatter Plots: Wasserstein Distance vs BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→FR BLEU\n",
    "axes[0].scatter(df['wasserstein_distance'], df['bleu_en_fr'], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[0].set_ylabel('EN→FR BLEU Score')\n",
    "axes[0].set_title(f'Wasserstein vs EN→FR BLEU\\nr = {pearson_en_fr:.3f}, p = {p_pearson_en_fr:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_en_fr'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[0].legend()\n",
    "\n",
    "# FR→EN BLEU\n",
    "axes[1].scatter(df['wasserstein_distance'], df['bleu_fr_en'], alpha=0.3, s=10)\n",
    "axes[1].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[1].set_ylabel('FR→EN BLEU Score')\n",
    "axes[1].set_title(f'Wasserstein vs FR→EN BLEU\\nr = {pearson_fr_en:.3f}, p = {p_pearson_fr_en:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_fr_en'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[1].legend()\n",
    "\n",
    "# Average BLEU\n",
    "axes[2].scatter(df['wasserstein_distance'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[2].set_xlabel('Wasserstein Distance\\n(Lower = More Topologically Similar)')\n",
    "axes[2].set_ylabel('Average BLEU Score')\n",
    "axes[2].set_title(f'Wasserstein vs Average BLEU\\nr = {pearson_avg:.3f}, p = {p_pearson_avg:.2e}')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_distance'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[2].plot(df['wasserstein_distance'], p(df['wasserstein_distance']), \"r--\", alpha=0.5, label='Trend')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. H0 vs H1 Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze H0 and H1 components separately\n",
    "print(\"=\" * 70)\n",
    "print(\"H0 vs H1 CONTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# H0 correlations\n",
    "pearson_h0, p_h0 = pearsonr(df['wasserstein_h0'], df['bleu_avg'])\n",
    "print(f\"H0 (Connected Components) vs Avg BLEU:\")\n",
    "print(f\"  Pearson r = {pearson_h0:.4f}, p = {p_h0:.2e}\")\n",
    "print()\n",
    "\n",
    "# H1 correlations\n",
    "pearson_h1, p_h1 = pearsonr(df['wasserstein_h1'], df['bleu_avg'])\n",
    "print(f\"H1 (Loops/Holes) vs Avg BLEU:\")\n",
    "print(f\"  Pearson r = {pearson_h1:.4f}, p = {p_h1:.2e}\")\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(df['wasserstein_h0'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[0].set_xlabel('H0 Wasserstein Distance')\n",
    "axes[0].set_ylabel('Average BLEU Score')\n",
    "axes[0].set_title(f'H0 vs BLEU\\nr = {pearson_h0:.3f}, p = {p_h0:.2e}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_h0'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(df['wasserstein_h0'], p(df['wasserstein_h0']), \"r--\", alpha=0.5)\n",
    "\n",
    "axes[1].scatter(df['wasserstein_h1'], df['bleu_avg'], alpha=0.3, s=10)\n",
    "axes[1].set_xlabel('H1 Wasserstein Distance')\n",
    "axes[1].set_ylabel('Average BLEU Score')\n",
    "axes[1].set_title(f'H1 vs BLEU\\nr = {pearson_h1:.3f}, p = {p_h1:.2e}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "z = np.polyfit(df['wasserstein_h1'], df['bleu_avg'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(df['wasserstein_h1'], p(df['wasserstein_h1']), \"r--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 8.5. BLEU vs Token Count Correlation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze correlation between BLEU scores and token counts\nprint(\"=\" * 70)\nprint(\"CORRELATION ANALYSIS: Token Count vs BLEU\")\nprint(\"=\" * 70)\nprint()\n\n# Compute correlations\npearson_en_tokens_en_fr, p_en_en_fr = pearsonr(df['en_num_tokens'], df['bleu_en_fr'])\npearson_en_tokens_fr_en, p_en_fr_en = pearsonr(df['en_num_tokens'], df['bleu_fr_en'])\npearson_en_tokens_avg, p_en_avg = pearsonr(df['en_num_tokens'], df['bleu_avg'])\n\npearson_fr_tokens_en_fr, p_fr_en_fr = pearsonr(df['fr_num_tokens'], df['bleu_en_fr'])\npearson_fr_tokens_fr_en, p_fr_fr_en = pearsonr(df['fr_num_tokens'], df['bleu_fr_en'])\npearson_fr_tokens_avg, p_fr_avg = pearsonr(df['fr_num_tokens'], df['bleu_avg'])\n\nprint(\"English Token Count vs BLEU:\")\nprint(f\"  EN tokens vs EN→FR BLEU: r = {pearson_en_tokens_en_fr:.4f}, p = {p_en_en_fr:.2e}\")\nprint(f\"  EN tokens vs FR→EN BLEU: r = {pearson_en_tokens_fr_en:.4f}, p = {p_en_fr_en:.2e}\")\nprint(f\"  EN tokens vs Avg BLEU:   r = {pearson_en_tokens_avg:.4f}, p = {p_en_avg:.2e}\")\nprint()\n\nprint(\"French Token Count vs BLEU:\")\nprint(f\"  FR tokens vs EN→FR BLEU: r = {pearson_fr_tokens_en_fr:.4f}, p = {p_fr_en_fr:.2e}\")\nprint(f\"  FR tokens vs FR→EN BLEU: r = {pearson_fr_tokens_fr_en:.4f}, p = {p_fr_fr_en:.2e}\")\nprint(f\"  FR tokens vs Avg BLEU:   r = {pearson_fr_tokens_avg:.4f}, p = {p_fr_avg:.2e}\")\nprint()\n\n# Visualize\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# English tokens vs BLEU scores\naxes[0, 0].scatter(df['en_num_tokens'], df['bleu_en_fr'], alpha=0.3, s=10, color='blue')\nen_slope_en_fr, en_int_en_fr, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_en_fr'])\nen_x = np.array([df['en_num_tokens'].min(), df['en_num_tokens'].max()])\naxes[0, 0].plot(en_x, en_slope_en_fr * en_x + en_int_en_fr, 'b-', linewidth=2, alpha=0.8)\naxes[0, 0].set_xlabel('English Token Count')\naxes[0, 0].set_ylabel('EN→FR BLEU Score')\naxes[0, 0].set_title(f'EN Tokens vs EN→FR BLEU\\nr = {pearson_en_tokens_en_fr:.3f}, p = {p_en_en_fr:.2e}')\naxes[0, 0].grid(alpha=0.3)\n\naxes[0, 1].scatter(df['en_num_tokens'], df['bleu_fr_en'], alpha=0.3, s=10, color='blue')\nen_slope_fr_en, en_int_fr_en, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_fr_en'])\naxes[0, 1].plot(en_x, en_slope_fr_en * en_x + en_int_fr_en, 'b-', linewidth=2, alpha=0.8)\naxes[0, 1].set_xlabel('English Token Count')\naxes[0, 1].set_ylabel('FR→EN BLEU Score')\naxes[0, 1].set_title(f'EN Tokens vs FR→EN BLEU\\nr = {pearson_en_tokens_fr_en:.3f}, p = {p_en_fr_en:.2e}')\naxes[0, 1].grid(alpha=0.3)\n\naxes[0, 2].scatter(df['en_num_tokens'], df['bleu_avg'], alpha=0.3, s=10, color='blue')\nen_slope_avg, en_int_avg, _, _, _ = stats.linregress(df['en_num_tokens'], df['bleu_avg'])\naxes[0, 2].plot(en_x, en_slope_avg * en_x + en_int_avg, 'b-', linewidth=2, alpha=0.8)\naxes[0, 2].set_xlabel('English Token Count')\naxes[0, 2].set_ylabel('Average BLEU Score')\naxes[0, 2].set_title(f'EN Tokens vs Avg BLEU\\nr = {pearson_en_tokens_avg:.3f}, p = {p_en_avg:.2e}')\naxes[0, 2].grid(alpha=0.3)\n\n# French tokens vs BLEU scores\naxes[1, 0].scatter(df['fr_num_tokens'], df['bleu_en_fr'], alpha=0.3, s=10, color='green')\nfr_slope_en_fr, fr_int_en_fr, _, _, _ = stats.linregress(df['fr_num_tokens'], df['bleu_en_fr'])\nfr_x = np.array([df['fr_num_tokens'].min(), df['fr_num_tokens'].max()])\naxes[1, 0].plot(fr_x, fr_slope_en_fr * fr_x + fr_int_en_fr, 'g-', linewidth=2, alpha=0.8)\naxes[1, 0].set_xlabel('French Token Count')\naxes[1, 0].set_ylabel('EN→FR BLEU Score')\naxes[1, 0].set_title(f'FR Tokens vs EN→FR BLEU\\nr = {pearson_fr_tokens_en_fr:.3f}, p = {p_fr_en_fr:.2e}')\naxes[1, 0].grid(alpha=0.3)\n\naxes[1, 1].scatter(df['fr_num_tokens'], df['bleu_fr_en'], alpha=0.3, s=10, color='green')\nfr_slope_fr_en, fr_int_fr_en, _, _, _ = stats.linregress(df['fr_num_tokens'], df['bleu_fr_en'])\naxes[1, 1].plot(fr_x, fr_slope_fr_en * fr_x + fr_int_fr_en, 'g-', linewidth=2, alpha=0.8)\naxes[1, 1].set_xlabel('French Token Count')\naxes[1, 1].set_ylabel('FR→EN BLEU Score')\naxes[1, 1].set_title(f'FR Tokens vs FR→EN BLEU\\nr = {pearson_fr_tokens_fr_en:.3f}, p = {p_fr_fr_en:.2e}')\naxes[1, 1].grid(alpha=0.3)\n\naxes[1, 2].scatter(df['fr_num_tokens'], df['bleu_avg'], alpha=0.3, s=10, color='green')\nfr_slope_avg, fr_int_avg, _, _, _ = stats.linregress(df['fr_num_tokens'], df['bleu_avg'])\naxes[1, 2].plot(fr_x, fr_slope_avg * fr_x + fr_int_avg, 'g-', linewidth=2, alpha=0.8)\naxes[1, 2].set_xlabel('French Token Count')\naxes[1, 2].set_ylabel('Average BLEU Score')\naxes[1, 2].set_title(f'FR Tokens vs Avg BLEU\\nr = {pearson_fr_tokens_avg:.3f}, p = {p_fr_avg:.2e}')\naxes[1, 2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8.6. Partial Correlation: Wasserstein vs BLEU (Controlling for Token Count)\n\nSince both Wasserstein distance and BLEU are correlated with token counts, we need to compute **partial correlation** to determine if the Wasserstein-BLEU relationship is genuine or spurious.\n\n**Method**: Use linear regression residuals\n1. Regress Wasserstein distance on token counts → get residuals\n2. Regress BLEU on token counts → get residuals  \n3. Correlate the residuals (= partial correlation, controlling for token count effect)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.linear_model import LinearRegression\n\n# Prepare predictor matrix: both English and French token counts\nX_tokens = df[['en_num_tokens', 'fr_num_tokens']].values\n\nprint(\"=\" * 70)\nprint(\"PARTIAL CORRELATION: Wasserstein vs BLEU (Controlling for Token Counts)\")\nprint(\"=\" * 70)\nprint()\n\n# Function to compute partial correlation via residuals\ndef partial_correlation_residuals(y1, y2, X):\n    \"\"\"\n    Compute partial correlation between y1 and y2, controlling for X.\n    \n    Method: Regress both y1 and y2 on X, then correlate the residuals.\n    \"\"\"\n    # Reshape if needed\n    y1 = np.array(y1).reshape(-1, 1)\n    y2 = np.array(y2).reshape(-1, 1)\n    \n    # Regress y1 on X\n    model1 = LinearRegression()\n    model1.fit(X, y1)\n    residuals1 = y1.flatten() - model1.predict(X).flatten()\n    \n    # Regress y2 on X\n    model2 = LinearRegression()\n    model2.fit(X, y2)\n    residuals2 = y2.flatten() - model2.predict(X).flatten()\n    \n    # Correlate residuals\n    r, p = pearsonr(residuals1, residuals2)\n    return r, p, residuals1, residuals2\n\n# Compute partial correlations for each BLEU metric\nprint(\"Partial Correlation (controlling for EN and FR token counts):\")\nprint()\n\n# Wasserstein vs EN→FR BLEU\nr_partial_en_fr, p_partial_en_fr, w_resid_en_fr, bleu_resid_en_fr = partial_correlation_residuals(\n    df['wasserstein_distance'], df['bleu_en_fr'], X_tokens\n)\nprint(f\"Wasserstein vs EN→FR BLEU:\")\nprint(f\"  Original correlation:  r = {pearson_en_fr:.4f}, p = {p_pearson_en_fr:.2e}\")\nprint(f\"  Partial correlation:   r = {r_partial_en_fr:.4f}, p = {p_partial_en_fr:.2e}\")\nprint()\n\n# Wasserstein vs FR→EN BLEU\nr_partial_fr_en, p_partial_fr_en, w_resid_fr_en, bleu_resid_fr_en = partial_correlation_residuals(\n    df['wasserstein_distance'], df['bleu_fr_en'], X_tokens\n)\nprint(f\"Wasserstein vs FR→EN BLEU:\")\nprint(f\"  Original correlation:  r = {pearson_fr_en:.4f}, p = {p_pearson_fr_en:.2e}\")\nprint(f\"  Partial correlation:   r = {r_partial_fr_en:.4f}, p = {p_partial_fr_en:.2e}\")\nprint()\n\n# Wasserstein vs Average BLEU\nr_partial_avg, p_partial_avg, w_resid_avg, bleu_resid_avg = partial_correlation_residuals(\n    df['wasserstein_distance'], df['bleu_avg'], X_tokens\n)\nprint(f\"Wasserstein vs Average BLEU:\")\nprint(f\"  Original correlation:  r = {pearson_avg:.4f}, p = {p_pearson_avg:.2e}\")\nprint(f\"  Partial correlation:   r = {r_partial_avg:.4f}, p = {p_partial_avg:.2e}\")\nprint()\n\n# Interpretation\nprint(\"Interpretation:\")\nchange_en_fr = abs(r_partial_en_fr) - abs(pearson_en_fr)\nchange_fr_en = abs(r_partial_fr_en) - abs(pearson_fr_en)\nchange_avg = abs(r_partial_avg) - abs(pearson_avg)\n\nprint(f\"  EN→FR: Correlation changed by {change_en_fr:+.4f} after controlling for token counts\")\nprint(f\"  FR→EN: Correlation changed by {change_fr_en:+.4f} after controlling for token counts\")\nprint(f\"  Avg:   Correlation changed by {change_avg:+.4f} after controlling for token counts\")\nprint()\n\nif abs(change_avg) < 0.05:\n    print(\"  → Token count has minimal confounding effect. The Wasserstein-BLEU relationship is genuine.\")\nelif abs(r_partial_avg) < abs(pearson_avg) * 0.5:\n    print(\"  → Token count is a major confounder. Much of the Wasserstein-BLEU correlation is explained by token count.\")\nelse:\n    print(\"  → Token count has some confounding effect, but the Wasserstein-BLEU relationship persists.\")\n\n# Visualize partial correlation (residuals plot)\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# EN→FR\naxes[0].scatter(w_resid_en_fr, bleu_resid_en_fr, alpha=0.3, s=10)\nz = np.polyfit(w_resid_en_fr, bleu_resid_en_fr, 1)\np_fit = np.poly1d(z)\naxes[0].plot(w_resid_en_fr, p_fit(w_resid_en_fr), \"r--\", alpha=0.5)\naxes[0].axhline(0, color='gray', linestyle='--', alpha=0.3)\naxes[0].axvline(0, color='gray', linestyle='--', alpha=0.3)\naxes[0].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\naxes[0].set_ylabel('EN→FR BLEU\\n(residuals after removing token count effect)')\naxes[0].set_title(f'Partial Correlation: Wasserstein vs EN→FR BLEU\\nr_partial = {r_partial_en_fr:.3f}, p = {p_partial_en_fr:.2e}')\naxes[0].grid(alpha=0.3)\n\n# FR→EN\naxes[1].scatter(w_resid_fr_en, bleu_resid_fr_en, alpha=0.3, s=10)\nz = np.polyfit(w_resid_fr_en, bleu_resid_fr_en, 1)\np_fit = np.poly1d(z)\naxes[1].plot(w_resid_fr_en, p_fit(w_resid_fr_en), \"r--\", alpha=0.5)\naxes[1].axhline(0, color='gray', linestyle='--', alpha=0.3)\naxes[1].axvline(0, color='gray', linestyle='--', alpha=0.3)\naxes[1].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\naxes[1].set_ylabel('FR→EN BLEU\\n(residuals after removing token count effect)')\naxes[1].set_title(f'Partial Correlation: Wasserstein vs FR→EN BLEU\\nr_partial = {r_partial_fr_en:.3f}, p = {p_partial_fr_en:.2e}')\naxes[1].grid(alpha=0.3)\n\n# Average\naxes[2].scatter(w_resid_avg, bleu_resid_avg, alpha=0.3, s=10)\nz = np.polyfit(w_resid_avg, bleu_resid_avg, 1)\np_fit = np.poly1d(z)\naxes[2].plot(w_resid_avg, p_fit(w_resid_avg), \"r--\", alpha=0.5)\naxes[2].axhline(0, color='gray', linestyle='--', alpha=0.3)\naxes[2].axvline(0, color='gray', linestyle='--', alpha=0.3)\naxes[2].set_xlabel('Wasserstein Distance\\n(residuals after removing token count effect)')\naxes[2].set_ylabel('Average BLEU\\n(residuals after removing token count effect)')\naxes[2].set_title(f'Partial Correlation: Wasserstein vs Average BLEU\\nr_partial = {r_partial_avg:.3f}, p = {p_partial_avg:.2e}')\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Binned Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin by Wasserstein distance and compare mean BLEU scores\n",
    "df['w_bin'] = pd.qcut(df['wasserstein_distance'], q=5, labels=['Very Similar', 'Similar', 'Moderate', 'Dissimilar', 'Very Dissimilar'])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BINNED ANALYSIS: Mean BLEU by Topological Similarity\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "binned_stats = df.groupby('w_bin').agg({\n",
    "    'bleu_avg': ['mean', 'std', 'count'],\n",
    "    'wasserstein_distance': ['mean', 'std']\n",
    "})\n",
    "\n",
    "print(binned_stats)\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bin_means = df.groupby('w_bin')['bleu_avg'].mean()\n",
    "bin_stds = df.groupby('w_bin')['bleu_avg'].std()\n",
    "\n",
    "ax.bar(range(len(bin_means)), bin_means, yerr=bin_stds, capsize=5, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(range(len(bin_means)))\n",
    "ax.set_xticklabels(bin_means.index, rotation=45, ha='right')\n",
    "ax.set_xlabel('Topological Similarity (Wasserstein Distance Bins)')\n",
    "ax.set_ylabel('Mean BLEU Score')\n",
    "ax.set_title('Translation Quality by Topological Similarity')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Examples: High Similarity vs Low Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Wasserstein distance\n",
    "df_sorted = df.sort_values('wasserstein_distance')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HIGH TOPOLOGICAL SIMILARITY (Low Wasserstein Distance)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    row = df_sorted.iloc[i]\n",
    "    print(f\"\\n[{i+1}] Pair {int(row['idx'])}: W = {row['wasserstein_distance']:.4f}, BLEU = {row['bleu_avg']:.2f}\")\n",
    "    \n",
    "    # Get original text from TDA results\n",
    "    original = tda_results[int(row['idx'])]\n",
    "    print(f\"    EN: {original['en_text']}\")\n",
    "    print(f\"    FR: {original['fr_text']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOW TOPOLOGICAL SIMILARITY (High Wasserstein Distance)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    row = df_sorted.iloc[-(i+1)]\n",
    "    print(f\"\\n[{i+1}] Pair {int(row['idx'])}: W = {row['wasserstein_distance']:.4f}, BLEU = {row['bleu_avg']:.2f}\")\n",
    "    \n",
    "    original = tda_results[int(row['idx'])]\n",
    "    print(f\"    EN: {original['en_text']}\")\n",
    "    print(f\"    FR: {original['fr_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Findings:**\n",
    "- Correlation between topological similarity (Wasserstein distance) and translation quality (BLEU)\n",
    "- H0 vs H1 contribution to the relationship\n",
    "- Binned analysis showing trend across similarity levels\n",
    "\n",
    "**Hypothesis Test:**\n",
    "Does lower Wasserstein distance (more topologically similar attention patterns) predict higher BLEU scores (better translation quality)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}