{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Explore BLEU Score Computation\n",
    "\n",
    "Compute BLEU scores for translation quality assessment.\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path in code_fr_en\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell and run from \"Verify Working Directory\"\n",
    "\n",
    "---\n",
    "\n",
    "We have:\n",
    "- `en_translation`: Generated French (EN → FR)\n",
    "- `fr_translation`: Generated English (FR → EN)\n",
    "- `fr_text`: Reference French\n",
    "- `en_text`: Reference English\n",
    "\n",
    "We'll compute:\n",
    "- **EN→FR BLEU**: Compare `en_translation` with `fr_text`\n",
    "- **FR→EN BLEU**: Compare `fr_translation` with `en_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only needed for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # IMPORTANT: Set this to your code_fr_en directory path\n",
    "    # This should point to where THIS notebook is located\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_fr_en\"\n",
    "    \n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"✓ Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab, using local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Verify Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify working directory and required files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check TDA results file\n",
    "tda_file = \"../data/tda_results_fr_en/tda_results_last_layer_filtered.pkl\"\n",
    "if os.path.exists(tda_file):\n",
    "    print(f\"✓ TDA results file exists: {tda_file}\")\n",
    "    print(f\"  File size: {Path(tda_file).stat().st_size / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    print(f\"✗ TDA results file NOT found: {tda_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sacrebleu (standard BLEU implementation)\n",
    "# Note: sacrebleu is not installed by default on Google Colab\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Load TDA Results\n",
    "\n",
    "Load results that contain both translations and original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TDA results (contains translations)\n",
    "data_path = Path(\"../data/tda_results_fr_en/tda_results_last_layer_filtered.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")\n",
    "print()\n",
    "\n",
    "# Examine first result\n",
    "print(\"Data structure:\")\n",
    "print(f\"Keys: {list(results[0].keys())}\")\n",
    "print()\n",
    "print(\"Sample:\")\n",
    "print(f\"EN text:         {results[0]['en_text']}\")\n",
    "print(f\"FR text:         {results[0]['fr_text']}\")\n",
    "print(f\"EN→FR (generated): {results[0]['en_translation']}\")\n",
    "print(f\"FR→EN (generated): {results[0]['fr_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Compute BLEU Scores for Sample Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_scores(en_text, fr_text, en_translation, fr_translation):\n",
    "    \"\"\"\n",
    "    Compute BLEU scores for both translation directions.\n",
    "    \n",
    "    Args:\n",
    "        en_text: Original English text (reference for FR→EN)\n",
    "        fr_text: Original French text (reference for EN→FR)\n",
    "        en_translation: Generated French from English (hypothesis for EN→FR)\n",
    "        fr_translation: Generated English from French (hypothesis for FR→EN)\n",
    "    \n",
    "    Returns:\n",
    "        dict with BLEU scores\n",
    "    \"\"\"\n",
    "    # EN→FR: Compare generated French with reference French\n",
    "    bleu_en_fr = sentence_bleu(en_translation, [fr_text]).score\n",
    "    \n",
    "    # FR→EN: Compare generated English with reference English\n",
    "    bleu_fr_en = sentence_bleu(fr_translation, [en_text]).score\n",
    "    \n",
    "    # Average BLEU\n",
    "    bleu_avg = (bleu_en_fr + bleu_fr_en) / 2\n",
    "    \n",
    "    return {\n",
    "        'bleu_en_fr': bleu_en_fr,\n",
    "        'bleu_fr_en': bleu_fr_en,\n",
    "        'bleu_avg': bleu_avg\n",
    "    }\n",
    "\n",
    "print(\"✓ Function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first 10 examples\n",
    "print(\"Testing BLEU computation on first 10 examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(10):\n",
    "    example = results[i]\n",
    "    scores = compute_bleu_scores(\n",
    "        en_text=example['en_text'],\n",
    "        fr_text=example['fr_text'],\n",
    "        en_translation=example['en_translation'],\n",
    "        fr_translation=example['fr_translation']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[{i}] EN→FR: {scores['bleu_en_fr']:.2f}, FR→EN: {scores['bleu_fr_en']:.2f}, Avg: {scores['bleu_avg']:.2f}\")\n",
    "    print(f\"    EN: {example['en_text'][:70]}...\")\n",
    "    print(f\"    FR: {example['fr_text'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Compute BLEU for All Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU scores for all examples\n",
    "print(f\"Computing BLEU scores for {len(results)} sentence pairs...\")\n",
    "\n",
    "bleu_results = []\n",
    "for i, example in enumerate(results):\n",
    "    scores = compute_bleu_scores(\n",
    "        en_text=example['en_text'],\n",
    "        fr_text=example['fr_text'],\n",
    "        en_translation=example['en_translation'],\n",
    "        fr_translation=example['fr_translation']\n",
    "    )\n",
    "    \n",
    "    bleu_results.append({\n",
    "        'idx': i,\n",
    "        **scores\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(results)}\")\n",
    "\n",
    "print(f\"✓ Computed BLEU scores for all {len(bleu_results)} pairs\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_bleu = pd.DataFrame(bleu_results)\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df_bleu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BLEU SCORE STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"EN→FR BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_en_fr'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_en_fr'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_en_fr'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_en_fr'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_en_fr'].std():.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"FR→EN BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_fr_en'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_fr_en'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_fr_en'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_fr_en'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_fr_en'].std():.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"Average BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_avg'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_avg'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_avg'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_avg'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_avg'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Visualize BLEU Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→FR BLEU\n",
    "axes[0].hist(df_bleu['bleu_en_fr'], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(df_bleu['bleu_en_fr'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_en_fr\"].mean():.2f}')\n",
    "axes[0].set_xlabel('BLEU Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('EN→FR BLEU Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# FR→EN BLEU\n",
    "axes[1].hist(df_bleu['bleu_fr_en'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(df_bleu['bleu_fr_en'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_fr_en\"].mean():.2f}')\n",
    "axes[1].set_xlabel('BLEU Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('FR→EN BLEU Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Average BLEU\n",
    "axes[2].hist(df_bleu['bleu_avg'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[2].axvline(df_bleu['bleu_avg'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_avg\"].mean():.2f}')\n",
    "axes[2].set_xlabel('BLEU Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Average BLEU Distribution')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Best and Worst Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by average BLEU\n",
    "df_bleu_sorted = df_bleu.sort_values('bleu_avg', ascending=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BEST TRANSLATIONS (Highest BLEU)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    idx = int(df_bleu_sorted.iloc[i]['idx'])\n",
    "    example = results[idx]\n",
    "    scores = df_bleu_sorted.iloc[i]\n",
    "    \n",
    "    print(f\"\\n[{i+1}] Pair {idx}: Avg BLEU = {scores['bleu_avg']:.2f} (EN→FR: {scores['bleu_en_fr']:.2f}, FR→EN: {scores['bleu_fr_en']:.2f})\")\n",
    "    print(f\"    EN: {example['en_text']}\")\n",
    "    print(f\"    FR: {example['fr_text']}\")\n",
    "    print(f\"    Generated FR: {example['en_translation']}\")\n",
    "    print(f\"    Generated EN: {example['fr_translation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WORST TRANSLATIONS (Lowest BLEU)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    idx = int(df_bleu_sorted.iloc[-(i+1)]['idx'])\n",
    "    example = results[idx]\n",
    "    scores = df_bleu_sorted.iloc[-(i+1)]\n",
    "    \n",
    "    print(f\"\\n[{i+1}] Pair {idx}: Avg BLEU = {scores['bleu_avg']:.2f} (EN→FR: {scores['bleu_en_fr']:.2f}, FR→EN: {scores['bleu_fr_en']:.2f})\")\n",
    "    print(f\"    EN: {example['en_text']}\")\n",
    "    print(f\"    FR: {example['fr_text']}\")\n",
    "    print(f\"    Generated FR: {example['en_translation']}\")\n",
    "    print(f\"    Generated EN: {example['fr_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Correlation Between Translation Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(df_bleu['bleu_en_fr'], df_bleu['bleu_fr_en'], alpha=0.3, s=10)\n",
    "plt.xlabel('EN→FR BLEU')\n",
    "plt.ylabel('FR→EN BLEU')\n",
    "plt.title('Correlation Between Translation Directions')\n",
    "plt.plot([0, 100], [0, 100], 'r--', alpha=0.5, label='y=x')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation\n",
    "corr = df_bleu['bleu_en_fr'].corr(df_bleu['bleu_fr_en'])\n",
    "print(f\"Correlation between EN→FR and FR→EN BLEU: r = {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BLEU scores to CSV for later analysis\n",
    "output_path = Path(\"../data/bleu_scores_fr_en.csv\")\n",
    "\n",
    "print(f\"Saving BLEU scores to {output_path}...\")\n",
    "df_bleu.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved to {output_path}\")\n",
    "print(f\"  Shape: {df_bleu.shape}\")\n",
    "print(f\"  Columns: {list(df_bleu.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **BLEU Score Computation Complete!**\n",
    "\n",
    "**What we computed:**\n",
    "- BLEU scores for all 2000 sentence pairs (both translation directions)\n",
    "- BLEU score distributions (EN→FR, FR→EN, Average)\n",
    "- Best and worst translations by BLEU score\n",
    "- Correlation between translation directions\n",
    "- Saved results to CSV for correlation analysis\n",
    "\n",
    "**Next steps:**\n",
    "- Correlate BLEU scores with topological similarity (notebook 14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
