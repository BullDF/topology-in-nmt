{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Topological Data Analysis - Exploration\n\nExplore building attention graphs and computing persistent homology on encoder attention patterns.\n\n**Methodology:**\n1. Extract last layer attention\n2. Average across attention heads\n3. (Optional) Filter special tokens and renormalize\n4. Symmetrize attention matrix\n5. Convert to distance: `d_ij = 1 - attention_ij`\n6. Compute Vietoris-Rips persistent homology\n7. Extract persistence diagrams (β₀, β₁)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install TDA libraries if not already installed\n# !pip install ripser persim"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\n# TDA libraries (Scikit-TDA)\nfrom ripser import ripser\nfrom persim import plot_diagrams, wasserstein\n\nprint(\"✓ Libraries imported\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Attention Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted attention maps\n",
    "data_path = Path(\"../data/attention_maps/all_encoder_attention.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")\n",
    "print(f\"\\nModel architecture: {results[0]['en_attention'].shape[0]} layers, {results[0]['en_attention'].shape[1]} heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Graph Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_distance_matrix(attention, tokens, layer=-1, filter_special=True):\n    \"\"\"\n    Build distance matrix from attention weights.\n    \n    Args:\n        attention: Attention tensor (num_layers, num_heads, seq_len, seq_len)\n        tokens: List of token strings\n        layer: Which layer to use (-1 for last layer)\n        filter_special: Whether to filter out special tokens\n    \n    Returns:\n        distance_matrix: (N, N) array where N = number of tokens (or content tokens if filtered)\n        filtered_tokens: List of token strings (content tokens if filtered, all tokens otherwise)\n    \"\"\"\n    # 1. Extract last layer and average over heads\n    attn = attention[layer].mean(axis=0)  # (seq_len, seq_len)\n    \n    # 2. Filter special tokens (optional)\n    if filter_special:\n        special_tokens = {'</s>', '<s>', '<pad>', 'eng_Latn', 'fra_Latn'}\n        content_mask = np.array([tok not in special_tokens for tok in tokens])\n        \n        if sum(content_mask) > 0:  # Only filter if there are content tokens\n            # Filter attention matrix\n            attn_filtered = attn[content_mask][:, content_mask]\n            filtered_tokens = [tok for tok, keep in zip(tokens, content_mask) if keep]\n            \n            # 3. Renormalize\n            row_sums = attn_filtered.sum(axis=1, keepdims=True)\n            attn_filtered = attn_filtered / row_sums\n        else:\n            # No content tokens, use original\n            attn_filtered = attn\n            filtered_tokens = tokens\n    else:\n        # No filtering, use all tokens\n        attn_filtered = attn\n        filtered_tokens = tokens\n    \n    # 4. Symmetrize (make undirected)\n    attn_sym = (attn_filtered + attn_filtered.T) / 2\n    \n    # 5. Convert to distance: d = 1 - attention\n    distance_matrix = 1 - attn_sym\n    \n    # Ensure diagonal is 0 and symmetric\n    np.fill_diagonal(distance_matrix, 0)\n    \n    return distance_matrix, filtered_tokens\n\n\nprint(\"✓ Function defined: build_distance_matrix()\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test on Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first example\n",
    "idx = 0\n",
    "example = results[idx]\n",
    "\n",
    "print(f\"Example {idx}:\")\n",
    "print(f\"English: {example['en_text']}\")\n",
    "print(f\"French:  {example['fr_text']}\")\n",
    "print()\n",
    "\n",
    "# Build distance matrices\n",
    "en_dist, en_tokens = build_distance_matrix(example['en_attention'], example['en_tokens'])\n",
    "fr_dist, fr_tokens = build_distance_matrix(example['fr_attention'], example['fr_tokens'])\n",
    "\n",
    "print(f\"English distance matrix: {en_dist.shape}\")\n",
    "print(f\"  Content tokens: {en_tokens}\")\n",
    "print(f\"  Min distance: {en_dist.min():.4f}\")\n",
    "print(f\"  Max distance: {en_dist.max():.4f}\")\n",
    "print(f\"  Mean distance: {en_dist.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"French distance matrix: {fr_dist.shape}\")\n",
    "print(f\"  Content tokens: {fr_tokens}\")\n",
    "print(f\"  Min distance: {fr_dist.min():.4f}\")\n",
    "print(f\"  Max distance: {fr_dist.max():.4f}\")\n",
    "print(f\"  Mean distance: {fr_dist.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distance Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distance matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# English\n",
    "sns.heatmap(en_dist, xticklabels=en_tokens, yticklabels=en_tokens, \n",
    "            cmap='RdYlBu_r', ax=ax1, square=True,\n",
    "            cbar_kws={'label': 'Distance (1 - attention)'})\n",
    "ax1.set_title(f'English Distance Matrix\\n{example[\"en_text\"][:50]}...')\n",
    "ax1.set_xlabel('Tokens')\n",
    "ax1.set_ylabel('Tokens')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# French\n",
    "sns.heatmap(fr_dist, xticklabels=fr_tokens, yticklabels=fr_tokens, \n",
    "            cmap='RdYlBu_r', ax=ax2, square=True,\n",
    "            cbar_kws={'label': 'Distance (1 - attention)'})\n",
    "ax2.set_title(f'French Distance Matrix\\n{example[\"fr_text\"][:50]}...')\n",
    "ax2.set_xlabel('Tokens')\n",
    "ax2.set_ylabel('Tokens')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Persistent Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute Vietoris-Rips persistent homology using ripser\n# maxdim=1 computes β₀ (connected components) and β₁ (loops)\n\nprint(\"✓ Computing Vietoris-Rips persistence using ripser\")\nprint(f\"  Computing: β₀ (connected components) and β₁ (1-dimensional holes/loops)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute persistence for English\n# ripser expects distance matrix, returns dict with 'dgms' key\nen_result = ripser(en_dist, maxdim=1, distance_matrix=True)\nen_diagrams = en_result['dgms']  # List of diagrams: [H0, H1]\n\nprint(\"English persistence diagrams:\")\nprint(f\"  H0 (β₀) shape: {en_diagrams[0].shape}\")\nprint(f\"  H1 (β₁) shape: {en_diagrams[1].shape}\")\nprint(f\"  Format: (birth, death)\")\nprint(f\"\\nFirst 5 H0 features:\")\nprint(en_diagrams[0][:5])\nprint(f\"\\nFirst 5 H1 features:\")\nprint(en_diagrams[1][:5] if len(en_diagrams[1]) > 0 else \"None\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute persistence for French\nfr_result = ripser(fr_dist, maxdim=1, distance_matrix=True)\nfr_diagrams = fr_result['dgms']\n\nprint(\"French persistence diagrams:\")\nprint(f\"  H0 (β₀) shape: {fr_diagrams[0].shape}\")\nprint(f\"  H1 (β₁) shape: {fr_diagrams[1].shape}\")\nprint(f\"\\nFirst 5 H0 features:\")\nprint(fr_diagrams[0][:5])\nprint(f\"\\nFirst 5 H1 features:\")\nprint(fr_diagrams[1][:5] if len(fr_diagrams[1]) > 0 else \"None\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Persistence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count features by dimension\ndef analyze_diagram(diagrams, language=\"\"):\n    \"\"\"\n    Analyze persistence diagrams from ripser.\n    diagrams is a list: [H0, H1] where each is (n_features, 2) array\n    \"\"\"\n    dim0 = diagrams[0]  # H0 features (birth, death)\n    dim1 = diagrams[1]  # H1 features\n    \n    # Remove infinite persistence (last feature in H0 is typically infinite)\n    dim0_finite = dim0[np.isfinite(dim0).all(axis=1)]\n    dim1_finite = dim1[np.isfinite(dim1).all(axis=1)]\n    \n    # Persistence = death - birth\n    persistence0 = dim0_finite[:, 1] - dim0_finite[:, 0]\n    persistence1 = dim1_finite[:, 1] - dim1_finite[:, 0]\n    \n    print(f\"{language} Persistence Summary:\")\n    print(\"=\"*50)\n    print(f\"β₀ features (connected components): {len(dim0_finite)}\")\n    if len(persistence0) > 0:\n        print(f\"  Max persistence: {persistence0.max():.4f}\")\n        print(f\"  Mean persistence: {persistence0.mean():.4f}\")\n    \n    print(f\"\\nβ₁ features (loops/holes): {len(dim1_finite)}\")\n    if len(persistence1) > 0:\n        print(f\"  Max persistence: {persistence1.max():.4f}\")\n        print(f\"  Mean persistence: {persistence1.mean():.4f}\")\n    else:\n        print(f\"  No H1 features detected\")\n    print()\n\nanalyze_diagram(en_diagrams, \"English\")\nanalyze_diagram(fr_diagrams, \"French\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Persistence Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot English persistence diagram\nplot_diagrams(en_diagrams, show=False)\nplt.title(f'English Persistence Diagram\\n{example[\"en_text\"][:60]}...')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot French persistence diagram\nplot_diagrams(fr_diagrams, show=False)\nplt.title(f'French Persistence Diagram\\n{example[\"fr_text\"][:60]}...')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Persistence Diagrams\n",
    "\n",
    "Use Wasserstein distance to measure similarity between English and French topologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute Wasserstein distance between diagrams using persim\n# We compute for both H0 and H1 separately, then can combine\n\n# Wasserstein distance for H0 (connected components)\nw_dist_h0 = wasserstein(en_diagrams[0], fr_diagrams[0])\n\n# Wasserstein distance for H1 (loops) - handle case where one diagram is empty\nif len(en_diagrams[1]) > 0 and len(fr_diagrams[1]) > 0:\n    w_dist_h1 = wasserstein(en_diagrams[1], fr_diagrams[1])\nelse:\n    # If one has no H1 features, compute distance to empty diagram\n    w_dist_h1 = wasserstein(en_diagrams[1], fr_diagrams[1])\n\n# Total distance (can weight differently, but simple sum for now)\ntotal_w_dist = w_dist_h0 + w_dist_h1\n\nprint(\"Wasserstein Distances:\")\nprint(\"=\"*50)\nprint(f\"H0 (connected components): {w_dist_h0:.6f}\")\nprint(f\"H1 (loops/holes):          {w_dist_h1:.6f}\")\nprint(f\"Total:                     {total_w_dist:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process Multiple Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process first 10 examples\nnum_examples = 10\nwasserstein_distances = []\n\nprint(f\"Computing persistent homology for {num_examples} sentence pairs...\\n\")\n\nfor idx in range(num_examples):\n    example = results[idx]\n    \n    # Build distance matrices\n    en_dist, _ = build_distance_matrix(example['en_attention'], example['en_tokens'])\n    fr_dist, _ = build_distance_matrix(example['fr_attention'], example['fr_tokens'])\n    \n    # Compute persistence\n    en_result = ripser(en_dist, maxdim=1, distance_matrix=True)\n    fr_result = ripser(fr_dist, maxdim=1, distance_matrix=True)\n    \n    en_diagrams = en_result['dgms']\n    fr_diagrams = fr_result['dgms']\n    \n    # Compute Wasserstein distances\n    w_dist_h0 = wasserstein(en_diagrams[0], fr_diagrams[0])\n    w_dist_h1 = wasserstein(en_diagrams[1], fr_diagrams[1])\n    total_w_dist = w_dist_h0 + w_dist_h1\n    \n    wasserstein_distances.append({\n        'idx': idx,\n        'en_text': example['en_text'],\n        'fr_text': example['fr_text'],\n        'wasserstein_distance': total_w_dist,\n        'wasserstein_h0': w_dist_h0,\n        'wasserstein_h1': w_dist_h1,\n        'en_diagrams': en_diagrams,\n        'fr_diagrams': fr_diagrams\n    })\n    \n    print(f\"[{idx}] W-dist: {total_w_dist:.6f} (H0: {w_dist_h0:.4f}, H1: {w_dist_h1:.4f}) | EN: {example['en_text'][:40]}...\")\n\nprint(f\"\\n✓ Processed {num_examples} examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "w_dists = [d['wasserstein_distance'] for d in wasserstein_distances]\n",
    "\n",
    "print(\"Wasserstein Distance Statistics (first 10 pairs):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Min:    {np.min(w_dists):.6f}\")\n",
    "print(f\"Max:    {np.max(w_dists):.6f}\")\n",
    "print(f\"Mean:   {np.mean(w_dists):.6f}\")\n",
    "print(f\"Median: {np.median(w_dists):.6f}\")\n",
    "print(f\"Std:    {np.std(w_dists):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(w_dists, bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.axvline(np.mean(w_dists), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(w_dists):.6f}')\n",
    "plt.xlabel('Wasserstein Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Topological Similarity (English vs French)\\nLower = More Similar')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare High vs Low Similarity Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most and least topologically similar pairs\n",
    "sorted_by_similarity = sorted(wasserstein_distances, key=lambda x: x['wasserstein_distance'])\n",
    "\n",
    "print(\"Most topologically similar (lowest Wasserstein distance):\")\n",
    "print(\"=\"*70)\n",
    "most_similar = sorted_by_similarity[0]\n",
    "print(f\"Pair {most_similar['idx']}: W-dist = {most_similar['wasserstein_distance']:.6f}\")\n",
    "print(f\"  EN: {most_similar['en_text']}\")\n",
    "print(f\"  FR: {most_similar['fr_text']}\")\n",
    "print()\n",
    "\n",
    "print(\"Least topologically similar (highest Wasserstein distance):\")\n",
    "print(\"=\"*70)\n",
    "least_similar = sorted_by_similarity[-1]\n",
    "print(f\"Pair {least_similar['idx']}: W-dist = {least_similar['wasserstein_distance']:.6f}\")\n",
    "print(f\"  EN: {least_similar['en_text']}\")\n",
    "print(f\"  FR: {least_similar['fr_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot persistence diagrams for most similar pair\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# English\nplt.sca(ax1)\nplot_diagrams(most_similar['en_diagrams'], show=False)\nax1.set_title('English')\n\n# French\nplt.sca(ax2)\nplot_diagrams(most_similar['fr_diagrams'], show=False)\nax2.set_title('French')\n\nfig.suptitle(f\"Most Topologically Similar Pair (W-dist: {most_similar['wasserstein_distance']:.6f})\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Successfully computed topological features!**\n",
    "\n",
    "**What we learned:**\n",
    "- Built distance matrices from attention weights\n",
    "- Computed Vietoris-Rips persistent homology (β₀, β₁)\n",
    "- Visualized persistence diagrams\n",
    "- Measured topological similarity using Wasserstein distance\n",
    "- Analyzed first 10 sentence pairs\n",
    "\n",
    "**Next steps:**\n",
    "1. Scale to all 2000 sentence pairs\n",
    "2. Compute BLEU scores for translation quality\n",
    "3. Correlate Wasserstein distance with BLEU scores\n",
    "4. Statistical analysis and visualization\n",
    "\n",
    "**Key findings so far:**\n",
    "- Wasserstein distances range from ~{min} to ~{max}\n",
    "- Both languages show β₀ and β₁ features\n",
    "- Topological structure varies across sentence pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}