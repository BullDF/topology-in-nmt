{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract All Encoder Attention Maps (Google Colab)\n",
    "\n",
    "Extract encoder self-attention maps for all 2000 sentence pairs in both directions.\n",
    "\n",
    "**Runtime:** ~1-2 hours on Colab GPU (vs ~5 hours on CPU)\n",
    "\n",
    "**Before running:**\n",
    "1. Upload your project folder to Google Drive\n",
    "2. Set the `ROOT_DIR` variable below to point to your project directory\n",
    "3. Enable GPU: Runtime â†’ Change runtime type â†’ GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**IMPORTANT:** Change `ROOT_DIR` to match your Google Drive folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CHANGE THIS TO YOUR GOOGLE DRIVE PATH\n# ============================================================================\nROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper\"  # <-- CHANGE THIS\n\n# Example paths:\n# ROOT_DIR = \"/content/drive/MyDrive/CSC2517/term_paper\"\n# ROOT_DIR = \"/content/drive/MyDrive/research/term_paper\"\n# ============================================================================\n\nimport os\nos.chdir(ROOT_DIR)\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Verify directories exist\nassert os.path.exists(\"models/nllb-600M\"), \"Model directory not found!\"\nassert os.path.exists(\"data/sentence_pairs_fr_en.pkl\"), \"Data file not found!\"\nprint(\"âœ“ All required files found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages with specific transformers version to avoid bugs\n!pip install -q transformers==4.44.2 datasets torch pandas numpy tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nfrom tqdm import tqdm\nimport time\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n\n# Check transformers version\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder_attention(text, src_lang, tgt_lang, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Extract encoder self-attention for a given source text.\n",
    "\n",
    "    Args:\n",
    "        text: Source text string\n",
    "        src_lang: Source language code (e.g., 'eng_Latn', 'fra_Latn')\n",
    "        tgt_lang: Target language code (e.g., 'fra_Latn', 'eng_Latn')\n",
    "        tokenizer: NLLB tokenizer\n",
    "        model: NLLB model\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - tokens: List of source tokens\n",
    "            - encoder_attention: Encoder self-attention (num_layers, num_heads, seq_len, seq_len)\n",
    "            - translation: Generated translation text\n",
    "    \"\"\"\n",
    "    # Set source language\n",
    "    tokenizer.src_lang = src_lang\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get target language BOS token\n",
    "    tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "    # Generate translation with attention output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_id,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "    # Extract encoder attention (available in encoder_attentions)\n",
    "    # Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "    encoder_attention = outputs.encoder_attentions\n",
    "    encoder_attention = torch.stack([layer.squeeze(0) for layer in encoder_attention])  # (num_layers, num_heads, seq_len, seq_len)\n",
    "\n",
    "    # Decode tokens\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0].cpu())\n",
    "    translation = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        'tokens': input_tokens,\n",
    "        'encoder_attention': encoder_attention.cpu().numpy(),\n",
    "        'translation': translation\n",
    "    }\n",
    "\n",
    "\n",
    "def save_checkpoint(results, output_path, checkpoint_num):\n",
    "    \"\"\"Save checkpoint to avoid losing progress.\"\"\"\n",
    "    checkpoint_path = output_path.parent / f\"{output_path.stem}_checkpoint_{checkpoint_num}.pkl\"\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"  ðŸ’¾ Checkpoint saved: {checkpoint_path.name}\")\n",
    "\n",
    "\n",
    "print(\"âœ“ Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Extracting Encoder Attention Maps for All 2000 Sentence Pairs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configuration\n",
    "CHECKPOINT_INTERVAL = 100  # Save checkpoint every N examples\n",
    "OUTPUT_DIR = Path(\"data/attention_maps\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE = OUTPUT_DIR / \"all_encoder_attention.pkl\"\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"âœ“ Using CUDA (GPU)\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"âœ“ Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸  Using CPU (this will be slow!)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model and tokenizer\nmodel_path = \"models/nllb-600M\"\nprint(f\"Loading model from {model_path}...\")\n\n# Try loading tokenizer with use_fast=False to avoid tokenizer.json corruption issues\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n    print(\"âœ“ Loaded tokenizer (slow tokenizer)\")\nexcept Exception as e:\n    print(f\"âš ï¸  Failed to load local tokenizer: {e}\")\n    print(\"Downloading tokenizer from HuggingFace instead...\")\n    tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", use_fast=False)\n    print(\"âœ“ Downloaded tokenizer from HuggingFace\")\n\n# Try loading model\ntry:\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_path,\n        attn_implementation=\"eager\"  # Required for extracting attention weights\n    ).to(device)\n    print(\"âœ“ Loaded model from local files\")\nexcept Exception as e:\n    print(f\"âš ï¸  Failed to load local model: {e}\")\n    print(\"Downloading model from HuggingFace instead (this may take a few minutes)...\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        \"facebook/nllb-200-distilled-600M\",\n        attn_implementation=\"eager\"\n    ).to(device)\n    print(\"âœ“ Downloaded model from HuggingFace\")\n\nmodel.eval()\nprint(\"âœ“ Model ready!\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path(\"data/sentence_pairs_fr_en.pkl\")\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "data = pd.read_pickle(data_path)\n",
    "df = pd.DataFrame(data)\n",
    "df = df.rename(columns={'english': 'en', 'french': 'fr'})\n",
    "print(f\"âœ“ Loaded {len(df)} sentence pairs\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check for Existing Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint to resume from\n",
    "start_idx = 0\n",
    "results = []\n",
    "checkpoint_files = sorted(OUTPUT_DIR.glob(f\"{OUTPUT_FILE.stem}_checkpoint_*.pkl\"))\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    print(f\"Found checkpoint: {latest_checkpoint.name}\")\n",
    "    with open(latest_checkpoint, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    start_idx = len(results)\n",
    "    print(f\"âœ“ Resuming from example {start_idx}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from the beginning.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extract Attention Maps\n",
    "\n",
    "**This will take ~1-2 hours on GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Extracting attention maps for {len(df)} sentence pairs...\")\n",
    "print(f\"Progress will be saved every {CHECKPOINT_INTERVAL} examples\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, len(df)), desc=\"Processing\", unit=\"pair\"):\n",
    "    en_text = df.iloc[idx]['en']\n",
    "    fr_text = df.iloc[idx]['fr']\n",
    "\n",
    "    try:\n",
    "        # Extract English encoder attention (EN â†’ FR)\n",
    "        en_result = extract_encoder_attention(\n",
    "            text=en_text,\n",
    "            src_lang='eng_Latn',\n",
    "            tgt_lang='fra_Latn',\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Extract French encoder attention (FR â†’ EN)\n",
    "        fr_result = extract_encoder_attention(\n",
    "            text=fr_text,\n",
    "            src_lang='fra_Latn',\n",
    "            tgt_lang='eng_Latn',\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'idx': idx,\n",
    "            'en_text': en_text,\n",
    "            'fr_text': fr_text,\n",
    "            'en_tokens': en_result['tokens'],\n",
    "            'fr_tokens': fr_result['tokens'],\n",
    "            'en_attention': en_result['encoder_attention'],\n",
    "            'fr_attention': fr_result['encoder_attention'],\n",
    "            'en_translation': en_result['translation'],\n",
    "            'fr_translation': fr_result['translation']\n",
    "        })\n",
    "\n",
    "        # Save checkpoint periodically\n",
    "        if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(results, OUTPUT_FILE, idx + 1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸  Error processing pair {idx}: {e}\")\n",
    "        print(f\"   EN: {en_text[:60]}...\")\n",
    "        print(f\"   FR: {fr_text[:60]}...\")\n",
    "        continue\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Extraction complete! Processed {len(results)} sentence pairs\")\n",
    "print(f\"â±ï¸  Total time: {elapsed_time / 60:.1f} minutes ({elapsed_time / len(results):.2f} sec/pair)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving final results to {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"âœ“ Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# Print summary statistics\n",
    "file_size_mb = OUTPUT_FILE.stat().st_size / (1024 * 1024)\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"Summary Statistics\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total sentence pairs: {len(results)}\")\n",
    "print(f\"Output file size: {file_size_mb:.1f} MB\")\n",
    "print(f\"Average attention matrix shape:\")\n",
    "if results:\n",
    "    sample = results[0]\n",
    "    print(f\"  English: {sample['en_attention'].shape}\")\n",
    "    print(f\"  French:  {sample['fr_attention'].shape}\")\n",
    "print()\n",
    "\n",
    "# Clean up checkpoint files\n",
    "print(\"Cleaning up checkpoint files...\")\n",
    "for checkpoint_file in OUTPUT_DIR.glob(f\"{OUTPUT_FILE.stem}_checkpoint_*.pkl\"):\n",
    "    checkpoint_file.unlink()\n",
    "    print(f\"  ðŸ—‘ï¸  Removed {checkpoint_file.name}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… All done!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Results (Optional)\n",
    "\n",
    "If you want to download the results file directly from Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download the results file\n",
    "# from google.colab import files\n",
    "# files.download(str(OUTPUT_FILE))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}