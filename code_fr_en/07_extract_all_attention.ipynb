{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Extract All Encoder Attention Maps\n\nExtract encoder self-attention maps for all 2000 sentence pairs in both directions.\n\n**For Google Colab:**\n1. Mount Google Drive (run cell below)\n2. Set `ROOT_DIR` to your project folder path in code_fr_en\n3. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n4. Runtime: ~1-2 hours on Colab GPU (vs ~5 hours on CPU)\n\n**For local execution:** Skip the Google Drive cell and run from \"Import Libraries\"\n\n---"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Mount Google Drive (only needed for Google Colab)\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # IMPORTANT: Set this to your code_fr_en directory path\n    # This should point to where THIS notebook is located\n    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_fr_en\"\n    \n    import os\n    os.chdir(ROOT_DIR)\n    print(f\"‚úì Changed to: {os.getcwd()}\")\nexcept ImportError:\n    print(\"Not running on Colab, using local environment\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify working directory and required files\nimport os\nfrom pathlib import Path\n\nprint(f\"Current directory: {os.getcwd()}\")\n\n# Check model\nmodel_path = \"../models/nllb-1.3B\"\nif os.path.exists(model_path):\n    print(f\"‚úì Model directory exists: {model_path}\")\nelse:\n    print(f\"‚úó Model directory NOT found: {model_path}\")\n\n# Check data\ndata_path = \"../data/sentence_pairs_fr_en.pkl\"\nif os.path.exists(data_path):\n    print(f\"‚úì Data file exists: {data_path}\")\nelse:\n    print(f\"‚úó Data file NOT found: {data_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Import Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nfrom tqdm import tqdm\nimport time\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Define Attention Extraction Function\n\n**Key modification:** Only extract the **last encoder layer** (layer 23 out of 24) to save memory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages if running on Colab\ntry:\n    import google.colab\n    print(\"Installing packages for Colab...\")\n    import subprocess\n    subprocess.run(['pip', 'install', '-q', 'transformers', 'datasets', 'torch', 'pandas', 'numpy', 'tqdm'], check=True)\n    print(\"‚úì Packages installed\")\nexcept ImportError:\n    print(\"Not on Colab, skipping package installation\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Load Model and Data"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Device setup\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA (NVIDIA GPU)\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Using MPS (Apple Silicon)\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\n# Load model and tokenizer with eager attention (required for output_attentions=True)\nmodel_path = \"../models/nllb-1.3B\"\nprint(f\"Loading model from {model_path}...\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n    attn_implementation=\"eager\"  # Required for extracting attention weights\n).to(device)\nmodel.eval()\nprint(\"‚úì Model loaded successfully!\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_encoder_attention(text, src_lang, tgt_lang, tokenizer, model, device):\n    \"\"\"\n    Extract LAST LAYER encoder self-attention for a given source text.\n    \n    Args:\n        text: Source text string\n        src_lang: Source language code (e.g., 'eng_Latn', 'fra_Latn')\n        tgt_lang: Target language code (e.g., 'fra_Latn', 'eng_Latn')\n        tokenizer: NLLB tokenizer\n        model: NLLB model (1.3B has 24 encoder layers)\n        device: torch device\n    \n    Returns:\n        dict with keys:\n            - tokens: List of source tokens\n            - encoder_attention: LAST LAYER encoder self-attention (num_heads, seq_len, seq_len)\n            - translation: Generated translation text\n    \"\"\"\n    # Set source language\n    tokenizer.src_lang = src_lang\n    \n    # Tokenize input\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    \n    # Get target language BOS token\n    tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n    \n    # Generate translation with attention output\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            forced_bos_token_id=tgt_lang_id,\n            output_attentions=True,\n            return_dict_in_generate=True,\n            max_length=128\n        )\n    \n    # Extract ONLY the last encoder layer attention (layer 23 out of 24 layers)\n    # outputs.encoder_attentions is a tuple of (num_layers,)\n    # Each element has shape: (batch_size, num_heads, seq_len, seq_len)\n    last_layer_attention = outputs.encoder_attentions[-1]  # Get last layer\n    last_layer_attention = last_layer_attention.squeeze(0)  # Remove batch dimension -> (num_heads, seq_len, seq_len)\n    \n    # Decode tokens\n    input_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0].cpu())\n    translation = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    \n    return {\n        'tokens': input_tokens,\n        'encoder_attention': last_layer_attention.cpu().numpy().astype(np.float32),  # (num_heads, seq_len, seq_len)\n        'translation': translation\n    }\n\n\ndef save_checkpoint(results, output_path, checkpoint_num):\n    \"\"\"Save checkpoint to avoid losing progress.\"\"\"\n    checkpoint_path = output_path.parent / f\"{output_path.stem}_checkpoint_{checkpoint_num}.pkl\"\n    with open(checkpoint_path, 'wb') as f:\n        pickle.dump(results, f)\n    print(f\"  üíæ Checkpoint saved: {checkpoint_path.name}\")\n\n\nprint(\"‚úì Functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"Extracting LAST LAYER Encoder Attention Maps for All 2000 Sentence Pairs\")\nprint(\"=\"*80)\n\n# Configuration\nCHECKPOINT_INTERVAL = 100  # Save checkpoint every N examples\nOUTPUT_DIR = Path(\"../data/attention_maps_fr_en\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUT_FILE = OUTPUT_DIR / \"all_encoder_attention_last_layer.pkl\"\n\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Output file: {OUTPUT_FILE.name}\")\nprint(f\"Checkpoint interval: {CHECKPOINT_INTERVAL} examples\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\ndata_path = Path(\"../data/sentence_pairs_fr_en.pkl\")\nprint(f\"Loading data from {data_path}...\")\ndata = pd.read_pickle(data_path)\ndf = pd.DataFrame(data)\ndf = df.rename(columns={'english': 'en', 'french': 'fr'})\nprint(f\"‚úì Loaded {len(df)} sentence pairs\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Check for Existing Checkpoint"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check for existing checkpoint to resume from\nstart_idx = 0\nresults = []\ncheckpoint_files = sorted(OUTPUT_DIR.glob(f\"{OUTPUT_FILE.stem}_checkpoint_*.pkl\"))\n\nif checkpoint_files:\n    latest_checkpoint = checkpoint_files[-1]\n    print(f\"Found checkpoint: {latest_checkpoint.name}\")\n    with open(latest_checkpoint, 'rb') as f:\n        results = pickle.load(f)\n    start_idx = len(results)\n    print(f\"‚úì Resuming from example {start_idx}\")\n    print()\nelse:\n    print(\"No checkpoint found. Starting from the beginning.\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Extract Attention Maps\n\n**Note:** This extracts only the **last encoder layer** (layer 23 out of 24) to save memory.\n\n**Runtime:** ~1-2 hours on GPU"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(f\"Extracting attention maps for {len(df)} sentence pairs...\")\nprint(f\"Progress will be saved every {CHECKPOINT_INTERVAL} examples\")\nprint()\n\nstart_time = time.time()\n\nfor idx in tqdm(range(start_idx, len(df)), desc=\"Processing\", unit=\"pair\"):\n    en_text = df.iloc[idx]['en']\n    fr_text = df.iloc[idx]['fr']\n    \n    try:\n        # Extract English encoder attention (EN ‚Üí FR)\n        en_result = extract_encoder_attention(\n            text=en_text,\n            src_lang='eng_Latn',\n            tgt_lang='fra_Latn',\n            tokenizer=tokenizer,\n            model=model,\n            device=device\n        )\n        \n        # Extract French encoder attention (FR ‚Üí EN)\n        fr_result = extract_encoder_attention(\n            text=fr_text,\n            src_lang='fra_Latn',\n            tgt_lang='eng_Latn',\n            tokenizer=tokenizer,\n            model=model,\n            device=device\n        )\n        \n        # Store results\n        results.append({\n            'idx': idx,\n            'en_text': en_text,\n            'fr_text': fr_text,\n            'en_tokens': en_result['tokens'],\n            'fr_tokens': fr_result['tokens'],\n            'en_attention': en_result['encoder_attention'],  # (num_heads, seq_len, seq_len)\n            'fr_attention': fr_result['encoder_attention'],  # (num_heads, seq_len, seq_len)\n            'en_translation': en_result['translation'],\n            'fr_translation': fr_result['translation']\n        })\n        \n        # Save checkpoint periodically\n        if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n            save_checkpoint(results, OUTPUT_FILE, idx + 1)\n    \n    except Exception as e:\n        print(f\"\\n‚ö†Ô∏è  Error processing pair {idx}: {e}\")\n        print(f\"   EN: {en_text[:60]}...\")\n        print(f\"   FR: {fr_text[:60]}...\")\n        continue\n\nelapsed_time = time.time() - start_time\n\nprint()\nprint(\"=\"*80)\nprint(f\"‚úì Extraction complete! Processed {len(results)} sentence pairs\")\nprint(f\"‚è±Ô∏è  Total time: {elapsed_time / 60:.1f} minutes ({elapsed_time / len(results):.2f} sec/pair)\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Save Final Results"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(f\"Saving final results to {OUTPUT_FILE}...\")\nwith open(OUTPUT_FILE, 'wb') as f:\n    pickle.dump(results, f)\nprint(f\"‚úì Saved to {OUTPUT_FILE}\")\n\n# Print summary statistics\nfile_size_mb = OUTPUT_FILE.stat().st_size / (1024 * 1024)\nprint()\nprint(\"=\"*80)\nprint(\"Summary Statistics\")\nprint(\"=\"*80)\nprint(f\"Total sentence pairs: {len(results)}\")\nprint(f\"Output file size: {file_size_mb:.1f} MB\")\nprint(f\"Average attention matrix shape (LAST LAYER ONLY):\")\nif results:\n    sample = results[0]\n    print(f\"  English: {sample['en_attention'].shape} (num_heads, seq_len, seq_len)\")\n    print(f\"  French:  {sample['fr_attention'].shape} (num_heads, seq_len, seq_len)\")\nprint()\n\n# Clean up checkpoint files\nprint(\"Cleaning up checkpoint files...\")\nfor checkpoint_file in OUTPUT_DIR.glob(f\"{OUTPUT_FILE.stem}_checkpoint_*.pkl\"):\n    checkpoint_file.unlink()\n    print(f\"  üóëÔ∏è  Removed {checkpoint_file.name}\")\n\nprint()\nprint(\"=\"*80)\nprint(\"‚úÖ All done!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nThis notebook extracts encoder self-attention maps for all 2000 sentence pairs in both directions.\n\n**Key changes from previous version:**\n- ‚úÖ **Only extracts last encoder layer (layer 23 out of 24)** to save memory (~24x less storage)\n- ‚úÖ Supports both Colab and local environments\n- ‚úÖ Upgraded to NLLB-1.3B model (24 encoder layers, 16 attention heads per layer)\n- ‚úÖ GPU acceleration (CUDA/MPS) with float16 precision on CUDA\n- ‚úÖ Checkpoint system to resume from interruptions\n\n**Output format:**\n- File: `all_encoder_attention_last_layer.pkl`\n- Each entry contains:\n  - `en_attention`: (16, seq_len, seq_len) - 16 attention heads from last layer\n  - `fr_attention`: (16, seq_len, seq_len) - 16 attention heads from last layer\n  \n**Next steps:**\n- Use this data for TDA analysis (persistent homology)\n- Compare topological structure across languages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download the results file\n",
    "# from google.colab import files\n",
    "# files.download(str(OUTPUT_FILE))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}