{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Extract Attention Maps from NLLB-1.3B Model\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path\n",
    "3. Run the rest of the notebook\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell and run from \"Import Libraries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only needed for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # IMPORTANT: Set this to your code_fr_en directory path\n",
    "    # This should point to where THIS notebook is located\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_fr_en\"\n",
    "    \n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"✓ Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab, using local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify working directory and required files\n",
    "import os\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check model\n",
    "model_path = \"../models/nllb-1.3B\"\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"✓ Model directory exists: {model_path}\")\n",
    "else:\n",
    "    print(f\"✗ Model directory NOT found: {model_path}\")\n",
    "\n",
    "# Check data\n",
    "data_path = \"../data/wmt14_fr_en_validation_2000\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"✓ Data directory exists: {data_path}\")\n",
    "else:\n",
    "    print(f\"✗ Data directory NOT found: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 1. Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with eager attention implementation (required for attention output)\n",
    "model_dir = \"../models/nllb-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_dir,\n",
    "    attn_implementation=\"eager\"  # Required for output_attentions=True\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "print(f\"Attention implementation: eager\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(\"../data/wmt14_fr_en_validation_2000\")\n",
    "print(f\"\\nLoaded {len(dataset)} sentence pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Extract attention from a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first example\n",
    "example = dataset[0][\"translation\"]\n",
    "english = example[\"en\"]\n",
    "french = example[\"fr\"]\n",
    "\n",
    "print(f\"English: {english}\")\n",
    "print(f\"French:  {french}\")\n",
    "\n",
    "# Tokenize English input\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "inputs = tokenizer(english, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"\\nInput shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translation with attention output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n",
    "        max_length=100,\n",
    "        output_attentions=True,  # IMPORTANT: Enable attention output\n",
    "        return_dict_in_generate=True  # Return structured output\n",
    "    )\n",
    "\n",
    "# Decode translation\n",
    "translation = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"\\nOutput tokens: {tokenizer.convert_ids_to_tokens(outputs.sequences[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 3. Understand attention structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output structure\n",
    "print(\"Output keys:\", outputs.keys())\n",
    "print(\"\\nAttention types available:\")\n",
    "if hasattr(outputs, 'encoder_attentions') and outputs.encoder_attentions is not None:\n",
    "    print(f\"  - Encoder self-attention: {len(outputs.encoder_attentions)} layers\")\n",
    "if hasattr(outputs, 'decoder_attentions') and outputs.decoder_attentions is not None:\n",
    "    print(f\"  - Decoder self-attention: {len(outputs.decoder_attentions)} timesteps\")\n",
    "if hasattr(outputs, 'cross_attentions') and outputs.cross_attentions is not None:\n",
    "    print(f\"  - Cross-attention: {len(outputs.cross_attentions)} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine encoder attention structure\n",
    "if outputs.encoder_attentions is not None:\n",
    "    encoder_attn = outputs.encoder_attentions\n",
    "    print(f\"Encoder attention:\")\n",
    "    print(f\"  Number of layers: {len(encoder_attn)}\")\n",
    "    print(f\"  Shape per layer: {encoder_attn[0].shape}\")  # (batch, heads, seq_len, seq_len)\n",
    "    print(f\"  Format: (batch_size, num_heads, seq_length, seq_length)\")\n",
    "    \n",
    "    # Get last layer attention\n",
    "    last_layer_attn = encoder_attn[-1][0]  # Remove batch dimension\n",
    "    print(f\"\\n  Last layer shape: {last_layer_attn.shape}\")\n",
    "    print(f\"  Number of attention heads: {last_layer_attn.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine decoder and cross-attention structure\n",
    "if outputs.decoder_attentions is not None:\n",
    "    print(f\"\\nDecoder self-attention:\")\n",
    "    print(f\"  Number of timesteps: {len(outputs.decoder_attentions)}\")\n",
    "    print(f\"  Each timestep contains {len(outputs.decoder_attentions[0])} layers\")\n",
    "    print(f\"  Shape format: (batch_size, num_heads, query_length, key_length)\")\n",
    "    print(f\"  Shape varies per timestep due to causal masking:\")\n",
    "    # Show first few timesteps to illustrate the pattern\n",
    "    for t in range(min(3, len(outputs.decoder_attentions))):\n",
    "        shape = outputs.decoder_attentions[t][0].shape\n",
    "        print(f\"    Timestep {t}: {shape}\")\n",
    "\n",
    "if outputs.cross_attentions is not None:\n",
    "    print(f\"\\nCross-attention (decoder attending to encoder):\")\n",
    "    print(f\"  Number of timesteps: {len(outputs.cross_attentions)}\")\n",
    "    print(f\"  Each timestep contains {len(outputs.cross_attentions[0])} layers\")\n",
    "    print(f\"  Shape format: (batch_size, num_heads, decoder_length, encoder_length)\")\n",
    "    print(f\"  Shape varies per timestep:\")\n",
    "    for t in range(min(3, len(outputs.cross_attentions))):\n",
    "        shape = outputs.cross_attentions[t][0].shape\n",
    "        print(f\"    Timestep {t}: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4. Visualize encoder self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoder attention from last layer\n",
    "encoder_attn_last = outputs.encoder_attentions[-1][0].cpu().numpy()  # (heads, seq_len, seq_len)\n",
    "\n",
    "# Average over all attention heads\n",
    "encoder_attn_avg = encoder_attn_last.mean(axis=0)  # (seq_len, seq_len)\n",
    "\n",
    "# Get tokens for axis labels\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(encoder_attn_avg, \n",
    "            xticklabels=input_tokens, \n",
    "            yticklabels=input_tokens,\n",
    "            cmap='viridis',\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Encoder Self-Attention (Last Layer, Averaged over Heads)\\nEnglish Sentence')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Attention matrix shape: {encoder_attn_avg.shape}\")\n",
    "print(f\"Min attention: {encoder_attn_avg.min():.4f}\")\n",
    "print(f\"Max attention: {encoder_attn_avg.max():.4f}\")\n",
    "print(f\"Mean attention: {encoder_attn_avg.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "6hhldeeligi",
   "source": "# Filter out special tokens and renormalize attention\n\n# Identify special tokens to filter (BOS, EOS, language codes, padding)\nspecial_tokens = {'</s>', '<s>', '<pad>', 'eng_Latn', 'fra_Latn', 'zho_Hans'}\n\n# Create mask for non-special tokens\nmask = [token not in special_tokens for token in input_tokens]\nfiltered_indices = [i for i, keep in enumerate(mask) if keep]\n\nprint(f\"Original tokens ({len(input_tokens)}): {input_tokens}\")\nprint(f\"Filtered tokens ({len(filtered_indices)}): {[input_tokens[i] for i in filtered_indices]}\")\nprint(f\"Removed tokens: {[input_tokens[i] for i, keep in enumerate(mask) if not keep]}\")\n\n# Filter attention matrix (remove special token rows and columns)\nencoder_attn_filtered = encoder_attn_avg[np.ix_(filtered_indices, filtered_indices)]\n\n# Renormalize so each row sums to 1\nrow_sums = encoder_attn_filtered.sum(axis=1, keepdims=True)\nencoder_attn_normalized = encoder_attn_filtered / row_sums\n\n# Get filtered tokens for axis labels\nfiltered_tokens = [input_tokens[i] for i in filtered_indices]\n\n# Plot filtered and normalized heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(encoder_attn_normalized, \n            xticklabels=filtered_tokens, \n            yticklabels=filtered_tokens,\n            cmap='viridis',\n            cbar_kws={'label': 'Attention Weight'})\nplt.title('Encoder Self-Attention (Filtered & Renormalized)\\nSpecial Tokens Removed')\nplt.xlabel('Key Position')\nplt.ylabel('Query Position')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFiltered attention matrix shape: {encoder_attn_normalized.shape}\")\nprint(f\"Min attention: {encoder_attn_normalized.min():.4f}\")\nprint(f\"Max attention: {encoder_attn_normalized.max():.4f}\")\nprint(f\"Mean attention: {encoder_attn_normalized.mean():.4f}\")\nprint(f\"Row sum check (should be 1.0): {encoder_attn_normalized.sum(axis=1)[:3]}\")  # Check first 3 rows",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Visualize individual attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 4 attention heads from last encoder layer\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(min(4, encoder_attn_last.shape[0])):\n",
    "    attn_head = encoder_attn_last[head_idx]\n",
    "    \n",
    "    sns.heatmap(attn_head, \n",
    "                xticklabels=input_tokens, \n",
    "                yticklabels=input_tokens,\n",
    "                cmap='viridis',\n",
    "                ax=axes[head_idx],\n",
    "                cbar_kws={'label': 'Weight'})\n",
    "    axes[head_idx].set_title(f'Attention Head {head_idx}')\n",
    "    axes[head_idx].set_xlabel('Key')\n",
    "    axes[head_idx].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Encoder Self-Attention Heads (Last Layer)', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "## 6. Extract attention for multiple examples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_maps(text, tokenizer, model, device, src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\"):\n",
    "    \"\"\"\n",
    "    Extract encoder self-attention for a given text.\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - 'tokens': list of tokens\n",
    "            - 'encoder_attention': numpy array (layers, heads, seq_len, seq_len)\n",
    "            - 'encoder_attention_avg': numpy array (seq_len, seq_len) - averaged over layers and heads\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokenizer.src_lang = src_lang\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "    \n",
    "    # Generate with attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "            max_length=100,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    # Extract encoder attention\n",
    "    encoder_attn_all = torch.stack([layer[0] for layer in outputs.encoder_attentions]).cpu().numpy()\n",
    "    # Shape: (layers, heads, seq_len, seq_len)\n",
    "    \n",
    "    # Average over layers and heads\n",
    "    encoder_attn_avg = encoder_attn_all.mean(axis=(0, 1))  # (seq_len, seq_len)\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'encoder_attention': encoder_attn_all,\n",
    "        'encoder_attention_avg': encoder_attn_avg\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_result = extract_attention_maps(english, tokenizer, model, device)\n",
    "print(f\"Extracted attention for: {english}\")\n",
    "print(f\"Tokens: {test_result['tokens']}\")\n",
    "print(f\"Encoder attention shape: {test_result['encoder_attention'].shape}\")\n",
    "print(f\"Averaged attention shape: {test_result['encoder_attention_avg'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention for first 5 examples\n",
    "num_examples = 5\n",
    "attention_data = []\n",
    "\n",
    "print(f\"Extracting attention for {num_examples} examples...\\n\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    example = dataset[i][\"translation\"]\n",
    "    english = example[\"en\"]\n",
    "    french = example[\"fr\"]\n",
    "    \n",
    "    result = extract_attention_maps(english, tokenizer, model, device)\n",
    "    \n",
    "    attention_data.append({\n",
    "        'index': i,\n",
    "        'english': english,\n",
    "        'french': french,\n",
    "        'tokens': result['tokens'],\n",
    "        'attention_avg': result['encoder_attention_avg']\n",
    "    })\n",
    "    \n",
    "    print(f\"[{i+1}/{num_examples}] Extracted attention for: {english[:50]}...\")\n",
    "\n",
    "print(f\"\\n✓ Extracted attention for {len(attention_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for multiple examples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i in range(min(3, len(attention_data))):\n",
    "    data = attention_data[i]\n",
    "    \n",
    "    sns.heatmap(data['attention_avg'],\n",
    "                xticklabels=data['tokens'],\n",
    "                yticklabels=data['tokens'],\n",
    "                cmap='viridis',\n",
    "                ax=axes[i],\n",
    "                cbar_kws={'label': 'Weight'})\n",
    "    axes[i].set_title(f\"Example {i+1}\\n{data['english'][:40]}...\", fontsize=10)\n",
    "    axes[i].set_xlabel('Key')\n",
    "    axes[i].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Encoder Self-Attention (Averaged)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": "## Summary\n\n**Successfully extracted:**\n- Encoder self-attention maps (English sentence structure)\n- Attention weights across all layers and heads\n- Averaged attention for graph construction\n\n**Next steps:**\n1. Build attention graphs (tokens as nodes, attention weights as edges)\n2. Compute persistent homology on the graphs\n3. Compare English vs French topological structures"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}