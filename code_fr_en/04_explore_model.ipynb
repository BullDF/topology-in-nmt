{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NLLB-600M Model Exploration\nLoad the saved model and test English → French translation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import all required libraries\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom datasets import load_from_disk"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load tokenizer and model from local directory\nmodel_dir = \"../models/nllb-600M\"\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nprint(\"✓ Tokenizer loaded\")\n\nprint(\"\\nLoading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\nprint(\"✓ Model loaded\")\n\n# Determine device: CUDA (Colab/NVIDIA) > MPS (Apple Silicon) > CPU\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n\nmodel = model.to(device)\nprint(f\"\\n✓ Model moved to device: {device}\")\n\nprint(f\"\\nModel: NLLB-200-distilled-600M\")\nprint(f\"Parameters: ~600M\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Test English → French translation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# NLLB uses language codes: eng_Latn (English), fra_Latn (French)\ntest_sentence = \"Hello, how are you?\"\nprint(f\"Input (English): {test_sentence}\")\n\n# Set source language\ntokenizer.src_lang = \"eng_Latn\"\n\n# Tokenize and move to device\ninputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n\n# Generate translation\ntranslated_tokens = model.generate(\n    **inputs,\n    forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n    max_length=50\n)\n\n# Decode\ntranslation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\nprint(f\"Output (French): {translation}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test with multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_sentences = [\n    \"Hello, how are you?\",\n    \"I am a student at the university.\",\n    \"The cat is on the table.\",\n    \"What time is it?\",\n    \"I love learning languages.\"\n]\n\nprint(\"=\"*80)\nprint(\"English → French Translations\")\nprint(\"=\"*80)\n\ntokenizer.src_lang = \"eng_Latn\"\n\nfor i, sentence in enumerate(test_sentences, 1):\n    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n        max_length=50\n    )\n    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n    \n    print(f\"\\n{i}. EN: {sentence}\")\n    print(f\"   FR: {translation}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Number of encoder layers: {model.config.encoder_layers}\")\n",
    "print(f\"  Number of decoder layers: {model.config.decoder_layers}\")\n",
    "print(f\"  Number of attention heads: {model.config.encoder_attention_heads}\")\n",
    "print(f\"  Hidden size: {model.config.d_model}\")\n",
    "print(f\"  Vocabulary size: {model.config.vocab_size}\")\n",
    "print(f\"\\nModel has encoder-decoder architecture for sequence-to-sequence translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nParameters (in millions): {total_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test with dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load some examples from our saved dataset\ndataset = load_from_disk(\"../data/wmt14_fr_en_validation_2000\")\nprint(f\"Loaded {len(dataset)} sentence pairs\\n\")\n\n# Test on first 3 examples\nprint(\"=\"*80)\nprint(\"Testing on WMT14 dataset examples (English → French)\")\nprint(\"=\"*80)\n\ntokenizer.src_lang = \"eng_Latn\"\n\nfor i in range(3):\n    example = dataset[i][\"translation\"]\n    english = example[\"en\"]\n    french_ref = example[\"fr\"]\n    \n    # Translate\n    inputs = tokenizer(english, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n        max_length=100\n    )\n    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n    \n    print(f\"\\nExample {i+1}:\")\n    print(f\"EN: {english}\")\n    print(f\"FR (reference): {french_ref}\")\n    print(f\"FR (translated): {translation}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n**Model loaded successfully:**\n- NLLB-200-distilled-600M (~600M parameters)\n- English → French translation working\n- GPU acceleration enabled (CUDA/MPS/CPU)\n- Ready for attention extraction\n\n**Next steps:**\n1. Extract attention weights from encoder and decoder\n2. Build attention graphs\n3. Compute persistent homology"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}