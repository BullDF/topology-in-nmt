{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NLLB-1.3B Model Exploration\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Mount Google Drive (run cell below)\n",
    "2. Set `ROOT_DIR` to your project folder path\n",
    "3. Run the rest of the notebook\n",
    "\n",
    "**For local execution:** Skip the Google Drive cell and run from \"Import Libraries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (only needed for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # IMPORTANT: Set this to your code_fr_en directory path\n",
    "    # This should point to where THIS notebook is located\n",
    "    ROOT_DIR = \"/content/drive/MyDrive/UofT/CSC2517/term_paper/code_fr_en\"\n",
    "    \n",
    "    import os\n",
    "    os.chdir(ROOT_DIR)\n",
    "    print(f\"✓ Changed to: {os.getcwd()}\")\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab, using local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify working directory and model files\n",
    "import os\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"\\nContents of current directory:\")\n",
    "print(os.listdir('.'))\n",
    "\n",
    "model_path = \"../models/nllb-1.3B\"\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"\\n✓ Model directory exists: {model_path}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Model directory NOT found: {model_path}\")\n",
    "    print(\"\\nMake sure you:\")\n",
    "    print(\"1. Ran 03_load_model.py to download the model\")\n",
    "    print(\"2. Set ROOT_DIR to point to code_fr_en/ directory\")\n",
    "    print(f\"\\nExpected structure:\")\n",
    "    print(f\"  ROOT_DIR/\")\n",
    "    print(f\"  ├── models/\")\n",
    "    print(f\"  │   └── nllb-1.3B/\")\n",
    "    print(f\"  └── <you are here>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 1. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model from local directory\n",
    "model_dir = \"../models/nllb-1.3B\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "print(\"✓ Tokenizer loaded\")\n",
    "\n",
    "print(\"\\nLoading model...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "# Determine device: CUDA (Colab/NVIDIA) > MPS (Apple Silicon) > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"\\n✓ Model moved to device: {device}\")\n",
    "\n",
    "print(f\"\\nModel: NLLB-200-distilled-1.3B\")\n",
    "print(f\"Parameters: ~1.3B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Test English → French translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLLB uses language codes: eng_Latn (English), fra_Latn (French)\n",
    "test_sentence = \"Hello, how are you?\"\n",
    "print(f\"Input (English): {test_sentence}\")\n",
    "\n",
    "# Set source language\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate translation\n",
    "translated_tokens = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "# Decode\n",
    "translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "print(f\"Output (French): {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Test with multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am a student at the university.\",\n",
    "    \"The cat is on the table.\",\n",
    "    \"What time is it?\",\n",
    "    \"I love learning languages.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"English → French Translations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n",
    "        max_length=50\n",
    "    )\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"\\n{i}. EN: {sentence}\")\n",
    "    print(f\"   FR: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Number of encoder layers: {model.config.encoder_layers}\")\n",
    "print(f\"  Number of decoder layers: {model.config.decoder_layers}\")\n",
    "print(f\"  Number of attention heads: {model.config.encoder_attention_heads}\")\n",
    "print(f\"  Hidden size: {model.config.d_model}\")\n",
    "print(f\"  Vocabulary size: {model.config.vocab_size}\")\n",
    "print(f\"\\nModel has encoder-decoder architecture for sequence-to-sequence translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nParameters (in millions): {total_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Test with dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some examples from our saved dataset\n",
    "dataset = load_from_disk(\"../data/wmt14_fr_en_validation_2000\")\n",
    "print(f\"Loaded {len(dataset)} sentence pairs\\n\")\n",
    "\n",
    "# Test on first 3 examples\n",
    "print(\"=\"*80)\n",
    "print(\"Testing on WMT14 dataset examples (English → French)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "\n",
    "for i in range(3):\n",
    "    example = dataset[i][\"translation\"]\n",
    "    english = example[\"en\"]\n",
    "    french_ref = example[\"fr\"]\n",
    "    \n",
    "    # Translate\n",
    "    inputs = tokenizer(english, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n",
    "        max_length=100\n",
    "    )\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"EN: {english}\")\n",
    "    print(f\"FR (reference): {french_ref}\")\n",
    "    print(f\"FR (translated): {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model loaded successfully:**\n",
    "- NLLB-200-distilled-1.3B (~1.3B parameters)\n",
    "- English → French translation working\n",
    "- GPU acceleration enabled (CUDA/MPS/CPU)\n",
    "- Ready for attention extraction\n",
    "\n",
    "**Next steps:**\n",
    "1. Extract attention weights from encoder and decoder\n",
    "2. Build attention graphs\n",
    "3. Compute persistent homology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
