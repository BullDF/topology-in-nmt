# Topology in Neural Machine Translation

**A Topological Study of Transformers through Attention**

üìÑ **[Read the Final Paper (PDF)](final_paper/final_paper.pdf)**

Author: Yuwei (Johnny) Meng
Institution: University of Toronto

---

## Overview

This research applies **Topological Data Analysis (TDA)** to Neural Machine Translation (NMT) systems, specifically examining how transformer models process different languages through the lens of persistent homology. By analyzing attention maps from the NLLB-200 model, we investigate whether English and other languages (French and Chinese) create similar topological structures during translation, and whether these topological similarities correlate with translation quality.

### Research Question

> Do French and English sentences create similar or different topological structures in the attention maps generated by transformer models during translation? If so, do topological differences in attention maps correlate with translation quality? What about Chinese and English sentences?

### Key Findings

- **French-English**: Statistically significant negative correlation between topological dissimilarity (Wasserstein distance) and translation quality (BLEU scores): **r = -0.132, p = 2.84 √ó 10‚Åª‚Åπ** after controlling for sentence length
- **Topological preservation matters**: Preserving topological structures in attention maps independently contributes to translation quality for typologically similar languages
- **Cross-language isomorphism**: Strong correlation (>0.65) in topological features across languages, supporting Meirom & Bobrowski (2022)'s claim of cross-lingual isomorphism
- **H‚ÇÄ dominates H‚ÇÅ**: Connected components (Œ≤‚ÇÄ) account for most topological differences; loops (Œ≤‚ÇÅ) are rare and ephemeral in attention maps
- **Chinese-English limitation**: Correlation becomes negligible for Chinese-English (r = -0.020, p = 0.364) due to systematic NLLB model truncation issues
