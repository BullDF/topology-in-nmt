# Topology in Neural Machine Translation

**A Topological Study of Transformers through Attention**

ğŸ“„ **[Read the Final Paper (PDF)](final_paper/final_paper.pdf)**

Author: Yuwei (Johnny) Meng
Course: CSC2517 - Discrete Mathematical Models of Sentence Structure
Institution: University of Toronto
Instructor: Professor Gerald Penn
Date: December 23, 2025

---

## Overview

This research applies **Topological Data Analysis (TDA)** to Neural Machine Translation (NMT) systems, specifically examining how transformer models process different languages through the lens of persistent homology. By analyzing attention maps from the NLLB-200 model, we investigate whether English and other languages (French and Chinese) create similar topological structures during translation, and whether these topological similarities correlate with translation quality.

### Research Question

> Do French and English sentences create similar topological structures in the attention maps generated by transformer models during translation? Do topological differences correlate with translation quality? What about Chinese and English?

### Key Findings

- **French-English**: Statistically significant negative correlation between topological dissimilarity (Wasserstein distance) and translation quality (BLEU scores): **r = -0.132, p = 2.84 Ã— 10â»â¹** after controlling for sentence length
- **Topological preservation matters**: Preserving topological structures in attention maps independently contributes to translation quality for typologically similar languages
- **Cross-language isomorphism**: Strong correlation (>0.85) in topological features across languages, supporting Meirom & Bobrowski (2022)'s claim of cross-lingual isomorphism
- **Hâ‚€ dominates Hâ‚**: Connected components (Î²â‚€) account for most topological differences; loops (Î²â‚) are rare and ephemeral in attention maps
- **Chinese-English limitation**: Correlation becomes negligible for Chinese-English (r = -0.020, p = 0.364) due to systematic NLLB model truncation issues

---

## Repository Structure

```
.
â”œâ”€â”€ final_paper/                    # Final paper (Typst format)
â”‚   â”œâ”€â”€ final_paper.typ            # Typst source
â”‚   â”œâ”€â”€ final_paper.pdf            # Compiled PDF (17 pages)
â”‚   â”œâ”€â”€ bib.bib                    # Bibliography
â”‚   â””â”€â”€ images/                    # All figures (13 publication-quality visualizations)
â”‚
â”œâ”€â”€ code_fr_en/                     # French-English analysis pipeline
â”‚   â”œâ”€â”€ 01_load_data.py            # Load WMT14 data (2000 pairs)
â”‚   â”œâ”€â”€ 02_explore_data.ipynb      # Data exploration
â”‚   â”œâ”€â”€ 03_load_model.py           # Download NLLB-1.3B model
â”‚   â”œâ”€â”€ 04_explore_model.ipynb     # Model testing
â”‚   â”œâ”€â”€ 05_extract_attention.ipynb # Attention visualization
â”‚   â”œâ”€â”€ 06_bidirectional_encoder_extraction.ipynb
â”‚   â”œâ”€â”€ 07_extract_all_attention.ipynb  # Extract all 2000 pairs
â”‚   â”œâ”€â”€ 08_verify_attention.ipynb  # Verification
â”‚   â”œâ”€â”€ 09_explore_tda.ipynb       # TDA methodology
â”‚   â”œâ”€â”€ 10_compute_tda_all.py      # Compute persistent homology
â”‚   â”œâ”€â”€ 11_analyze_tda_results.ipynb
â”‚   â”œâ”€â”€ 12_explore_bleu.ipynb      # BLEU scores
â”‚   â””â”€â”€ 13_explore_tda_bleu_correlation.ipynb
â”‚
â”œâ”€â”€ code_zh_en/                     # Chinese-English analysis (same structure)
â”‚
â”œâ”€â”€ data/                           # Dataset storage (gitignored)
â”‚   â”œâ”€â”€ wmt14_fr_en_validation_2000/
â”‚   â”œâ”€â”€ wmt17_zh_en_validation_2000/
â”‚   â”œâ”€â”€ attention_maps_fr_en/
â”‚   â”œâ”€â”€ attention_maps_zh_en/
â”‚   â”œâ”€â”€ tda_results_fr_en/
â”‚   â”œâ”€â”€ tda_results_zh_en/
â”‚   â”œâ”€â”€ bleu_scores_fr_en.csv
â”‚   â””â”€â”€ bleu_scores_zh_en.csv
â”‚
â”œâ”€â”€ models/                         # Model storage (gitignored)
â”‚   â””â”€â”€ nllb-1.3B/                 # NLLB-200-distilled-1.3B (~5.5GB)
â”‚
â”œâ”€â”€ proposal/                       # LaTeX project proposal
â”‚   â”œâ”€â”€ proposal.tex
â”‚   â””â”€â”€ proposal.pdf
â”‚
â”œâ”€â”€ filtration_demo.ipynb          # Vietoris-Rips filtration visualization
â””â”€â”€ PROJECT_SUMMARY.md             # High-level project status
```

---

## Methodology

### 1. Model & Data
- **Model**: NLLB-200-distilled-1.3B (Meta, 1.3B parameters, 24 encoder layers, 16 heads/layer)
- **Datasets**:
  - WMT14 French-English validation (2000 sentence pairs)
  - WMT17 Chinese-English validation (2000 sentence pairs)

### 2. Attention Extraction
- **Bidirectional extraction**: Translate in both directions (ENâ†’FR/ZH and FR/ZHâ†’EN)
- Extract encoder self-attention from **last layer only** (layer 23/24)
- Mean-aggregate across 16 attention heads
- Filter special tokens (`</s>`, language tags) and renormalize

### 3. Topological Analysis
- Build weighted graphs: tokens as nodes, attention weights as edges
- Convert to distance: d = 1 - attention
- Compute persistent homology using **Vietoris-Rips filtration**
- Generate persistence diagrams (Î²â‚€ and Î²â‚ features)
- Measure cross-language similarity using **Wasserstein distance**

### 4. Correlation Analysis
- Compute BLEU scores for translation quality
- Correlate Wasserstein distances with BLEU scores
- Apply **partial correlation** controlling for sentence length

---

## Key Results Summary

| Metric | French-English | Chinese-English |
|--------|----------------|-----------------|
| Mean Wasserstein Distance | 5.0 | 4.4 |
| Hâ‚€ Cross-Language Correlation | 0.95 | 0.85 |
| Hâ‚ Cross-Language Correlation | 0.86 | 0.68 |
| Mean BLEU Score | 30.16 | 19.96 |
| Partial Correlation (r) | **-0.132*** | -0.020 (n.s.) |
| p-value | 2.84 Ã— 10â»â¹ | 0.364 |

***: Statistically significant at p < 0.001
n.s.: Not statistically significant

---

## Known Issues

### âš ï¸ NLLB Chinese Translation Truncation

The NLLB-200-distilled-1.3B model exhibits a systematic bug when generating Chinese text (ENâ†’ZH direction):

- **Symptom**: Translations prematurely terminate after commas (typically 15-22 tokens)
- **Impact**: Chinese BLEU scores artificially low, correlation analysis confounded
- **Direction**: Only affects ENâ†’ZH (not ZHâ†’EN or FR-EN)
- **Persistence**: Issue persists across greedy decoding, beam search, and parameter variations
- **Documentation**: See Section 5.4 of the final paper for detailed error analysis

**Recommendation**: Future work should use NLLB-3.3B, mBART, or M2M-100 models.

---

## Setup & Requirements

### Environment
- Python 3.8+
- GPU recommended (CUDA or Apple Silicon MPS)
- ~10GB disk space (model + data)
- ~4-5GB VRAM for inference

### Dependencies

**Core Libraries:**
```
transformers
torch
datasets
sacrebleu
```

**TDA Stack:**
```
ripser        # Persistent homology computation
persim        # Persistence diagram comparison
```

**Analysis & Visualization:**
```
numpy
pandas
matplotlib
seaborn
scipy
```

### Installation

1. Clone this repository
2. Install dependencies:
   ```bash
   pip install transformers torch datasets sacrebleu ripser persim numpy pandas matplotlib seaborn scipy
   ```
3. Run notebooks in order (01 â†’ 13) for each language pair

### Google Colab Support
All analysis notebooks (02, 04-13) support both local and Google Colab execution. For Colab:
- Mount Google Drive in first code cell
- SimHei font automatically downloaded for Chinese rendering

---

## Reproducing Results

### Quick Start (French-English)

```bash
# 1. Load data
python code_fr_en/01_load_data.py

# 2. Download model
python code_fr_en/03_load_model.py

# 3. Extract attention maps (run notebook 07)
# 4. Compute TDA metrics
python code_fr_en/10_compute_tda_all.py

# 5. Analyze results in notebooks 11-13
```

Repeat for `code_zh_en/` for Chinese-English analysis.

---

## Citation

If you use this work, please cite:

```bibtex
@misc{meng2025topology,
  title={Topology in Neural Machine Translation: A Topological Study of Transformers through Attention},
  author={Meng, Yuwei},
  year={2025},
  institution={University of Toronto},
  note={CSC2517 Term Paper}
}
```

---

## Key References

1. **Vaswani et al. (2017)** - "Attention is All you Need" (Transformer architecture)
2. **Meirom & Bobrowski (2022)** - Cross-lingual embedding isomorphism (motivation)
3. **Kushnareva et al. (2021)** - TDA for attention analysis (methodology)
4. **Team et al. (2022)** - NLLB model paper

See `final_paper/bib.bib` for complete bibliography.

---

## Acknowledgements

Special thanks to:
- **Professor Gerald Penn** (CSC2517 Instructor) for conceptual guidance
- **TA Jinman Zhao** for feedback on algebraic topology methodology
- **Meta AI** for open-sourcing the NLLB model

---

## License

This is an academic research project completed for CSC2517 at the University of Toronto.
For questions or collaboration inquiries, please contact the author.

---

ğŸ“„ **[Read the Full Paper (PDF)](final_paper/final_paper.pdf)** | 17 pages | 13 figures | 11 references
