# Topology in NMT - Project Summary
> Generated by Claude - Dec 4, 2024

## Overview
**Course**: CSC2517 (Gerald Penn) | **Status**: âœ… Proposal APPROVED

**Research Question**: Do EN/FR sentences create similar topological structures in attention maps during translation? Does similarity correlate with translation quality?

**Motivation**: Meirom & Bobrowski (2022) showed embeddings are isomorphic across languages. Does NMT preserve this?

## Methodology
**Model**: NLLB-600M (pre-trained only, NO training!)  
**Dataset**: WMT14 fr-en (val: 3K, test: 3K)

**Pipeline**:
1. Load WMT14 fr-en val/test
2. Extract encoder/decoder self-attention maps
3. Build graphs: tokens=nodes, attention=edges (Kushnareva et al. 2021)
4. Compute persistent homology â†’ Î²â‚€, Î²â‚ â†’ persistence diagrams
5. Measure similarity: Wasserstein distance between EN/FR diagrams
6. Correlate with BLEU scores

## Current Status
âœ… Proposal approved, `01_load_data.py` ready  
ğŸ“‹ Next: Run script â†’ load NLLB â†’ extract attention â†’ verify structure

## Key Notes
- Start with 5-10 sentences on CPU
- Use Wasserstein distance (just learned in class!)
- Scale to GPU/Colab only if needed
- TDA libraries: gudhi, giotto-tda, or ripser

**Key Papers**: Meirom & Bobrowski 2022 (motivation), Kushnareva et al. 2021 (method)
