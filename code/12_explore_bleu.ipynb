{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore BLEU Score Computation\n",
    "\n",
    "Compute BLEU scores for translation quality assessment.\n",
    "\n",
    "We have:\n",
    "- `en_translation`: Generated French (EN → FR)\n",
    "- `fr_translation`: Generated English (FR → EN)\n",
    "- `fr_text`: Reference French\n",
    "- `en_text`: Reference English\n",
    "\n",
    "We'll compute:\n",
    "- **EN→FR BLEU**: Compare `en_translation` with `fr_text`\n",
    "- **FR→EN BLEU**: Compare `fr_translation` with `en_text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sacrebleu if needed (standard BLEU implementation)\n",
    "# !pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TDA Results\n",
    "\n",
    "Load results that contain both translations and original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TDA results (contains translations)\n",
    "data_path = Path(\"../data/tda_results/tda_results_last_layer_filtered.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")\n",
    "print()\n",
    "\n",
    "# Examine first result\n",
    "print(\"Data structure:\")\n",
    "print(f\"Keys: {list(results[0].keys())}\")\n",
    "print()\n",
    "print(\"Sample:\")\n",
    "print(f\"EN text:         {results[0]['en_text']}\")\n",
    "print(f\"FR text:         {results[0]['fr_text']}\")\n",
    "print(f\"EN→FR (generated): {results[0]['en_translation']}\")\n",
    "print(f\"FR→EN (generated): {results[0]['fr_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute BLEU Scores for Sample Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_scores(en_text, fr_text, en_translation, fr_translation):\n",
    "    \"\"\"\n",
    "    Compute BLEU scores for both translation directions.\n",
    "    \n",
    "    Args:\n",
    "        en_text: Original English text (reference for FR→EN)\n",
    "        fr_text: Original French text (reference for EN→FR)\n",
    "        en_translation: Generated French from English (hypothesis for EN→FR)\n",
    "        fr_translation: Generated English from French (hypothesis for FR→EN)\n",
    "    \n",
    "    Returns:\n",
    "        dict with BLEU scores\n",
    "    \"\"\"\n",
    "    # EN→FR: Compare generated French with reference French\n",
    "    bleu_en_fr = sentence_bleu(en_translation, [fr_text]).score\n",
    "    \n",
    "    # FR→EN: Compare generated English with reference English\n",
    "    bleu_fr_en = sentence_bleu(fr_translation, [en_text]).score\n",
    "    \n",
    "    # Average BLEU\n",
    "    bleu_avg = (bleu_en_fr + bleu_fr_en) / 2\n",
    "    \n",
    "    return {\n",
    "        'bleu_en_fr': bleu_en_fr,\n",
    "        'bleu_fr_en': bleu_fr_en,\n",
    "        'bleu_avg': bleu_avg\n",
    "    }\n",
    "\n",
    "print(\"✓ Function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first 10 examples\n",
    "print(\"Testing BLEU computation on first 10 examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(10):\n",
    "    example = results[i]\n",
    "    scores = compute_bleu_scores(\n",
    "        en_text=example['en_text'],\n",
    "        fr_text=example['fr_text'],\n",
    "        en_translation=example['en_translation'],\n",
    "        fr_translation=example['fr_translation']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[{i}] EN→FR: {scores['bleu_en_fr']:.2f}, FR→EN: {scores['bleu_fr_en']:.2f}, Avg: {scores['bleu_avg']:.2f}\")\n",
    "    print(f\"    EN: {example['en_text'][:70]}...\")\n",
    "    print(f\"    FR: {example['fr_text'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute BLEU for All Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU scores for all examples\n",
    "print(f\"Computing BLEU scores for {len(results)} sentence pairs...\")\n",
    "\n",
    "bleu_results = []\n",
    "for i, example in enumerate(results):\n",
    "    scores = compute_bleu_scores(\n",
    "        en_text=example['en_text'],\n",
    "        fr_text=example['fr_text'],\n",
    "        en_translation=example['en_translation'],\n",
    "        fr_translation=example['fr_translation']\n",
    "    )\n",
    "    \n",
    "    bleu_results.append({\n",
    "        'idx': i,\n",
    "        **scores\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(results)}\")\n",
    "\n",
    "print(f\"✓ Computed BLEU scores for all {len(bleu_results)} pairs\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_bleu = pd.DataFrame(bleu_results)\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df_bleu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BLEU SCORE STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"EN→FR BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_en_fr'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_en_fr'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_en_fr'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_en_fr'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_en_fr'].std():.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"FR→EN BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_fr_en'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_fr_en'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_fr_en'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_fr_en'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_fr_en'].std():.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"Average BLEU:\")\n",
    "print(f\"  Min:    {df_bleu['bleu_avg'].min():.2f}\")\n",
    "print(f\"  Max:    {df_bleu['bleu_avg'].max():.2f}\")\n",
    "print(f\"  Mean:   {df_bleu['bleu_avg'].mean():.2f}\")\n",
    "print(f\"  Median: {df_bleu['bleu_avg'].median():.2f}\")\n",
    "print(f\"  Std:    {df_bleu['bleu_avg'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize BLEU Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# EN→FR BLEU\n",
    "axes[0].hist(df_bleu['bleu_en_fr'], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(df_bleu['bleu_en_fr'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_en_fr\"].mean():.2f}')\n",
    "axes[0].set_xlabel('BLEU Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('EN→FR BLEU Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# FR→EN BLEU\n",
    "axes[1].hist(df_bleu['bleu_fr_en'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(df_bleu['bleu_fr_en'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_fr_en\"].mean():.2f}')\n",
    "axes[1].set_xlabel('BLEU Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('FR→EN BLEU Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Average BLEU\n",
    "axes[2].hist(df_bleu['bleu_avg'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[2].axvline(df_bleu['bleu_avg'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_bleu[\"bleu_avg\"].mean():.2f}')\n",
    "axes[2].set_xlabel('BLEU Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Average BLEU Distribution')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best and Worst Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sort by average BLEU\ndf_bleu_sorted = df_bleu.sort_values('bleu_avg', ascending=False)\n\nprint(\"=\" * 70)\nprint(\"BEST TRANSLATIONS (Highest BLEU)\")\nprint(\"=\" * 70)\nfor i in range(5):\n    idx = int(df_bleu_sorted.iloc[i]['idx'])\n    example = results[idx]\n    scores = df_bleu_sorted.iloc[i]\n    \n    print(f\"\\n[{i+1}] Pair {idx}: Avg BLEU = {scores['bleu_avg']:.2f} (EN→FR: {scores['bleu_en_fr']:.2f}, FR→EN: {scores['bleu_fr_en']:.2f})\")\n    print(f\"    EN: {example['en_text']}\")\n    print(f\"    FR: {example['fr_text']}\")\n    print(f\"    Generated FR: {example['en_translation']}\")\n    print(f\"    Generated EN: {example['fr_translation']}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"WORST TRANSLATIONS (Lowest BLEU)\")\nprint(\"=\" * 70)\nfor i in range(5):\n    idx = int(df_bleu_sorted.iloc[-(i+1)]['idx'])\n    example = results[idx]\n    scores = df_bleu_sorted.iloc[-(i+1)]\n    \n    print(f\"\\n[{i+1}] Pair {idx}: Avg BLEU = {scores['bleu_avg']:.2f} (EN→FR: {scores['bleu_en_fr']:.2f}, FR→EN: {scores['bleu_fr_en']:.2f})\")\n    print(f\"    EN: {example['en_text']}\")\n    print(f\"    FR: {example['fr_text']}\")\n    print(f\"    Generated FR: {example['en_translation']}\")\n    print(f\"    Generated EN: {example['fr_translation']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Between EN→FR and FR→EN BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(df_bleu['bleu_en_fr'], df_bleu['bleu_fr_en'], alpha=0.3, s=10)\n",
    "plt.xlabel('EN→FR BLEU')\n",
    "plt.ylabel('FR→EN BLEU')\n",
    "plt.title('Correlation Between Translation Directions')\n",
    "plt.plot([0, 100], [0, 100], 'r--', alpha=0.5, label='y=x')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation\n",
    "corr = df_bleu['bleu_en_fr'].corr(df_bleu['bleu_fr_en'])\n",
    "print(f\"Correlation between EN→FR and FR→EN BLEU: r = {corr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}