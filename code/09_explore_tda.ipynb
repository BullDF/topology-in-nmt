{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Data Analysis - Exploration\n",
    "\n",
    "Explore building attention graphs and computing persistent homology on encoder attention patterns.\n",
    "\n",
    "**Methodology:**\n",
    "1. Extract last layer attention\n",
    "2. Average across attention heads\n",
    "3. Filter special tokens and renormalize\n",
    "4. Symmetrize attention matrix\n",
    "5. Convert to distance: `d_ij = 1 - attention_ij`\n",
    "6. Compute Vietoris-Rips persistent homology\n",
    "7. Extract persistence diagrams (β₀, β₁)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install giotto-tda if not already installed\n",
    "# !pip install giotto-tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# TDA libraries\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Attention Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted attention maps\n",
    "data_path = Path(\"../data/attention_maps/all_encoder_attention.pkl\")\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(results)} sentence pairs\")\n",
    "print(f\"\\nModel architecture: {results[0]['en_attention'].shape[0]} layers, {results[0]['en_attention'].shape[1]} heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Graph Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distance_matrix(attention, tokens, layer=-1):\n",
    "    \"\"\"\n",
    "    Build distance matrix from attention weights.\n",
    "    \n",
    "    Args:\n",
    "        attention: Attention tensor (num_layers, num_heads, seq_len, seq_len)\n",
    "        tokens: List of token strings\n",
    "        layer: Which layer to use (-1 for last layer)\n",
    "    \n",
    "    Returns:\n",
    "        distance_matrix: (N, N) array where N = number of content tokens\n",
    "        content_tokens: List of content token strings\n",
    "    \"\"\"\n",
    "    # 1. Extract last layer and average over heads\n",
    "    attn = attention[layer].mean(axis=0)  # (seq_len, seq_len)\n",
    "    \n",
    "    # 2. Filter special tokens\n",
    "    special_tokens = {'</s>', '<s>', '<pad>', 'eng_Latn', 'fra_Latn'}\n",
    "    content_mask = np.array([tok not in special_tokens for tok in tokens])\n",
    "    \n",
    "    # Filter attention matrix\n",
    "    attn_filtered = attn[content_mask][:, content_mask]\n",
    "    content_tokens = [tok for tok, keep in zip(tokens, content_mask) if keep]\n",
    "    \n",
    "    # 3. Renormalize\n",
    "    row_sums = attn_filtered.sum(axis=1, keepdims=True)\n",
    "    attn_filtered = attn_filtered / row_sums\n",
    "    \n",
    "    # 4. Symmetrize (make undirected)\n",
    "    attn_sym = (attn_filtered + attn_filtered.T) / 2\n",
    "    \n",
    "    # 5. Convert to distance: d = 1 - attention\n",
    "    distance_matrix = 1 - attn_sym\n",
    "    \n",
    "    # Ensure diagonal is 0 and symmetric\n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    \n",
    "    return distance_matrix, content_tokens\n",
    "\n",
    "\n",
    "print(\"✓ Function defined: build_distance_matrix()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test on Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first example\n",
    "idx = 0\n",
    "example = results[idx]\n",
    "\n",
    "print(f\"Example {idx}:\")\n",
    "print(f\"English: {example['en_text']}\")\n",
    "print(f\"French:  {example['fr_text']}\")\n",
    "print()\n",
    "\n",
    "# Build distance matrices\n",
    "en_dist, en_tokens = build_distance_matrix(example['en_attention'], example['en_tokens'])\n",
    "fr_dist, fr_tokens = build_distance_matrix(example['fr_attention'], example['fr_tokens'])\n",
    "\n",
    "print(f\"English distance matrix: {en_dist.shape}\")\n",
    "print(f\"  Content tokens: {en_tokens}\")\n",
    "print(f\"  Min distance: {en_dist.min():.4f}\")\n",
    "print(f\"  Max distance: {en_dist.max():.4f}\")\n",
    "print(f\"  Mean distance: {en_dist.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"French distance matrix: {fr_dist.shape}\")\n",
    "print(f\"  Content tokens: {fr_tokens}\")\n",
    "print(f\"  Min distance: {fr_dist.min():.4f}\")\n",
    "print(f\"  Max distance: {fr_dist.max():.4f}\")\n",
    "print(f\"  Mean distance: {fr_dist.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distance Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distance matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# English\n",
    "sns.heatmap(en_dist, xticklabels=en_tokens, yticklabels=en_tokens, \n",
    "            cmap='RdYlBu_r', ax=ax1, square=True,\n",
    "            cbar_kws={'label': 'Distance (1 - attention)'})\n",
    "ax1.set_title(f'English Distance Matrix\\n{example[\"en_text\"][:50]}...')\n",
    "ax1.set_xlabel('Tokens')\n",
    "ax1.set_ylabel('Tokens')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# French\n",
    "sns.heatmap(fr_dist, xticklabels=fr_tokens, yticklabels=fr_tokens, \n",
    "            cmap='RdYlBu_r', ax=ax2, square=True,\n",
    "            cbar_kws={'label': 'Distance (1 - attention)'})\n",
    "ax2.set_title(f'French Distance Matrix\\n{example[\"fr_text\"][:50]}...')\n",
    "ax2.set_xlabel('Tokens')\n",
    "ax2.set_ylabel('Tokens')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Persistent Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vietoris-Rips persistence\n",
    "# homology_dimensions=[0, 1] computes β₀ (connected components) and β₁ (loops)\n",
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1], n_jobs=-1)\n",
    "\n",
    "print(\"✓ Vietoris-Rips persistence initialized\")\n",
    "print(f\"  Computing: β₀ (connected components) and β₁ (1-dimensional holes/loops)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute persistence for English\n",
    "# Input needs to be 3D: (n_samples, n_points, n_points)\n",
    "en_diagram = VR.fit_transform([en_dist])[0]  # Extract first (and only) diagram\n",
    "\n",
    "print(\"English persistence diagram:\")\n",
    "print(f\"  Shape: {en_diagram.shape}\")\n",
    "print(f\"  Format: (birth, death, dimension)\")\n",
    "print(f\"\\nFirst 10 features:\")\n",
    "print(en_diagram[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute persistence for French\n",
    "fr_diagram = VR.fit_transform([fr_dist])[0]\n",
    "\n",
    "print(\"French persistence diagram:\")\n",
    "print(f\"  Shape: {fr_diagram.shape}\")\n",
    "print(f\"\\nFirst 10 features:\")\n",
    "print(fr_diagram[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Persistence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count features by dimension\n",
    "def analyze_diagram(diagram, language=\"\"):\n",
    "    dim0 = diagram[diagram[:, 2] == 0]  # β₀ features\n",
    "    dim1 = diagram[diagram[:, 2] == 1]  # β₁ features\n",
    "    \n",
    "    # Persistence = death - birth\n",
    "    persistence0 = dim0[:, 1] - dim0[:, 0]\n",
    "    persistence1 = dim1[:, 1] - dim1[:, 0]\n",
    "    \n",
    "    print(f\"{language} Persistence Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"β₀ features (connected components): {len(dim0)}\")\n",
    "    if len(persistence0) > 0:\n",
    "        print(f\"  Max persistence: {persistence0.max():.4f}\")\n",
    "        print(f\"  Mean persistence: {persistence0.mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nβ₁ features (loops/holes): {len(dim1)}\")\n",
    "    if len(persistence1) > 0:\n",
    "        print(f\"  Max persistence: {persistence1.max():.4f}\")\n",
    "        print(f\"  Mean persistence: {persistence1.mean():.4f}\")\n",
    "    print()\n",
    "\n",
    "analyze_diagram(en_diagram, \"English\")\n",
    "analyze_diagram(fr_diagram, \"French\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Persistence Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot English persistence diagram\n",
    "fig = plot_diagram(en_diagram)\n",
    "plt.title(f'English Persistence Diagram\\n{example[\"en_text\"][:60]}...')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot French persistence diagram\n",
    "fig = plot_diagram(fr_diagram)\n",
    "plt.title(f'French Persistence Diagram\\n{example[\"fr_text\"][:60]}...')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Persistence Diagrams\n",
    "\n",
    "Use Wasserstein distance to measure similarity between English and French topologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PairwiseDistance\n",
    "\n",
    "# Initialize Wasserstein distance metric\n",
    "wasserstein = PairwiseDistance(metric='wasserstein', n_jobs=-1)\n",
    "\n",
    "# Compute distance between English and French diagrams\n",
    "# Need to reshape to (n_samples, n_features, 3)\n",
    "diagrams_pair = np.array([en_diagram, fr_diagram])\n",
    "\n",
    "# Compute pairwise distances\n",
    "distances = wasserstein.fit_transform(diagrams_pair)\n",
    "\n",
    "print(\"Wasserstein Distance Matrix:\")\n",
    "print(distances)\n",
    "print()\n",
    "print(f\"Wasserstein distance between English and French: {distances[0, 1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process Multiple Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process first 10 examples\n",
    "num_examples = 10\n",
    "wasserstein_distances = []\n",
    "\n",
    "print(f\"Computing persistent homology for {num_examples} sentence pairs...\\n\")\n",
    "\n",
    "for idx in range(num_examples):\n",
    "    example = results[idx]\n",
    "    \n",
    "    # Build distance matrices\n",
    "    en_dist, _ = build_distance_matrix(example['en_attention'], example['en_tokens'])\n",
    "    fr_dist, _ = build_distance_matrix(example['fr_attention'], example['fr_tokens'])\n",
    "    \n",
    "    # Compute persistence\n",
    "    en_diagram = VR.fit_transform([en_dist])[0]\n",
    "    fr_diagram = VR.fit_transform([fr_dist])[0]\n",
    "    \n",
    "    # Compute Wasserstein distance\n",
    "    diagrams_pair = np.array([en_diagram, fr_diagram])\n",
    "    dist_matrix = wasserstein.fit_transform(diagrams_pair)\n",
    "    w_dist = dist_matrix[0, 1]\n",
    "    \n",
    "    wasserstein_distances.append({\n",
    "        'idx': idx,\n",
    "        'en_text': example['en_text'],\n",
    "        'fr_text': example['fr_text'],\n",
    "        'wasserstein_distance': w_dist,\n",
    "        'en_diagram': en_diagram,\n",
    "        'fr_diagram': fr_diagram\n",
    "    })\n",
    "    \n",
    "    print(f\"[{idx}] W-dist: {w_dist:.6f} | EN: {example['en_text'][:40]}...\")\n",
    "\n",
    "print(f\"\\n✓ Processed {num_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "w_dists = [d['wasserstein_distance'] for d in wasserstein_distances]\n",
    "\n",
    "print(\"Wasserstein Distance Statistics (first 10 pairs):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Min:    {np.min(w_dists):.6f}\")\n",
    "print(f\"Max:    {np.max(w_dists):.6f}\")\n",
    "print(f\"Mean:   {np.mean(w_dists):.6f}\")\n",
    "print(f\"Median: {np.median(w_dists):.6f}\")\n",
    "print(f\"Std:    {np.std(w_dists):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(w_dists, bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.axvline(np.mean(w_dists), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(w_dists):.6f}')\n",
    "plt.xlabel('Wasserstein Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Topological Similarity (English vs French)\\nLower = More Similar')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare High vs Low Similarity Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most and least topologically similar pairs\n",
    "sorted_by_similarity = sorted(wasserstein_distances, key=lambda x: x['wasserstein_distance'])\n",
    "\n",
    "print(\"Most topologically similar (lowest Wasserstein distance):\")\n",
    "print(\"=\"*70)\n",
    "most_similar = sorted_by_similarity[0]\n",
    "print(f\"Pair {most_similar['idx']}: W-dist = {most_similar['wasserstein_distance']:.6f}\")\n",
    "print(f\"  EN: {most_similar['en_text']}\")\n",
    "print(f\"  FR: {most_similar['fr_text']}\")\n",
    "print()\n",
    "\n",
    "print(\"Least topologically similar (highest Wasserstein distance):\")\n",
    "print(\"=\"*70)\n",
    "least_similar = sorted_by_similarity[-1]\n",
    "print(f\"Pair {least_similar['idx']}: W-dist = {least_similar['wasserstein_distance']:.6f}\")\n",
    "print(f\"  EN: {least_similar['en_text']}\")\n",
    "print(f\"  FR: {least_similar['fr_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot persistence diagrams for most similar pair\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# English\n",
    "plot_diagram(most_similar['en_diagram'])\n",
    "plt.suptitle(f\"Most Topologically Similar Pair (W-dist: {most_similar['wasserstein_distance']:.6f})\")\n",
    "ax1 = plt.gca()\n",
    "ax1.set_title('English')\n",
    "\n",
    "# French  \n",
    "plt.figure(figsize=(7, 5))\n",
    "plot_diagram(most_similar['fr_diagram'])\n",
    "ax2 = plt.gca()\n",
    "ax2.set_title('French')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Successfully computed topological features!**\n",
    "\n",
    "**What we learned:**\n",
    "- Built distance matrices from attention weights\n",
    "- Computed Vietoris-Rips persistent homology (β₀, β₁)\n",
    "- Visualized persistence diagrams\n",
    "- Measured topological similarity using Wasserstein distance\n",
    "- Analyzed first 10 sentence pairs\n",
    "\n",
    "**Next steps:**\n",
    "1. Scale to all 2000 sentence pairs\n",
    "2. Compute BLEU scores for translation quality\n",
    "3. Correlate Wasserstein distance with BLEU scores\n",
    "4. Statistical analysis and visualization\n",
    "\n",
    "**Key findings so far:**\n",
    "- Wasserstein distances range from ~{min} to ~{max}\n",
    "- Both languages show β₀ and β₁ features\n",
    "- Topological structure varies across sentence pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
