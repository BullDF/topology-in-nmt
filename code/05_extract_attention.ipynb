{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Attention Maps from NLLB Model\n",
    "Explore and extract encoder/decoder attention weights for English → French translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model with eager attention implementation (required for attention output)\nmodel_dir = \"../models/nllb-600M\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_dir,\n    attn_implementation=\"eager\"  # Required for output_attentions=True\n)\n\n# Move to GPU if available\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n\nmodel = model.to(device)\nprint(f\"Model loaded on device: {device}\")\nprint(f\"Attention implementation: eager\")\n\n# Load dataset\ndataset = load_from_disk(\"../data/wmt14_fr-en_validation_2000\")\nprint(f\"\\nLoaded {len(dataset)} sentence pairs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract attention from a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first example\n",
    "example = dataset[0][\"translation\"]\n",
    "english = example[\"en\"]\n",
    "french = example[\"fr\"]\n",
    "\n",
    "print(f\"English: {english}\")\n",
    "print(f\"French:  {french}\")\n",
    "\n",
    "# Tokenize English input\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "inputs = tokenizer(english, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"\\nInput shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translation with attention output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n",
    "        max_length=100,\n",
    "        output_attentions=True,  # IMPORTANT: Enable attention output\n",
    "        return_dict_in_generate=True  # Return structured output\n",
    "    )\n",
    "\n",
    "# Decode translation\n",
    "translation = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"\\nOutput tokens: {tokenizer.convert_ids_to_tokens(outputs.sequences[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand attention structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output structure\n",
    "print(\"Output keys:\", outputs.keys())\n",
    "print(\"\\nAttention types available:\")\n",
    "if hasattr(outputs, 'encoder_attentions') and outputs.encoder_attentions is not None:\n",
    "    print(f\"  - Encoder self-attention: {len(outputs.encoder_attentions)} layers\")\n",
    "if hasattr(outputs, 'decoder_attentions') and outputs.decoder_attentions is not None:\n",
    "    print(f\"  - Decoder self-attention: {len(outputs.decoder_attentions)} timesteps\")\n",
    "if hasattr(outputs, 'cross_attentions') and outputs.cross_attentions is not None:\n",
    "    print(f\"  - Cross-attention: {len(outputs.cross_attentions)} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine encoder attention structure\n",
    "if outputs.encoder_attentions is not None:\n",
    "    encoder_attn = outputs.encoder_attentions\n",
    "    print(f\"Encoder attention:\")\n",
    "    print(f\"  Number of layers: {len(encoder_attn)}\")\n",
    "    print(f\"  Shape per layer: {encoder_attn[0].shape}\")  # (batch, heads, seq_len, seq_len)\n",
    "    print(f\"  Format: (batch_size, num_heads, seq_length, seq_length)\")\n",
    "    \n",
    "    # Get last layer attention\n",
    "    last_layer_attn = encoder_attn[-1][0]  # Remove batch dimension\n",
    "    print(f\"\\n  Last layer shape: {last_layer_attn.shape}\")\n",
    "    print(f\"  Number of attention heads: {last_layer_attn.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine decoder and cross-attention structure\nif outputs.decoder_attentions is not None:\n    print(f\"\\nDecoder self-attention:\")\n    print(f\"  Number of timesteps: {len(outputs.decoder_attentions)}\")\n    print(f\"  Each timestep contains {len(outputs.decoder_attentions[0])} layers\")\n    print(f\"  Shape format: (batch_size, num_heads, query_length, key_length)\")\n    print(f\"  Shape varies per timestep due to causal masking:\")\n    # Show first few timesteps to illustrate the pattern\n    for t in range(min(3, len(outputs.decoder_attentions))):\n        shape = outputs.decoder_attentions[t][0].shape\n        print(f\"    Timestep {t}: {shape}\")\n\nif outputs.cross_attentions is not None:\n    print(f\"\\nCross-attention (decoder attending to encoder):\")\n    print(f\"  Number of timesteps: {len(outputs.cross_attentions)}\")\n    print(f\"  Each timestep contains {len(outputs.cross_attentions[0])} layers\")\n    print(f\"  Shape format: (batch_size, num_heads, decoder_length, encoder_length)\")\n    print(f\"  Shape varies per timestep:\")\n    for t in range(min(3, len(outputs.cross_attentions))):\n        shape = outputs.cross_attentions[t][0].shape\n        print(f\"    Timestep {t}: {shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize encoder self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoder attention from last layer\n",
    "encoder_attn_last = outputs.encoder_attentions[-1][0].cpu().numpy()  # (heads, seq_len, seq_len)\n",
    "\n",
    "# Average over all attention heads\n",
    "encoder_attn_avg = encoder_attn_last.mean(axis=0)  # (seq_len, seq_len)\n",
    "\n",
    "# Get tokens for axis labels\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(encoder_attn_avg, \n",
    "            xticklabels=input_tokens, \n",
    "            yticklabels=input_tokens,\n",
    "            cmap='viridis',\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Encoder Self-Attention (Last Layer, Averaged over Heads)\\nEnglish Sentence')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Attention matrix shape: {encoder_attn_avg.shape}\")\n",
    "print(f\"Min attention: {encoder_attn_avg.min():.4f}\")\n",
    "print(f\"Max attention: {encoder_attn_avg.max():.4f}\")\n",
    "print(f\"Mean attention: {encoder_attn_avg.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize individual attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 4 attention heads from last encoder layer\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(min(4, encoder_attn_last.shape[0])):\n",
    "    attn_head = encoder_attn_last[head_idx]\n",
    "    \n",
    "    sns.heatmap(attn_head, \n",
    "                xticklabels=input_tokens, \n",
    "                yticklabels=input_tokens,\n",
    "                cmap='viridis',\n",
    "                ax=axes[head_idx],\n",
    "                cbar_kws={'label': 'Weight'})\n",
    "    axes[head_idx].set_title(f'Attention Head {head_idx}')\n",
    "    axes[head_idx].set_xlabel('Key')\n",
    "    axes[head_idx].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Encoder Self-Attention Heads (Last Layer)', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Visualize cross-attention (decoder attending to encoder)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize cross-attention from multiple timesteps\n# Show how attention evolves as the decoder generates the translation\nif outputs.cross_attentions is not None:\n    # Pick 3 timesteps: early, middle, late\n    num_timesteps = len(outputs.cross_attentions)\n    timestep_indices = [num_timesteps // 4, num_timesteps // 2, num_timesteps - 1]\n    timestep_names = ['Early', 'Middle', 'Late']\n    \n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    for idx, (t, name) in enumerate(zip(timestep_indices, timestep_names)):\n        # Get cross-attention at this timestep (last layer)\n        cross_attn_t = outputs.cross_attentions[t][-1][0].cpu().numpy()  # (heads, decoder_len, encoder_len)\n        cross_attn_t_avg = cross_attn_t.mean(axis=0)  # Average over heads\n        \n        # Get decoder tokens up to this timestep\n        decoder_tokens_t = output_tokens[:t+1]\n        \n        sns.heatmap(cross_attn_t_avg,\n                    xticklabels=input_tokens,\n                    yticklabels=decoder_tokens_t,\n                    cmap='viridis',\n                    ax=axes[idx],\n                    cbar_kws={'label': 'Weight'})\n        axes[idx].set_title(f'{name} Generation (Timestep {t})')\n        axes[idx].set_xlabel('Encoder (English)')\n        axes[idx].set_ylabel('Decoder (French)')\n    \n    plt.suptitle('Cross-Attention Evolution During Translation', fontsize=14, y=1.02)\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"✓ Shows how the decoder's attention to English words changes as it generates the French translation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Extract and save attention for analysis"
  },
  {
   "cell_type": "code",
   "source": "def extract_attention_maps(text, tokenizer, model, device, src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\"):\n    \"\"\"\n    Extract encoder self-attention for a given text.\n    \n    Returns:\n        dict with:\n            - 'tokens': list of tokens\n            - 'encoder_attention': numpy array (layers, heads, seq_len, seq_len)\n            - 'encoder_attention_avg': numpy array (seq_len, seq_len) - averaged over layers and heads\n    \"\"\"\n    # Tokenize\n    tokenizer.src_lang = src_lang\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n    \n    # Generate with attention\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n            max_length=100,\n            output_attentions=True,\n            return_dict_in_generate=True\n        )\n    \n    # Extract encoder attention\n    encoder_attn_all = torch.stack([layer[0] for layer in outputs.encoder_attentions]).cpu().numpy()\n    # Shape: (layers, heads, seq_len, seq_len)\n    \n    # Average over layers and heads\n    encoder_attn_avg = encoder_attn_all.mean(axis=(0, 1))  # (seq_len, seq_len)\n    \n    return {\n        'tokens': tokens,\n        'encoder_attention': encoder_attn_all,\n        'encoder_attention_avg': encoder_attn_avg\n    }\n\n# Test the function\ntest_result = extract_attention_maps(english, tokenizer, model, device)\nprint(f\"Extracted attention for: {english}\")\nprint(f\"Tokens: {test_result['tokens']}\")\nprint(f\"Encoder attention shape: {test_result['encoder_attention'].shape}\")\nprint(f\"Averaged attention shape: {test_result['encoder_attention_avg'].shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract attention for first 5 examples\nnum_examples = 5\nattention_data = []\n\nprint(f\"Extracting attention for {num_examples} examples...\\n\")\n\nfor i in range(num_examples):\n    example = dataset[i][\"translation\"]\n    english = example[\"en\"]\n    french = example[\"fr\"]\n    \n    result = extract_attention_maps(english, tokenizer, model, device)\n    \n    attention_data.append({\n        'index': i,\n        'english': english,\n        'french': french,\n        'tokens': result['tokens'],\n        'attention_avg': result['encoder_attention_avg']\n    })\n    \n    print(f\"[{i+1}/{num_examples}] Extracted attention for: {english[:50]}...\")\n\nprint(f\"\\n✓ Extracted attention for {len(attention_data)} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize attention for multiple examples\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor i in range(min(3, len(attention_data))):\n    data = attention_data[i]\n    \n    sns.heatmap(data['attention_avg'],\n                xticklabels=data['tokens'],\n                yticklabels=data['tokens'],\n                cmap='viridis',\n                ax=axes[i],\n                cbar_kws={'label': 'Weight'})\n    axes[i].set_title(f\"Example {i+1}\\n{data['english'][:40]}...\", fontsize=10)\n    axes[i].set_xlabel('Key')\n    axes[i].set_ylabel('Query')\n\nplt.suptitle('Encoder Self-Attention (Averaged)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\n**Successfully extracted:**\n- Encoder self-attention maps (English sentence structure)\n- Cross-attention maps (decoder attending to encoder)\n- Attention weights across all layers and heads\n- Averaged attention for graph construction\n\n**Next steps:**\n1. Build attention graphs (tokens as nodes, attention weights as edges)\n2. Compute persistent homology on the graphs\n3. Compare English vs French topological structures",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}