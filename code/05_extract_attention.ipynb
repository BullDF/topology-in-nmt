{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Attention Maps from NLLB Model\n",
    "Explore and extract encoder/decoder attention weights for English → French translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model with eager attention implementation (required for attention output)\nmodel_dir = \"../models/nllb-600M\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_dir,\n    attn_implementation=\"eager\"  # Required for output_attentions=True\n)\n\n# Move to GPU if available\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n\nmodel = model.to(device)\nprint(f\"Model loaded on device: {device}\")\nprint(f\"Attention implementation: eager\")\n\n# Load dataset\ndataset = load_from_disk(\"../data/wmt14_fr-en_validation_2000\")\nprint(f\"\\nLoaded {len(dataset)} sentence pairs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract attention from a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first example\n",
    "example = dataset[0][\"translation\"]\n",
    "english = example[\"en\"]\n",
    "french = example[\"fr\"]\n",
    "\n",
    "print(f\"English: {english}\")\n",
    "print(f\"French:  {french}\")\n",
    "\n",
    "# Tokenize English input\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "inputs = tokenizer(english, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"\\nInput shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translation with attention output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"),\n",
    "        max_length=100,\n",
    "        output_attentions=True,  # IMPORTANT: Enable attention output\n",
    "        return_dict_in_generate=True  # Return structured output\n",
    "    )\n",
    "\n",
    "# Decode translation\n",
    "translation = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"\\nOutput tokens: {tokenizer.convert_ids_to_tokens(outputs.sequences[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand attention structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output structure\n",
    "print(\"Output keys:\", outputs.keys())\n",
    "print(\"\\nAttention types available:\")\n",
    "if hasattr(outputs, 'encoder_attentions') and outputs.encoder_attentions is not None:\n",
    "    print(f\"  - Encoder self-attention: {len(outputs.encoder_attentions)} layers\")\n",
    "if hasattr(outputs, 'decoder_attentions') and outputs.decoder_attentions is not None:\n",
    "    print(f\"  - Decoder self-attention: {len(outputs.decoder_attentions)} timesteps\")\n",
    "if hasattr(outputs, 'cross_attentions') and outputs.cross_attentions is not None:\n",
    "    print(f\"  - Cross-attention: {len(outputs.cross_attentions)} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine encoder attention structure\n",
    "if outputs.encoder_attentions is not None:\n",
    "    encoder_attn = outputs.encoder_attentions\n",
    "    print(f\"Encoder attention:\")\n",
    "    print(f\"  Number of layers: {len(encoder_attn)}\")\n",
    "    print(f\"  Shape per layer: {encoder_attn[0].shape}\")  # (batch, heads, seq_len, seq_len)\n",
    "    print(f\"  Format: (batch_size, num_heads, seq_length, seq_length)\")\n",
    "    \n",
    "    # Get last layer attention\n",
    "    last_layer_attn = encoder_attn[-1][0]  # Remove batch dimension\n",
    "    print(f\"\\n  Last layer shape: {last_layer_attn.shape}\")\n",
    "    print(f\"  Number of attention heads: {last_layer_attn.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine decoder and cross-attention structure\nif outputs.decoder_attentions is not None:\n    print(f\"\\nDecoder self-attention:\")\n    print(f\"  Number of timesteps: {len(outputs.decoder_attentions)}\")\n    print(f\"  Each timestep contains {len(outputs.decoder_attentions[0])} layers\")\n    print(f\"  Shape format: (batch_size, num_heads, query_length, key_length)\")\n    print(f\"  Shape varies per timestep due to causal masking:\")\n    # Show first few timesteps to illustrate the pattern\n    for t in range(min(3, len(outputs.decoder_attentions))):\n        shape = outputs.decoder_attentions[t][0].shape\n        print(f\"    Timestep {t}: {shape}\")\n\nif outputs.cross_attentions is not None:\n    print(f\"\\nCross-attention (decoder attending to encoder):\")\n    print(f\"  Number of timesteps: {len(outputs.cross_attentions)}\")\n    print(f\"  Each timestep contains {len(outputs.cross_attentions[0])} layers\")\n    print(f\"  Shape format: (batch_size, num_heads, decoder_length, encoder_length)\")\n    print(f\"  Shape varies per timestep:\")\n    for t in range(min(3, len(outputs.cross_attentions))):\n        shape = outputs.cross_attentions[t][0].shape\n        print(f\"    Timestep {t}: {shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize encoder self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoder attention from last layer\n",
    "encoder_attn_last = outputs.encoder_attentions[-1][0].cpu().numpy()  # (heads, seq_len, seq_len)\n",
    "\n",
    "# Average over all attention heads\n",
    "encoder_attn_avg = encoder_attn_last.mean(axis=0)  # (seq_len, seq_len)\n",
    "\n",
    "# Get tokens for axis labels\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(encoder_attn_avg, \n",
    "            xticklabels=input_tokens, \n",
    "            yticklabels=input_tokens,\n",
    "            cmap='viridis',\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Encoder Self-Attention (Last Layer, Averaged over Heads)\\nEnglish Sentence')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Attention matrix shape: {encoder_attn_avg.shape}\")\n",
    "print(f\"Min attention: {encoder_attn_avg.min():.4f}\")\n",
    "print(f\"Max attention: {encoder_attn_avg.max():.4f}\")\n",
    "print(f\"Mean attention: {encoder_attn_avg.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize individual attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 4 attention heads from last encoder layer\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(min(4, encoder_attn_last.shape[0])):\n",
    "    attn_head = encoder_attn_last[head_idx]\n",
    "    \n",
    "    sns.heatmap(attn_head, \n",
    "                xticklabels=input_tokens, \n",
    "                yticklabels=input_tokens,\n",
    "                cmap='viridis',\n",
    "                ax=axes[head_idx],\n",
    "                cbar_kws={'label': 'Weight'})\n",
    "    axes[head_idx].set_title(f'Attention Head {head_idx}')\n",
    "    axes[head_idx].set_xlabel('Key')\n",
    "    axes[head_idx].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Encoder Self-Attention Heads (Last Layer)', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5b. Compare attention across layers (early vs late)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare attention from different layers: early (layer 0), middle, and late (last layer)\nnum_layers = len(outputs.encoder_attentions)\nlayer_indices = [0, num_layers // 2, num_layers - 1]  # First, middle, last\nlayer_names = ['First Layer (0)', f'Middle Layer ({num_layers // 2})', f'Last Layer ({num_layers - 1})']\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, (layer_idx, layer_name) in enumerate(zip(layer_indices, layer_names)):\n    # Get attention from this layer and average over heads\n    layer_attn = outputs.encoder_attentions[layer_idx][0].cpu().numpy()  # (heads, seq_len, seq_len)\n    layer_attn_avg = layer_attn.mean(axis=0)  # Average over heads\n    \n    # Plot\n    sns.heatmap(layer_attn_avg,\n                xticklabels=input_tokens,\n                yticklabels=input_tokens,\n                cmap='viridis',\n                ax=axes[idx],\n                cbar_kws={'label': 'Weight'})\n    axes[idx].set_title(f'{layer_name}\\n(Averaged over {layer_attn.shape[0]} heads)')\n    axes[idx].set_xlabel('Key Position')\n    axes[idx].set_ylabel('Query Position')\n\nplt.suptitle('Encoder Self-Attention: Comparing Early vs Middle vs Late Layers', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Print statistics for each layer\nprint(\"Attention statistics by layer:\")\nprint(\"=\" * 60)\nfor layer_idx, layer_name in zip(layer_indices, layer_names):\n    layer_attn = outputs.encoder_attentions[layer_idx][0].cpu().numpy()\n    layer_attn_avg = layer_attn.mean(axis=0)\n    \n    # Exclude special tokens for content-to-content attention\n    # Assuming first token is lang tag and last is </s>\n    content_attn = layer_attn_avg[1:-1, 1:-1]\n    \n    print(f\"\\n{layer_name}:\")\n    print(f\"  Full attention - Mean: {layer_attn_avg.mean():.4f}, Max: {layer_attn_avg.max():.4f}\")\n    if content_attn.size > 0:\n        print(f\"  Content-only - Mean: {content_attn.mean():.4f}, Max: {content_attn.max():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5c. Filter special tokens and apply threshold",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def filter_and_threshold_attention(attn_matrix, tokens, threshold=0.1):\n    \"\"\"\n    Filter special tokens and apply threshold to attention matrix.\n    \n    Args:\n        attn_matrix: (seq_len, seq_len) attention matrix\n        tokens: list of token strings\n        threshold: minimum attention weight to keep (default 0.1)\n    \n    Returns:\n        dict with:\n            - 'filtered_attn': filtered and renormalized attention matrix\n            - 'content_tokens': tokens without special tokens\n            - 'num_edges': number of edges after thresholding\n    \"\"\"\n    # Identify special token indices (first and last)\n    # First token is usually language tag (eng_Latn), last is </s>\n    special_indices = [0, len(tokens) - 1]\n    \n    # Get content token indices\n    content_indices = [i for i in range(len(tokens)) if i not in special_indices]\n    \n    # Extract content-only attention (remove special tokens)\n    filtered_attn = attn_matrix[content_indices, :][:, content_indices]\n    content_tokens = [tokens[i] for i in content_indices]\n    \n    # Apply threshold (set values below threshold to 0)\n    thresholded_attn = filtered_attn.copy()\n    thresholded_attn[thresholded_attn < threshold] = 0\n    \n    # Renormalize rows to sum to 1 (only for non-zero rows)\n    row_sums = thresholded_attn.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1  # Avoid division by zero\n    renormalized_attn = thresholded_attn / row_sums\n    \n    # Count edges (non-zero entries)\n    num_edges = (renormalized_attn > 0).sum()\n    \n    return {\n        'filtered_attn': renormalized_attn,\n        'content_tokens': content_tokens,\n        'num_edges': num_edges,\n        'sparsity': 1 - (num_edges / (renormalized_attn.size))\n    }\n\n# Test with different thresholds\nthresholds = [0.0, 0.1, 0.2]\nprint(\"Testing different threshold values:\")\nprint(\"=\" * 70)\n\nfor thresh in thresholds:\n    result = filter_and_threshold_attention(encoder_attn_avg, input_tokens, threshold=thresh)\n    print(f\"\\nThreshold: {thresh}\")\n    print(f\"  Content tokens: {len(result['content_tokens'])}\")\n    print(f\"  Matrix shape: {result['filtered_attn'].shape}\")\n    print(f\"  Number of edges: {result['num_edges']}\")\n    print(f\"  Sparsity: {result['sparsity']:.2%}\")\n    print(f\"  Mean attention: {result['filtered_attn'].mean():.4f}\")\n    print(f\"  Max attention: {result['filtered_attn'].max():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize before and after filtering + thresholding\nthreshold = 0.1\nfiltered_result = filter_and_threshold_attention(encoder_attn_avg, input_tokens, threshold=threshold)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Before: Original attention\nsns.heatmap(encoder_attn_avg,\n            xticklabels=input_tokens,\n            yticklabels=input_tokens,\n            cmap='viridis',\n            ax=axes[0],\n            cbar_kws={'label': 'Weight'})\naxes[0].set_title(f'Before Filtering\\n({len(input_tokens)} tokens, including special tokens)')\naxes[0].set_xlabel('Key Position')\naxes[0].set_ylabel('Query Position')\n\n# After: Filtered and thresholded\nsns.heatmap(filtered_result['filtered_attn'],\n            xticklabels=filtered_result['content_tokens'],\n            yticklabels=filtered_result['content_tokens'],\n            cmap='viridis',\n            ax=axes[1],\n            cbar_kws={'label': 'Weight'})\naxes[1].set_title(f'After Filtering + Threshold={threshold}\\n({len(filtered_result[\"content_tokens\"])} content tokens, {filtered_result[\"num_edges\"]} edges, {filtered_result[\"sparsity\"]:.1%} sparse)')\naxes[1].set_xlabel('Key Position')\naxes[1].set_ylabel('Query Position')\n\nplt.suptitle('Attention Filtering: Removing Special Tokens + Thresholding', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFiltering summary:\")\nprint(f\"  Original shape: {encoder_attn_avg.shape}\")\nprint(f\"  Filtered shape: {filtered_result['filtered_attn'].shape}\")\nprint(f\"  Removed tokens: {[input_tokens[0], input_tokens[-1]]}\")\nprint(f\"  Content tokens: {filtered_result['content_tokens']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test with very low thresholds to preserve more structure\nprint(\"Testing lower thresholds to preserve attention structure\")\nprint(\"=\" * 70)\n\n# Test a range of low thresholds\nlow_thresholds = [0.0, 0.01, 0.02, 0.05, 0.10]\n\nfig, axes = plt.subplots(1, len(low_thresholds), figsize=(5*len(low_thresholds), 5))\n\nfor idx, thresh in enumerate(low_thresholds):\n    result = filter_and_threshold_attention(encoder_attn_avg, input_tokens, threshold=thresh)\n    \n    sns.heatmap(result['filtered_attn'],\n                xticklabels=result['content_tokens'],\n                yticklabels=result['content_tokens'],\n                cmap='viridis',\n                ax=axes[idx],\n                vmin=0,\n                vmax=0.3,  # Fix color scale for comparison\n                cbar_kws={'label': 'Weight'})\n    \n    axes[idx].set_title(f'Threshold = {thresh}\\n{result[\"num_edges\"]} edges ({(1-result[\"sparsity\"]):.1%} dense)')\n    axes[idx].set_xlabel('Key')\n    axes[idx].set_ylabel('Query')\n    \n    # Print stats\n    print(f\"\\nThreshold {thresh}:\")\n    print(f\"  Edges: {result['num_edges']} / {result['filtered_attn'].size} ({(1-result['sparsity']):.1%} dense)\")\n    print(f\"  Mean weight: {result['filtered_attn'].mean():.6f}\")\n    print(f\"  Max weight: {result['filtered_attn'].max():.6f}\")\n\nplt.suptitle('Effect of Threshold on Attention Graph Sparsity', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Recommendation: Use threshold between 0.01-0.05 to balance sparsity and structure\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check individual attention heads for diagonal patterns\n# Maybe averaging is washing out the diagonal pattern\nprint(\"Checking individual attention heads for diagonal patterns\")\nprint(\"=\" * 70)\n\nencoder_attn_last_layer = outputs.encoder_attentions[-1][0].cpu().numpy()  # (heads, seq_len, seq_len)\nnum_heads = encoder_attn_last_layer.shape[0]\n\n# For each head, compute diagonal strength\nhead_diagonal_strengths = []\nfor head_idx in range(num_heads):\n    head_attn = encoder_attn_last_layer[head_idx]\n    # Get content-only attention (remove special tokens)\n    content_head_attn = head_attn[1:-1, 1:-1]\n    \n    # Compute diagonal vs off-diagonal mean\n    diag_mean = np.diag(content_head_attn).mean()\n    off_diag_mask = ~np.eye(content_head_attn.shape[0], dtype=bool)\n    off_diag_mean = content_head_attn[off_diag_mask].mean()\n    \n    ratio = diag_mean / off_diag_mean if off_diag_mean > 0 else 0\n    head_diagonal_strengths.append({\n        'head': head_idx,\n        'diag_mean': diag_mean,\n        'off_diag_mean': off_diag_mean,\n        'ratio': ratio\n    })\n\n# Sort by ratio (strongest diagonal first)\nhead_diagonal_strengths.sort(key=lambda x: x['ratio'], reverse=True)\n\nprint(f\"\\nAttention heads ranked by diagonal strength (top 5):\")\nprint(f\"{'Head':<6} {'Diag Mean':<12} {'Off-Diag Mean':<15} {'Ratio':<10}\")\nprint(\"-\" * 50)\nfor i, stats in enumerate(head_diagonal_strengths[:5]):\n    print(f\"{stats['head']:<6} {stats['diag_mean']:<12.6f} {stats['off_diag_mean']:<15.6f} {stats['ratio']:<10.2f}x\")\n\n# Visualize the head with strongest diagonal\nbest_head_idx = head_diagonal_strengths[0]['head']\nbest_head_attn = encoder_attn_last_layer[best_head_idx][1:-1, 1:-1]  # Remove special tokens\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot the best head (strongest diagonal)\nsns.heatmap(best_head_attn,\n            xticklabels=content_tokens_list,\n            yticklabels=content_tokens_list,\n            cmap='viridis',\n            ax=axes[0],\n            cbar_kws={'label': 'Weight'})\naxes[0].set_title(f'Head {best_head_idx} (Strongest Diagonal)\\nRatio: {head_diagonal_strengths[0][\"ratio\"]:.2f}x')\naxes[0].set_xlabel('Key')\naxes[0].set_ylabel('Query')\n\n# Plot the averaged attention for comparison\nsns.heatmap(content_attn,\n            xticklabels=content_tokens_list,\n            yticklabels=content_tokens_list,\n            cmap='viridis',\n            ax=axes[1],\n            cbar_kws={'label': 'Weight'})\naxes[1].set_title(f'Averaged Over All {num_heads} Heads\\n(This is what we\\'ve been using)')\naxes[1].set_xlabel('Key')\naxes[1].set_ylabel('Query')\n\n# Plot distribution of diagonal ratios across all heads\nratios = [h['ratio'] for h in head_diagonal_strengths]\naxes[2].bar(range(len(ratios)), ratios)\naxes[2].axhline(1.0, color='red', linestyle='--', label='Equal (ratio=1)')\naxes[2].set_xlabel('Attention Head (sorted by diagonal strength)')\naxes[2].set_ylabel('Diagonal / Off-Diagonal Ratio')\naxes[2].set_title(f'Diagonal Strength Across All {num_heads} Heads')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle('Individual Attention Heads: Is Averaging Hiding Diagonal Patterns?', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n✓ Found {sum(1 for h in head_diagonal_strengths if h['ratio'] > 1.0)} heads with diagonal > off-diagonal\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze attention value distribution\nprint(\"Attention Value Distribution Analysis\")\nprint(\"=\" * 70)\n\n# Get content-only attention (remove special tokens)\ncontent_attn = encoder_attn_avg[1:-1, 1:-1]\ncontent_tokens_list = input_tokens[1:-1]\n\nprint(f\"\\nContent-only attention matrix shape: {content_attn.shape}\")\nprint(f\"Number of content tokens: {len(content_tokens_list)}\")\nprint(f\"Content tokens: {content_tokens_list}\")\n\n# Get diagonal and off-diagonal values\ndiagonal_values = np.diag(content_attn)\noff_diagonal_mask = ~np.eye(content_attn.shape[0], dtype=bool)\noff_diagonal_values = content_attn[off_diagonal_mask]\n\nprint(f\"\\nDiagonal (self-attention) values:\")\nprint(f\"  Min: {diagonal_values.min():.6f}\")\nprint(f\"  Max: {diagonal_values.max():.6f}\")\nprint(f\"  Mean: {diagonal_values.mean():.6f}\")\nprint(f\"  Median: {np.median(diagonal_values):.6f}\")\nprint(f\"  Values: {diagonal_values}\")\n\nprint(f\"\\nOff-diagonal values:\")\nprint(f\"  Min: {off_diagonal_values.min():.6f}\")\nprint(f\"  Max: {off_diagonal_values.max():.6f}\")\nprint(f\"  Mean: {off_diagonal_values.mean():.6f}\")\nprint(f\"  Median: {np.median(off_diagonal_values):.6f}\")\n\nprint(f\"\\nComparison:\")\nprint(f\"  Diagonal mean / Off-diagonal mean: {diagonal_values.mean() / off_diagonal_values.mean():.2f}x\")\nprint(f\"  → Diagonal is {'STRONGER' if diagonal_values.mean() > off_diagonal_values.mean() else 'WEAKER'} than off-diagonal\")\n\n# Visualize distribution\nfig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n# Histogram of all values\naxes[0].hist(content_attn.flatten(), bins=50, alpha=0.7, label='All values', edgecolor='black')\naxes[0].axvline(diagonal_values.mean(), color='red', linestyle='--', linewidth=2, label=f'Diagonal mean ({diagonal_values.mean():.4f})')\naxes[0].axvline(off_diagonal_values.mean(), color='blue', linestyle='--', linewidth=2, label=f'Off-diag mean ({off_diagonal_values.mean():.4f})')\naxes[0].set_xlabel('Attention Weight')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Distribution of All Attention Values')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Separate histograms\naxes[1].hist(diagonal_values, bins=20, alpha=0.7, color='red', label='Diagonal', edgecolor='black')\naxes[1].hist(off_diagonal_values, bins=50, alpha=0.5, color='blue', label='Off-diagonal', edgecolor='black')\naxes[1].set_xlabel('Attention Weight')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Diagonal vs Off-Diagonal Values')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Box plot\naxes[2].boxplot([diagonal_values, off_diagonal_values], labels=['Diagonal', 'Off-diagonal'])\naxes[2].set_ylabel('Attention Weight')\naxes[2].set_title('Value Distribution Comparison')\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle('Content-Only Attention: Analyzing Self-Attention (Diagonal) Patterns', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5d. Investigate attention patterns: Why no diagonal?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract and save attention for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_maps(text, tokenizer, model, device, src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\"):\n",
    "    \"\"\"\n",
    "    Extract encoder self-attention for a given text.\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "            - 'tokens': list of tokens\n",
    "            - 'encoder_attention': numpy array (layers, heads, seq_len, seq_len)\n",
    "            - 'encoder_attention_avg': numpy array (seq_len, seq_len) - averaged over layers and heads\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokenizer.src_lang = src_lang\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
    "    \n",
    "    # Generate with attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "            max_length=100,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    # Extract encoder attention\n",
    "    encoder_attn_all = torch.stack([layer[0] for layer in outputs.encoder_attentions]).cpu().numpy()\n",
    "    # Shape: (layers, heads, seq_len, seq_len)\n",
    "    \n",
    "    # Average over layers and heads\n",
    "    encoder_attn_avg = encoder_attn_all.mean(axis=(0, 1))  # (seq_len, seq_len)\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'encoder_attention': encoder_attn_all,\n",
    "        'encoder_attention_avg': encoder_attn_avg\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_result = extract_attention_maps(english, tokenizer, model, device)\n",
    "print(f\"Extracted attention for: {english}\")\n",
    "print(f\"Tokens: {test_result['tokens']}\")\n",
    "print(f\"Encoder attention shape: {test_result['encoder_attention'].shape}\")\n",
    "print(f\"Averaged attention shape: {test_result['encoder_attention_avg'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on multiple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention for first 5 examples\n",
    "num_examples = 5\n",
    "attention_data = []\n",
    "\n",
    "print(f\"Extracting attention for {num_examples} examples...\\n\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    example = dataset[i][\"translation\"]\n",
    "    english = example[\"en\"]\n",
    "    french = example[\"fr\"]\n",
    "    \n",
    "    result = extract_attention_maps(english, tokenizer, model, device)\n",
    "    \n",
    "    attention_data.append({\n",
    "        'index': i,\n",
    "        'english': english,\n",
    "        'french': french,\n",
    "        'tokens': result['tokens'],\n",
    "        'attention_avg': result['encoder_attention_avg']\n",
    "    })\n",
    "    \n",
    "    print(f\"[{i+1}/{num_examples}] Extracted attention for: {english[:50]}...\")\n",
    "\n",
    "print(f\"\\n✓ Extracted attention for {len(attention_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for multiple examples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i in range(min(3, len(attention_data))):\n",
    "    data = attention_data[i]\n",
    "    \n",
    "    sns.heatmap(data['attention_avg'],\n",
    "                xticklabels=data['tokens'],\n",
    "                yticklabels=data['tokens'],\n",
    "                cmap='viridis',\n",
    "                ax=axes[i],\n",
    "                cbar_kws={'label': 'Weight'})\n",
    "    axes[i].set_title(f\"Example {i+1}\\n{data['english'][:40]}...\", fontsize=10)\n",
    "    axes[i].set_xlabel('Key')\n",
    "    axes[i].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Encoder Self-Attention (Averaged)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Successfully extracted:**\n",
    "- Encoder self-attention maps (English sentence structure)\n",
    "- Attention weights across all layers and heads\n",
    "- Averaged attention for graph construction\n",
    "\n",
    "**Next steps:**\n",
    "1. Create batch script to process all 2000 examples\n",
    "2. Build attention graphs (tokens as nodes, attention weights as edges)\n",
    "3. Compute persistent homology on the graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}