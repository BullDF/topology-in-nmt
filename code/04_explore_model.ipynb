{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLLB-600M Model Exploration\n",
    "Load the saved model and test French ↔ English translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model from local directory\n",
    "model_dir = \"../models/nllb-600M\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "print(\"✓ Tokenizer loaded\")\n",
    "\n",
    "print(\"\\nLoading model...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "print(f\"\\nModel: NLLB-200-distilled-600M\")\n",
    "print(f\"Parameters: ~600M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test French → English translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLLB uses language codes: fra_Latn (French), eng_Latn (English)\n",
    "test_sentence = \"Bonjour, comment allez-vous?\"\n",
    "print(f\"Input (French): {test_sentence}\")\n",
    "\n",
    "# Set source language\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Generate translation\n",
    "translated_tokens = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"],\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "# Decode\n",
    "translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "print(f\"Output (English): {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test with multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Bonjour, comment allez-vous?\",\n",
    "    \"Je suis étudiant à l'université.\",\n",
    "    \"Le chat est sur la table.\",\n",
    "    \"Quelle heure est-il?\",\n",
    "    \"J'aime apprendre les langues.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"French → English Translations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"],\n",
    "        max_length=50\n",
    "    )\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"\\n{i}. FR: {sentence}\")\n",
    "    print(f\"   EN: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Number of encoder layers: {model.config.encoder_layers}\")\n",
    "print(f\"  Number of decoder layers: {model.config.decoder_layers}\")\n",
    "print(f\"  Number of attention heads: {model.config.encoder_attention_heads}\")\n",
    "print(f\"  Hidden size: {model.config.d_model}\")\n",
    "print(f\"  Vocabulary size: {model.config.vocab_size}\")\n",
    "print(f\"\\nModel has encoder-decoder architecture for sequence-to-sequence translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nParameters (in millions): {total_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test with dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some examples from our saved dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"../data/wmt14_fr-en_validation_2000\")\n",
    "print(f\"Loaded {len(dataset)} sentence pairs\\n\")\n",
    "\n",
    "# Test on first 3 examples\n",
    "print(\"=\"*80)\n",
    "print(\"Testing on WMT14 dataset examples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "\n",
    "for i in range(3):\n",
    "    example = dataset[i][\"translation\"]\n",
    "    french = example[\"fr\"]\n",
    "    english_ref = example[\"en\"]\n",
    "    \n",
    "    # Translate\n",
    "    inputs = tokenizer(french, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"],\n",
    "        max_length=100\n",
    "    )\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"FR: {french}\")\n",
    "    print(f\"EN (reference): {english_ref}\")\n",
    "    print(f\"EN (translated): {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model loaded successfully:**\n",
    "- NLLB-200-distilled-600M (~600M parameters)\n",
    "- French ↔ English translation working\n",
    "- Ready for attention extraction\n",
    "\n",
    "**Next steps:**\n",
    "1. Extract attention weights from encoder and decoder\n",
    "2. Build attention graphs\n",
    "3. Compute persistent homology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
