{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Attention Extraction\n",
    "\n",
    "This notebook demonstrates extracting encoder self-attention maps from **both** English and French sentences by running the NLLB model in both translation directions:\n",
    "\n",
    "1. **EN → FR**: English source → Extract English encoder attention\n",
    "2. **FR → EN**: French source → Extract French encoder attention\n",
    "\n",
    "This bidirectional extraction allows us to compare the topological structure of encoder attention patterns across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datasets import load_from_disk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"../models/nllb-600M\"\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load sample data\ndata_path = Path(\"../data/wmt14_fr-en_validation_2000\")\ndataset = load_from_disk(str(data_path))\n\n# Convert to pandas and extract en/fr columns from translation dict\ndf = dataset.to_pandas()\ndf['en'] = df['translation'].apply(lambda x: x['en'])\ndf['fr'] = df['translation'].apply(lambda x: x['fr'])\ndf = df[['en', 'fr']]  # Keep only en and fr columns\n\nprint(f\"Loaded {len(df)} sentence pairs\")\nprint(f\"\\nColumns: {df.columns.tolist()}\")\ndf.head(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Encoder Attention in Both Directions\n",
    "\n",
    "For each sentence pair, we'll:\n",
    "1. Run **EN → FR** translation and extract **English encoder attention**\n",
    "2. Run **FR → EN** translation and extract **French encoder attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder_attention(text, src_lang, tgt_lang, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Extract encoder self-attention for a given source text.\n",
    "    \n",
    "    Args:\n",
    "        text: Source text string\n",
    "        src_lang: Source language code (e.g., 'eng_Latn', 'fra_Latn')\n",
    "        tgt_lang: Target language code (e.g., 'fra_Latn', 'eng_Latn')\n",
    "        tokenizer: NLLB tokenizer\n",
    "        model: NLLB model\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - tokens: List of source tokens\n",
    "            - encoder_attention: Encoder self-attention (num_layers, num_heads, seq_len, seq_len)\n",
    "            - translation: Generated translation text\n",
    "    \"\"\"\n",
    "    # Set source language\n",
    "    tokenizer.src_lang = src_lang\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get target language BOS token\n",
    "    tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "    \n",
    "    # Generate translation with attention output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_id,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    # Extract encoder attention (available in encoder_attentions)\n",
    "    # Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "    encoder_attention = outputs.encoder_attentions\n",
    "    encoder_attention = torch.stack([layer.squeeze(0) for layer in encoder_attention])  # (num_layers, num_heads, seq_len, seq_len)\n",
    "    \n",
    "    # Decode tokens\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "    translation = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'tokens': input_tokens,\n",
    "        'encoder_attention': encoder_attention.cpu().numpy(),\n",
    "        'translation': translation\n",
    "    }\n",
    "\n",
    "print(\"Function defined: extract_encoder_attention()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test on Sample Sentence Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample sentence pair\n",
    "idx = 10\n",
    "en_text = df.iloc[idx]['en']\n",
    "fr_text = df.iloc[idx]['fr']\n",
    "\n",
    "print(f\"Sample {idx}:\")\n",
    "print(f\"English: {en_text}\")\n",
    "print(f\"French:  {fr_text}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract English encoder attention (EN → FR)\n",
    "print(\"Extracting English encoder attention (EN → FR)...\")\n",
    "en_result = extract_encoder_attention(\n",
    "    text=en_text,\n",
    "    src_lang='eng_Latn',\n",
    "    tgt_lang='fra_Latn',\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"English tokens: {en_result['tokens']}\")\n",
    "print(f\"English encoder attention shape: {en_result['encoder_attention'].shape}\")\n",
    "print(f\"Translation to French: {en_result['translation']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract French encoder attention (FR → EN)\n",
    "print(\"Extracting French encoder attention (FR → EN)...\")\n",
    "fr_result = extract_encoder_attention(\n",
    "    text=fr_text,\n",
    "    src_lang='fra_Latn',\n",
    "    tgt_lang='eng_Latn',\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"French tokens: {fr_result['tokens']}\")\n",
    "print(f\"French encoder attention shape: {fr_result['encoder_attention'].shape}\")\n",
    "print(f\"Translation to English: {fr_result['translation']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Encoder Attention Maps\n",
    "\n",
    "Compare encoder attention patterns from English and French for the same sentence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoder_attention(attention, tokens, layer=0, head=0, title=\"Encoder Self-Attention\", filter_special=True):\n",
    "    \"\"\"\n",
    "    Plot encoder self-attention heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention: Attention weights (num_layers, num_heads, seq_len, seq_len)\n",
    "        tokens: List of token strings\n",
    "        layer: Which layer to visualize\n",
    "        head: Which attention head to visualize\n",
    "        title: Plot title\n",
    "        filter_special: Whether to filter out special tokens\n",
    "    \"\"\"\n",
    "    # Extract specified layer and head\n",
    "    attn = attention[layer, head]  # (seq_len, seq_len)\n",
    "    \n",
    "    # Filter special tokens if requested\n",
    "    if filter_special:\n",
    "        # Keep only content tokens (not </s>, <pad>, etc.)\n",
    "        special_tokens = {'</s>', '<s>', '<pad>', '▁'}  # Common special tokens\n",
    "        content_mask = [tok not in special_tokens for tok in tokens]\n",
    "        \n",
    "        if sum(content_mask) > 0:  # Only filter if there are content tokens\n",
    "            attn = attn[content_mask][:, content_mask]\n",
    "            tokens = [tok for tok, keep in zip(tokens, content_mask) if keep]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        square=True\n",
    "    )\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.title(f\"{title}\\nLayer {layer}, Head {head}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Function defined: plot_encoder_attention()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize English encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=en_result['encoder_attention'],\n",
    "    tokens=en_result['tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"English Encoder Attention\\n'{en_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize French encoder attention\n",
    "plot_encoder_attention(\n",
    "    attention=fr_result['encoder_attention'],\n",
    "    tokens=fr_result['tokens'],\n",
    "    layer=0,\n",
    "    head=0,\n",
    "    title=f\"French Encoder Attention\\n'{fr_text}'\",\n",
    "    filter_special=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Attention Statistics\n",
    "\n",
    "Compute basic statistics to compare English and French encoder attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average attention across all layers and heads\n",
    "en_avg_attention = en_result['encoder_attention'].mean(axis=(0, 1))  # (seq_len, seq_len)\n",
    "fr_avg_attention = fr_result['encoder_attention'].mean(axis=(0, 1))  # (seq_len, seq_len)\n",
    "\n",
    "print(\"English encoder attention statistics:\")\n",
    "print(f\"  Shape: {en_avg_attention.shape}\")\n",
    "print(f\"  Mean:  {en_avg_attention.mean():.4f}\")\n",
    "print(f\"  Std:   {en_avg_attention.std():.4f}\")\n",
    "print(f\"  Min:   {en_avg_attention.min():.4f}\")\n",
    "print(f\"  Max:   {en_avg_attention.max():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"French encoder attention statistics:\")\n",
    "print(f\"  Shape: {fr_avg_attention.shape}\")\n",
    "print(f\"  Mean:  {fr_avg_attention.mean():.4f}\")\n",
    "print(f\"  Std:   {fr_avg_attention.std():.4f}\")\n",
    "print(f\"  Min:   {fr_avg_attention.min():.4f}\")\n",
    "print(f\"  Max:   {fr_avg_attention.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process Multiple Samples\n",
    "\n",
    "Extract encoder attention for a few more sentence pairs to verify the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process first 5 sentence pairs\n",
    "num_samples = 5\n",
    "results = []\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    en_text = df.iloc[idx]['en']\n",
    "    fr_text = df.iloc[idx]['fr']\n",
    "    \n",
    "    print(f\"\\nProcessing pair {idx}...\")\n",
    "    print(f\"  EN: {en_text[:60]}...\")\n",
    "    print(f\"  FR: {fr_text[:60]}...\")\n",
    "    \n",
    "    # Extract encoder attention for both directions\n",
    "    en_result = extract_encoder_attention(en_text, 'eng_Latn', 'fra_Latn', tokenizer, model, device)\n",
    "    fr_result = extract_encoder_attention(fr_text, 'fra_Latn', 'eng_Latn', tokenizer, model, device)\n",
    "    \n",
    "    results.append({\n",
    "        'idx': idx,\n",
    "        'en_text': en_text,\n",
    "        'fr_text': fr_text,\n",
    "        'en_tokens': en_result['tokens'],\n",
    "        'fr_tokens': fr_result['tokens'],\n",
    "        'en_attention': en_result['encoder_attention'],\n",
    "        'fr_attention': fr_result['encoder_attention'],\n",
    "        'en_translation': en_result['translation'],\n",
    "        'fr_translation': fr_result['translation']\n",
    "    })\n",
    "    \n",
    "    print(f\"  EN attention shape: {en_result['encoder_attention'].shape}\")\n",
    "    print(f\"  FR attention shape: {fr_result['encoder_attention'].shape}\")\n",
    "\n",
    "print(f\"\\n✓ Processed {len(results)} sentence pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of extracted data\n",
    "print(\"Summary of extracted encoder attention:\")\n",
    "print(f\"Total pairs processed: {len(results)}\")\n",
    "print()\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Pair {result['idx']}:\")\n",
    "    print(f\"  English: {len(result['en_tokens'])} tokens, attention shape {result['en_attention'].shape}\")\n",
    "    print(f\"  French:  {len(result['fr_tokens'])} tokens, attention shape {result['fr_attention'].shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates bidirectional encoder attention extraction:\n",
    "\n",
    "1. ✅ **EN → FR**: Extract English encoder attention\n",
    "2. ✅ **FR → EN**: Extract French encoder attention\n",
    "3. ✅ Visualize and compare attention patterns\n",
    "4. ✅ Process multiple sentence pairs\n",
    "\n",
    "**Next Steps:**\n",
    "- Scale to all 2000 sentence pairs\n",
    "- Build attention graphs (tokens as nodes, attention weights as edges)\n",
    "- Compute persistent homology (β₀, β₁) using TDA\n",
    "- Compare topological structure across languages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}