{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NLLB-600M Model Exploration\n",
    "Load the saved model and test English → Chinese translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model from local directory\n",
    "model_dir = \"../models/nllb-600M\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "print(\"✓ Tokenizer loaded\")\n",
    "\n",
    "print(\"\\nLoading model...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "# Determine device: CUDA (Colab/NVIDIA) > MPS (Apple Silicon) > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"\\n✓ Model moved to device: {device}\")\n",
    "\n",
    "print(f\"\\nModel: NLLB-200-distilled-600M\")\n",
    "print(f\"Parameters: ~600M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Test English → Chinese translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLLB uses language codes: eng_Latn (English), zho_Hans (Simplified Chinese)\n",
    "test_sentence = \"Hello, how are you?\"\n",
    "print(f\"Input (English): {test_sentence}\")\n",
    "\n",
    "# Set source language\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate translation\n",
    "translated_tokens = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "# Decode\n",
    "translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "print(f\"Output (Chinese): {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Test with multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am a student at the university.\",\n",
    "    \"The cat is on the table.\",\n",
    "    \"What time is it?\",\n",
    "    \"I love learning languages.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"English → Chinese Translations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "        max_length=50\n",
    "    )\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"\\n{i}. EN: {sentence}\")\n",
    "    print(f\"   ZH: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Number of encoder layers: {model.config.encoder_layers}\")\n",
    "print(f\"  Number of decoder layers: {model.config.decoder_layers}\")\n",
    "print(f\"  Number of attention heads: {model.config.encoder_attention_heads}\")\n",
    "print(f\"  Hidden size: {model.config.d_model}\")\n",
    "print(f\"  Vocabulary size: {model.config.vocab_size}\")\n",
    "print(f\"\\nModel has encoder-decoder architecture for sequence-to-sequence translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nParameters (in millions): {total_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Test with dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some examples from our saved dataset\n",
    "dataset = load_from_disk(\"../data/wmt17_en-zh_validation_2000\")\n",
    "print(f\"Loaded {len(dataset)} sentence pairs\\n\")\n",
    "\n",
    "# Test on first 3 examples\n",
    "print(\"=\"*80)\n",
    "print(\"Testing on WMT17 dataset examples (English → Chinese)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.src_lang = \"eng_Latn\"\n",
    "\n",
    "for i in range(3):\n",
    "    example = dataset[i][\"translation\"]\n",
    "    english = example[\"en\"]\n",
    "    chinese_ref = example[\"zh\"]\n",
    "    \n",
    "    # Translate\n",
    "    inputs = tokenizer(english, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"zho_Hans\"),\n",
    "        max_length=100\n",
    "    )\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"EN: {english}\")\n",
    "    print(f\"ZH (reference): {chinese_ref}\")\n",
    "    print(f\"ZH (translated): {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model loaded successfully:**\n",
    "- NLLB-200-distilled-600M (~600M parameters)\n",
    "- English → Chinese translation working\n",
    "- GPU acceleration enabled (CUDA/MPS/CPU)\n",
    "- Ready for attention extraction\n",
    "\n",
    "**Next steps:**\n",
    "1. Extract attention weights from encoder and decoder\n",
    "2. Build attention graphs\n",
    "3. Compute persistent homology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
